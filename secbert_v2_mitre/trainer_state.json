{
  "best_global_step": 115994,
  "best_metric": 0.5013225674629211,
  "best_model_checkpoint": "./secbert_v2_mitre/checkpoint-115994",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 115994,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0017242271151956135,
      "grad_norm": 0.43654024600982666,
      "learning_rate": 1.9993172060623826e-05,
      "loss": 0.8302,
      "step": 100
    },
    {
      "epoch": 0.003448454230391227,
      "grad_norm": 0.22383257746696472,
      "learning_rate": 1.9986275152163045e-05,
      "loss": 0.4778,
      "step": 200
    },
    {
      "epoch": 0.0051726813455868405,
      "grad_norm": 0.0814347192645073,
      "learning_rate": 1.9979378243702262e-05,
      "loss": 0.5517,
      "step": 300
    },
    {
      "epoch": 0.006896908460782454,
      "grad_norm": 0.07549021393060684,
      "learning_rate": 1.9972481335241482e-05,
      "loss": 0.6538,
      "step": 400
    },
    {
      "epoch": 0.008621135575978068,
      "grad_norm": 0.046227190643548965,
      "learning_rate": 1.99655844267807e-05,
      "loss": 0.505,
      "step": 500
    },
    {
      "epoch": 0.010345362691173681,
      "grad_norm": 0.035110954195261,
      "learning_rate": 1.9958687518319915e-05,
      "loss": 0.2588,
      "step": 600
    },
    {
      "epoch": 0.012069589806369295,
      "grad_norm": 0.4092867970466614,
      "learning_rate": 1.995179060985913e-05,
      "loss": 0.5986,
      "step": 700
    },
    {
      "epoch": 0.013793816921564908,
      "grad_norm": 0.03350860998034477,
      "learning_rate": 1.9944893701398348e-05,
      "loss": 0.3885,
      "step": 800
    },
    {
      "epoch": 0.015518044036760522,
      "grad_norm": 19.158279418945312,
      "learning_rate": 1.9937996792937568e-05,
      "loss": 0.5707,
      "step": 900
    },
    {
      "epoch": 0.017242271151956135,
      "grad_norm": 0.08893142640590668,
      "learning_rate": 1.9931099884476784e-05,
      "loss": 0.4077,
      "step": 1000
    },
    {
      "epoch": 0.01896649826715175,
      "grad_norm": 0.23051302134990692,
      "learning_rate": 1.9924202976016004e-05,
      "loss": 0.523,
      "step": 1100
    },
    {
      "epoch": 0.020690725382347362,
      "grad_norm": 0.11781883984804153,
      "learning_rate": 1.991730606755522e-05,
      "loss": 0.3978,
      "step": 1200
    },
    {
      "epoch": 0.022414952497542977,
      "grad_norm": 0.07504166662693024,
      "learning_rate": 1.9910409159094437e-05,
      "loss": 0.5444,
      "step": 1300
    },
    {
      "epoch": 0.02413917961273859,
      "grad_norm": 0.03969317302107811,
      "learning_rate": 1.9903512250633654e-05,
      "loss": 0.2592,
      "step": 1400
    },
    {
      "epoch": 0.025863406727934204,
      "grad_norm": 0.04236472025513649,
      "learning_rate": 1.9896615342172874e-05,
      "loss": 0.4815,
      "step": 1500
    },
    {
      "epoch": 0.027587633843129816,
      "grad_norm": 0.19428321719169617,
      "learning_rate": 1.988971843371209e-05,
      "loss": 0.4397,
      "step": 1600
    },
    {
      "epoch": 0.02931186095832543,
      "grad_norm": 0.11283038556575775,
      "learning_rate": 1.9882821525251307e-05,
      "loss": 0.3987,
      "step": 1700
    },
    {
      "epoch": 0.031036088073521043,
      "grad_norm": 0.06892871856689453,
      "learning_rate": 1.9875924616790527e-05,
      "loss": 0.5172,
      "step": 1800
    },
    {
      "epoch": 0.03276031518871666,
      "grad_norm": 0.08935455232858658,
      "learning_rate": 1.9869027708329743e-05,
      "loss": 0.3499,
      "step": 1900
    },
    {
      "epoch": 0.03448454230391227,
      "grad_norm": 0.24991576373577118,
      "learning_rate": 1.986213079986896e-05,
      "loss": 0.529,
      "step": 2000
    },
    {
      "epoch": 0.03620876941910788,
      "grad_norm": 25.281993865966797,
      "learning_rate": 1.985523389140818e-05,
      "loss": 0.5287,
      "step": 2100
    },
    {
      "epoch": 0.0379329965343035,
      "grad_norm": 0.2749495804309845,
      "learning_rate": 1.9848336982947396e-05,
      "loss": 0.4887,
      "step": 2200
    },
    {
      "epoch": 0.03965722364949911,
      "grad_norm": 0.14337186515331268,
      "learning_rate": 1.9841440074486613e-05,
      "loss": 0.4068,
      "step": 2300
    },
    {
      "epoch": 0.041381450764694724,
      "grad_norm": 0.2457929253578186,
      "learning_rate": 1.983454316602583e-05,
      "loss": 0.4991,
      "step": 2400
    },
    {
      "epoch": 0.043105677879890336,
      "grad_norm": 0.14194658398628235,
      "learning_rate": 1.982764625756505e-05,
      "loss": 0.4509,
      "step": 2500
    },
    {
      "epoch": 0.044829904995085955,
      "grad_norm": 24.26611328125,
      "learning_rate": 1.9820749349104266e-05,
      "loss": 0.5485,
      "step": 2600
    },
    {
      "epoch": 0.04655413211028157,
      "grad_norm": 0.020809104666113853,
      "learning_rate": 1.9813852440643485e-05,
      "loss": 0.4658,
      "step": 2700
    },
    {
      "epoch": 0.04827835922547718,
      "grad_norm": 20.286529541015625,
      "learning_rate": 1.9806955532182702e-05,
      "loss": 0.5649,
      "step": 2800
    },
    {
      "epoch": 0.05000258634067279,
      "grad_norm": 0.026746809482574463,
      "learning_rate": 1.980005862372192e-05,
      "loss": 0.489,
      "step": 2900
    },
    {
      "epoch": 0.05172681345586841,
      "grad_norm": 0.4252820611000061,
      "learning_rate": 1.9793161715261135e-05,
      "loss": 0.4452,
      "step": 3000
    },
    {
      "epoch": 0.05345104057106402,
      "grad_norm": 0.1019183099269867,
      "learning_rate": 1.978626480680035e-05,
      "loss": 0.5247,
      "step": 3100
    },
    {
      "epoch": 0.05517526768625963,
      "grad_norm": 23.3206729888916,
      "learning_rate": 1.977936789833957e-05,
      "loss": 0.4435,
      "step": 3200
    },
    {
      "epoch": 0.056899494801455244,
      "grad_norm": 0.028567232191562653,
      "learning_rate": 1.9772470989878788e-05,
      "loss": 0.2827,
      "step": 3300
    },
    {
      "epoch": 0.05862372191665086,
      "grad_norm": 0.03259672597050667,
      "learning_rate": 1.9765574081418008e-05,
      "loss": 0.5407,
      "step": 3400
    },
    {
      "epoch": 0.060347949031846475,
      "grad_norm": 0.1464560627937317,
      "learning_rate": 1.9758677172957224e-05,
      "loss": 0.4321,
      "step": 3500
    },
    {
      "epoch": 0.06207217614704209,
      "grad_norm": 0.05212569609284401,
      "learning_rate": 1.975178026449644e-05,
      "loss": 0.5244,
      "step": 3600
    },
    {
      "epoch": 0.0637964032622377,
      "grad_norm": 0.10199639201164246,
      "learning_rate": 1.9744883356035657e-05,
      "loss": 0.5255,
      "step": 3700
    },
    {
      "epoch": 0.06552063037743332,
      "grad_norm": 22.47277069091797,
      "learning_rate": 1.9737986447574877e-05,
      "loss": 0.6832,
      "step": 3800
    },
    {
      "epoch": 0.06724485749262893,
      "grad_norm": 17.46666145324707,
      "learning_rate": 1.9731089539114094e-05,
      "loss": 0.7046,
      "step": 3900
    },
    {
      "epoch": 0.06896908460782454,
      "grad_norm": 0.07556861639022827,
      "learning_rate": 1.972419263065331e-05,
      "loss": 0.4546,
      "step": 4000
    },
    {
      "epoch": 0.07069331172302015,
      "grad_norm": 0.10565564781427383,
      "learning_rate": 1.971729572219253e-05,
      "loss": 0.4536,
      "step": 4100
    },
    {
      "epoch": 0.07241753883821576,
      "grad_norm": 0.17455671727657318,
      "learning_rate": 1.9710398813731747e-05,
      "loss": 0.6605,
      "step": 4200
    },
    {
      "epoch": 0.07414176595341139,
      "grad_norm": 0.5686036348342896,
      "learning_rate": 1.9703501905270963e-05,
      "loss": 0.5878,
      "step": 4300
    },
    {
      "epoch": 0.075865993068607,
      "grad_norm": 0.05768488720059395,
      "learning_rate": 1.9696604996810183e-05,
      "loss": 0.4393,
      "step": 4400
    },
    {
      "epoch": 0.07759022018380261,
      "grad_norm": 0.13142180442810059,
      "learning_rate": 1.96897080883494e-05,
      "loss": 0.5471,
      "step": 4500
    },
    {
      "epoch": 0.07931444729899823,
      "grad_norm": 0.0518290139734745,
      "learning_rate": 1.9682811179888616e-05,
      "loss": 0.4033,
      "step": 4600
    },
    {
      "epoch": 0.08103867441419384,
      "grad_norm": 19.767127990722656,
      "learning_rate": 1.9675914271427833e-05,
      "loss": 0.4822,
      "step": 4700
    },
    {
      "epoch": 0.08276290152938945,
      "grad_norm": 22.7572021484375,
      "learning_rate": 1.9669017362967053e-05,
      "loss": 0.6191,
      "step": 4800
    },
    {
      "epoch": 0.08448712864458506,
      "grad_norm": 0.297323077917099,
      "learning_rate": 1.966212045450627e-05,
      "loss": 0.4401,
      "step": 4900
    },
    {
      "epoch": 0.08621135575978067,
      "grad_norm": 0.0894087627530098,
      "learning_rate": 1.965522354604549e-05,
      "loss": 0.5092,
      "step": 5000
    },
    {
      "epoch": 0.0879355828749763,
      "grad_norm": 0.056861452758312225,
      "learning_rate": 1.9648326637584706e-05,
      "loss": 0.5621,
      "step": 5100
    },
    {
      "epoch": 0.08965980999017191,
      "grad_norm": 0.04809039086103439,
      "learning_rate": 1.9641429729123922e-05,
      "loss": 0.429,
      "step": 5200
    },
    {
      "epoch": 0.09138403710536752,
      "grad_norm": 0.09800632297992706,
      "learning_rate": 1.963453282066314e-05,
      "loss": 0.5692,
      "step": 5300
    },
    {
      "epoch": 0.09310826422056313,
      "grad_norm": 19.36373519897461,
      "learning_rate": 1.9627635912202355e-05,
      "loss": 0.5805,
      "step": 5400
    },
    {
      "epoch": 0.09483249133575875,
      "grad_norm": 0.0005742073408327997,
      "learning_rate": 1.9620739003741575e-05,
      "loss": 0.2855,
      "step": 5500
    },
    {
      "epoch": 0.09655671845095436,
      "grad_norm": 0.011842627078294754,
      "learning_rate": 1.961384209528079e-05,
      "loss": 0.5528,
      "step": 5600
    },
    {
      "epoch": 0.09828094556614997,
      "grad_norm": 0.0004491545259952545,
      "learning_rate": 1.960694518682001e-05,
      "loss": 0.5811,
      "step": 5700
    },
    {
      "epoch": 0.10000517268134558,
      "grad_norm": 0.03350481390953064,
      "learning_rate": 1.9600048278359228e-05,
      "loss": 0.294,
      "step": 5800
    },
    {
      "epoch": 0.1017293997965412,
      "grad_norm": 0.29133424162864685,
      "learning_rate": 1.9593151369898444e-05,
      "loss": 0.4479,
      "step": 5900
    },
    {
      "epoch": 0.10345362691173682,
      "grad_norm": 0.46731120347976685,
      "learning_rate": 1.958625446143766e-05,
      "loss": 0.7266,
      "step": 6000
    },
    {
      "epoch": 0.10517785402693243,
      "grad_norm": 0.1990538239479065,
      "learning_rate": 1.9579357552976877e-05,
      "loss": 0.3981,
      "step": 6100
    },
    {
      "epoch": 0.10690208114212804,
      "grad_norm": 0.018611833453178406,
      "learning_rate": 1.9572460644516097e-05,
      "loss": 0.5507,
      "step": 6200
    },
    {
      "epoch": 0.10862630825732365,
      "grad_norm": 0.07254943251609802,
      "learning_rate": 1.9565563736055314e-05,
      "loss": 0.4584,
      "step": 6300
    },
    {
      "epoch": 0.11035053537251927,
      "grad_norm": 0.13143108785152435,
      "learning_rate": 1.9558666827594534e-05,
      "loss": 0.4962,
      "step": 6400
    },
    {
      "epoch": 0.11207476248771488,
      "grad_norm": 0.13804520666599274,
      "learning_rate": 1.955176991913375e-05,
      "loss": 0.5624,
      "step": 6500
    },
    {
      "epoch": 0.11379898960291049,
      "grad_norm": 0.05387239530682564,
      "learning_rate": 1.9544873010672967e-05,
      "loss": 0.5065,
      "step": 6600
    },
    {
      "epoch": 0.11552321671810611,
      "grad_norm": 0.19933223724365234,
      "learning_rate": 1.9537976102212183e-05,
      "loss": 0.3363,
      "step": 6700
    },
    {
      "epoch": 0.11724744383330173,
      "grad_norm": 0.09296108782291412,
      "learning_rate": 1.9531079193751403e-05,
      "loss": 0.5652,
      "step": 6800
    },
    {
      "epoch": 0.11897167094849734,
      "grad_norm": 0.02465016581118107,
      "learning_rate": 1.952418228529062e-05,
      "loss": 0.405,
      "step": 6900
    },
    {
      "epoch": 0.12069589806369295,
      "grad_norm": 19.271753311157227,
      "learning_rate": 1.9517285376829836e-05,
      "loss": 0.3303,
      "step": 7000
    },
    {
      "epoch": 0.12242012517888856,
      "grad_norm": 0.2247316986322403,
      "learning_rate": 1.9510388468369056e-05,
      "loss": 0.5638,
      "step": 7100
    },
    {
      "epoch": 0.12414435229408417,
      "grad_norm": 0.08714450150728226,
      "learning_rate": 1.9503491559908273e-05,
      "loss": 0.5154,
      "step": 7200
    },
    {
      "epoch": 0.12586857940927978,
      "grad_norm": 0.04089818149805069,
      "learning_rate": 1.9496594651447493e-05,
      "loss": 0.3946,
      "step": 7300
    },
    {
      "epoch": 0.1275928065244754,
      "grad_norm": 0.1618419587612152,
      "learning_rate": 1.948969774298671e-05,
      "loss": 0.4581,
      "step": 7400
    },
    {
      "epoch": 0.129317033639671,
      "grad_norm": 0.14145980775356293,
      "learning_rate": 1.9482800834525926e-05,
      "loss": 0.4518,
      "step": 7500
    },
    {
      "epoch": 0.13104126075486663,
      "grad_norm": 0.07942647486925125,
      "learning_rate": 1.9475903926065142e-05,
      "loss": 0.5203,
      "step": 7600
    },
    {
      "epoch": 0.13276548787006223,
      "grad_norm": 24.80043601989746,
      "learning_rate": 1.946900701760436e-05,
      "loss": 0.5976,
      "step": 7700
    },
    {
      "epoch": 0.13448971498525786,
      "grad_norm": 22.21590805053711,
      "learning_rate": 1.946211010914358e-05,
      "loss": 0.5074,
      "step": 7800
    },
    {
      "epoch": 0.13621394210045348,
      "grad_norm": 0.3139655590057373,
      "learning_rate": 1.9455213200682795e-05,
      "loss": 0.5103,
      "step": 7900
    },
    {
      "epoch": 0.13793816921564908,
      "grad_norm": 0.10108236223459244,
      "learning_rate": 1.9448316292222015e-05,
      "loss": 0.4696,
      "step": 8000
    },
    {
      "epoch": 0.1396623963308447,
      "grad_norm": 0.1588584929704666,
      "learning_rate": 1.944141938376123e-05,
      "loss": 0.4812,
      "step": 8100
    },
    {
      "epoch": 0.1413866234460403,
      "grad_norm": 26.833192825317383,
      "learning_rate": 1.9434522475300448e-05,
      "loss": 0.5181,
      "step": 8200
    },
    {
      "epoch": 0.14311085056123593,
      "grad_norm": 0.09247786551713943,
      "learning_rate": 1.9427625566839665e-05,
      "loss": 0.5554,
      "step": 8300
    },
    {
      "epoch": 0.14483507767643153,
      "grad_norm": 0.36675798892974854,
      "learning_rate": 1.942072865837888e-05,
      "loss": 0.4836,
      "step": 8400
    },
    {
      "epoch": 0.14655930479162715,
      "grad_norm": 0.11268141865730286,
      "learning_rate": 1.94138317499181e-05,
      "loss": 0.5118,
      "step": 8500
    },
    {
      "epoch": 0.14828353190682278,
      "grad_norm": 0.11936801671981812,
      "learning_rate": 1.9406934841457317e-05,
      "loss": 0.4361,
      "step": 8600
    },
    {
      "epoch": 0.15000775902201838,
      "grad_norm": 22.57618522644043,
      "learning_rate": 1.9400037932996537e-05,
      "loss": 0.4384,
      "step": 8700
    },
    {
      "epoch": 0.151731986137214,
      "grad_norm": 0.14044396579265594,
      "learning_rate": 1.9393141024535754e-05,
      "loss": 0.5637,
      "step": 8800
    },
    {
      "epoch": 0.1534562132524096,
      "grad_norm": 0.06072361022233963,
      "learning_rate": 1.938624411607497e-05,
      "loss": 0.2345,
      "step": 8900
    },
    {
      "epoch": 0.15518044036760523,
      "grad_norm": 0.2126447558403015,
      "learning_rate": 1.9379347207614187e-05,
      "loss": 0.5138,
      "step": 9000
    },
    {
      "epoch": 0.15690466748280082,
      "grad_norm": 21.670169830322266,
      "learning_rate": 1.9372450299153407e-05,
      "loss": 0.502,
      "step": 9100
    },
    {
      "epoch": 0.15862889459799645,
      "grad_norm": 0.20115219056606293,
      "learning_rate": 1.9365553390692623e-05,
      "loss": 0.5888,
      "step": 9200
    },
    {
      "epoch": 0.16035312171319205,
      "grad_norm": 0.065118707716465,
      "learning_rate": 1.935865648223184e-05,
      "loss": 0.3122,
      "step": 9300
    },
    {
      "epoch": 0.16207734882838767,
      "grad_norm": 0.059954285621643066,
      "learning_rate": 1.935175957377106e-05,
      "loss": 0.3884,
      "step": 9400
    },
    {
      "epoch": 0.1638015759435833,
      "grad_norm": 15.987607955932617,
      "learning_rate": 1.9344862665310276e-05,
      "loss": 0.2919,
      "step": 9500
    },
    {
      "epoch": 0.1655258030587789,
      "grad_norm": 0.043059494346380234,
      "learning_rate": 1.9337965756849493e-05,
      "loss": 0.3361,
      "step": 9600
    },
    {
      "epoch": 0.16725003017397452,
      "grad_norm": 0.01123635284602642,
      "learning_rate": 1.9331068848388713e-05,
      "loss": 0.3687,
      "step": 9700
    },
    {
      "epoch": 0.16897425728917012,
      "grad_norm": 0.12663181126117706,
      "learning_rate": 1.932417193992793e-05,
      "loss": 0.6048,
      "step": 9800
    },
    {
      "epoch": 0.17069848440436575,
      "grad_norm": 0.3351566791534424,
      "learning_rate": 1.9317275031467146e-05,
      "loss": 0.3938,
      "step": 9900
    },
    {
      "epoch": 0.17242271151956134,
      "grad_norm": 0.12081491947174072,
      "learning_rate": 1.9310378123006362e-05,
      "loss": 0.4729,
      "step": 10000
    },
    {
      "epoch": 0.17414693863475697,
      "grad_norm": 29.02656364440918,
      "learning_rate": 1.9303481214545582e-05,
      "loss": 0.5985,
      "step": 10100
    },
    {
      "epoch": 0.1758711657499526,
      "grad_norm": 0.4163720905780792,
      "learning_rate": 1.92965843060848e-05,
      "loss": 0.5521,
      "step": 10200
    },
    {
      "epoch": 0.1775953928651482,
      "grad_norm": 0.45713523030281067,
      "learning_rate": 1.928968739762402e-05,
      "loss": 0.5334,
      "step": 10300
    },
    {
      "epoch": 0.17931961998034382,
      "grad_norm": 0.06653561443090439,
      "learning_rate": 1.9282790489163235e-05,
      "loss": 0.4445,
      "step": 10400
    },
    {
      "epoch": 0.18104384709553942,
      "grad_norm": 0.10309978574514389,
      "learning_rate": 1.927589358070245e-05,
      "loss": 0.3792,
      "step": 10500
    },
    {
      "epoch": 0.18276807421073504,
      "grad_norm": 0.3676570653915405,
      "learning_rate": 1.9268996672241668e-05,
      "loss": 0.5202,
      "step": 10600
    },
    {
      "epoch": 0.18449230132593064,
      "grad_norm": 6.537371518788859e-05,
      "learning_rate": 1.9262099763780885e-05,
      "loss": 0.5051,
      "step": 10700
    },
    {
      "epoch": 0.18621652844112627,
      "grad_norm": 0.13961121439933777,
      "learning_rate": 1.9255202855320105e-05,
      "loss": 0.2871,
      "step": 10800
    },
    {
      "epoch": 0.1879407555563219,
      "grad_norm": 0.04515913873910904,
      "learning_rate": 1.924830594685932e-05,
      "loss": 0.5431,
      "step": 10900
    },
    {
      "epoch": 0.1896649826715175,
      "grad_norm": 0.08427336066961288,
      "learning_rate": 1.924140903839854e-05,
      "loss": 0.3409,
      "step": 11000
    },
    {
      "epoch": 0.19138920978671312,
      "grad_norm": 0.21965491771697998,
      "learning_rate": 1.9234512129937757e-05,
      "loss": 0.4413,
      "step": 11100
    },
    {
      "epoch": 0.1931134369019087,
      "grad_norm": 19.377273559570312,
      "learning_rate": 1.9227615221476974e-05,
      "loss": 0.5161,
      "step": 11200
    },
    {
      "epoch": 0.19483766401710434,
      "grad_norm": 0.3472515046596527,
      "learning_rate": 1.922071831301619e-05,
      "loss": 0.4731,
      "step": 11300
    },
    {
      "epoch": 0.19656189113229994,
      "grad_norm": 0.36036673188209534,
      "learning_rate": 1.921382140455541e-05,
      "loss": 0.5627,
      "step": 11400
    },
    {
      "epoch": 0.19828611824749556,
      "grad_norm": 0.03870830684900284,
      "learning_rate": 1.9206924496094627e-05,
      "loss": 0.4926,
      "step": 11500
    },
    {
      "epoch": 0.20001034536269116,
      "grad_norm": 0.10208456963300705,
      "learning_rate": 1.9200027587633843e-05,
      "loss": 0.3743,
      "step": 11600
    },
    {
      "epoch": 0.2017345724778868,
      "grad_norm": 0.33733704686164856,
      "learning_rate": 1.9193130679173063e-05,
      "loss": 0.6345,
      "step": 11700
    },
    {
      "epoch": 0.2034587995930824,
      "grad_norm": 0.33790549635887146,
      "learning_rate": 1.918623377071228e-05,
      "loss": 0.3657,
      "step": 11800
    },
    {
      "epoch": 0.205183026708278,
      "grad_norm": 0.17542117834091187,
      "learning_rate": 1.9179336862251496e-05,
      "loss": 0.5754,
      "step": 11900
    },
    {
      "epoch": 0.20690725382347364,
      "grad_norm": 0.10886528342962265,
      "learning_rate": 1.9172439953790716e-05,
      "loss": 0.6354,
      "step": 12000
    },
    {
      "epoch": 0.20863148093866923,
      "grad_norm": 0.08201085031032562,
      "learning_rate": 1.9165543045329933e-05,
      "loss": 0.5564,
      "step": 12100
    },
    {
      "epoch": 0.21035570805386486,
      "grad_norm": 23.44236183166504,
      "learning_rate": 1.915864613686915e-05,
      "loss": 0.5109,
      "step": 12200
    },
    {
      "epoch": 0.21207993516906046,
      "grad_norm": 29.66079330444336,
      "learning_rate": 1.9151749228408366e-05,
      "loss": 0.6445,
      "step": 12300
    },
    {
      "epoch": 0.21380416228425608,
      "grad_norm": 0.17053380608558655,
      "learning_rate": 1.9144852319947586e-05,
      "loss": 0.5275,
      "step": 12400
    },
    {
      "epoch": 0.2155283893994517,
      "grad_norm": 0.13797526061534882,
      "learning_rate": 1.9137955411486802e-05,
      "loss": 0.4187,
      "step": 12500
    },
    {
      "epoch": 0.2172526165146473,
      "grad_norm": 0.2448430359363556,
      "learning_rate": 1.9131058503026022e-05,
      "loss": 0.5768,
      "step": 12600
    },
    {
      "epoch": 0.21897684362984293,
      "grad_norm": 0.14729928970336914,
      "learning_rate": 1.912416159456524e-05,
      "loss": 0.4726,
      "step": 12700
    },
    {
      "epoch": 0.22070107074503853,
      "grad_norm": 0.663054347038269,
      "learning_rate": 1.9117264686104455e-05,
      "loss": 0.6218,
      "step": 12800
    },
    {
      "epoch": 0.22242529786023416,
      "grad_norm": 0.14843261241912842,
      "learning_rate": 1.911036777764367e-05,
      "loss": 0.3819,
      "step": 12900
    },
    {
      "epoch": 0.22414952497542975,
      "grad_norm": 0.09272941201925278,
      "learning_rate": 1.9103470869182888e-05,
      "loss": 0.7136,
      "step": 13000
    },
    {
      "epoch": 0.22587375209062538,
      "grad_norm": 0.25922659039497375,
      "learning_rate": 1.9096573960722108e-05,
      "loss": 0.5804,
      "step": 13100
    },
    {
      "epoch": 0.22759797920582098,
      "grad_norm": 3.779753751587123e-05,
      "learning_rate": 1.9089677052261325e-05,
      "loss": 0.4048,
      "step": 13200
    },
    {
      "epoch": 0.2293222063210166,
      "grad_norm": 25.198970794677734,
      "learning_rate": 1.9082780143800545e-05,
      "loss": 0.6401,
      "step": 13300
    },
    {
      "epoch": 0.23104643343621223,
      "grad_norm": 18.665483474731445,
      "learning_rate": 1.907588323533976e-05,
      "loss": 0.5933,
      "step": 13400
    },
    {
      "epoch": 0.23277066055140783,
      "grad_norm": 0.1234968975186348,
      "learning_rate": 1.9068986326878978e-05,
      "loss": 0.4574,
      "step": 13500
    },
    {
      "epoch": 0.23449488766660345,
      "grad_norm": 0.20060428977012634,
      "learning_rate": 1.9062089418418194e-05,
      "loss": 0.7179,
      "step": 13600
    },
    {
      "epoch": 0.23621911478179905,
      "grad_norm": 0.4243275225162506,
      "learning_rate": 1.9055192509957414e-05,
      "loss": 0.5381,
      "step": 13700
    },
    {
      "epoch": 0.23794334189699468,
      "grad_norm": 0.2730705738067627,
      "learning_rate": 1.904829560149663e-05,
      "loss": 0.3928,
      "step": 13800
    },
    {
      "epoch": 0.23966756901219027,
      "grad_norm": 0.22646670043468475,
      "learning_rate": 1.9041398693035847e-05,
      "loss": 0.6741,
      "step": 13900
    },
    {
      "epoch": 0.2413917961273859,
      "grad_norm": 0.14518462121486664,
      "learning_rate": 1.9034501784575067e-05,
      "loss": 0.3581,
      "step": 14000
    },
    {
      "epoch": 0.24311602324258152,
      "grad_norm": 0.0668015107512474,
      "learning_rate": 1.9027604876114283e-05,
      "loss": 0.4979,
      "step": 14100
    },
    {
      "epoch": 0.24484025035777712,
      "grad_norm": 0.28609928488731384,
      "learning_rate": 1.90207079676535e-05,
      "loss": 0.5208,
      "step": 14200
    },
    {
      "epoch": 0.24656447747297275,
      "grad_norm": 0.06719350069761276,
      "learning_rate": 1.901381105919272e-05,
      "loss": 0.5257,
      "step": 14300
    },
    {
      "epoch": 0.24828870458816835,
      "grad_norm": 0.13379569351673126,
      "learning_rate": 1.9006914150731936e-05,
      "loss": 0.4843,
      "step": 14400
    },
    {
      "epoch": 0.25001293170336397,
      "grad_norm": 17.049760818481445,
      "learning_rate": 1.9000017242271153e-05,
      "loss": 0.5957,
      "step": 14500
    },
    {
      "epoch": 0.25173715881855957,
      "grad_norm": 13.634404182434082,
      "learning_rate": 1.899312033381037e-05,
      "loss": 0.5271,
      "step": 14600
    },
    {
      "epoch": 0.25346138593375517,
      "grad_norm": 0.2901800572872162,
      "learning_rate": 1.898622342534959e-05,
      "loss": 0.5192,
      "step": 14700
    },
    {
      "epoch": 0.2551856130489508,
      "grad_norm": 24.82898712158203,
      "learning_rate": 1.8979326516888806e-05,
      "loss": 0.6358,
      "step": 14800
    },
    {
      "epoch": 0.2569098401641464,
      "grad_norm": 0.05462644249200821,
      "learning_rate": 1.8972429608428026e-05,
      "loss": 0.3897,
      "step": 14900
    },
    {
      "epoch": 0.258634067279342,
      "grad_norm": 0.27446040511131287,
      "learning_rate": 1.8965532699967242e-05,
      "loss": 0.4476,
      "step": 15000
    },
    {
      "epoch": 0.26035829439453767,
      "grad_norm": 0.02699488215148449,
      "learning_rate": 1.895863579150646e-05,
      "loss": 0.2992,
      "step": 15100
    },
    {
      "epoch": 0.26208252150973327,
      "grad_norm": 0.06739504635334015,
      "learning_rate": 1.8951738883045675e-05,
      "loss": 0.4551,
      "step": 15200
    },
    {
      "epoch": 0.26380674862492887,
      "grad_norm": 0.10045856237411499,
      "learning_rate": 1.8944841974584892e-05,
      "loss": 0.5573,
      "step": 15300
    },
    {
      "epoch": 0.26553097574012446,
      "grad_norm": 1.601911753823515e-05,
      "learning_rate": 1.893794506612411e-05,
      "loss": 0.4715,
      "step": 15400
    },
    {
      "epoch": 0.2672552028553201,
      "grad_norm": 0.13411860167980194,
      "learning_rate": 1.8931048157663328e-05,
      "loss": 0.4652,
      "step": 15500
    },
    {
      "epoch": 0.2689794299705157,
      "grad_norm": 0.12925688922405243,
      "learning_rate": 1.8924151249202548e-05,
      "loss": 0.7487,
      "step": 15600
    },
    {
      "epoch": 0.2707036570857113,
      "grad_norm": 0.1752367466688156,
      "learning_rate": 1.8917254340741765e-05,
      "loss": 0.4681,
      "step": 15700
    },
    {
      "epoch": 0.27242788420090697,
      "grad_norm": 0.14675010740756989,
      "learning_rate": 1.891035743228098e-05,
      "loss": 0.5295,
      "step": 15800
    },
    {
      "epoch": 0.27415211131610256,
      "grad_norm": 0.20006150007247925,
      "learning_rate": 1.8903460523820198e-05,
      "loss": 0.5077,
      "step": 15900
    },
    {
      "epoch": 0.27587633843129816,
      "grad_norm": 0.32043150067329407,
      "learning_rate": 1.8896563615359414e-05,
      "loss": 0.4072,
      "step": 16000
    },
    {
      "epoch": 0.27760056554649376,
      "grad_norm": 0.09177098423242569,
      "learning_rate": 1.8889666706898634e-05,
      "loss": 0.4693,
      "step": 16100
    },
    {
      "epoch": 0.2793247926616894,
      "grad_norm": 0.08086059242486954,
      "learning_rate": 1.888276979843785e-05,
      "loss": 0.352,
      "step": 16200
    },
    {
      "epoch": 0.281049019776885,
      "grad_norm": 0.08576386421918869,
      "learning_rate": 1.887587288997707e-05,
      "loss": 0.2901,
      "step": 16300
    },
    {
      "epoch": 0.2827732468920806,
      "grad_norm": 0.45664188265800476,
      "learning_rate": 1.8868975981516287e-05,
      "loss": 0.6234,
      "step": 16400
    },
    {
      "epoch": 0.28449747400727626,
      "grad_norm": 22.840635299682617,
      "learning_rate": 1.8862079073055504e-05,
      "loss": 0.5137,
      "step": 16500
    },
    {
      "epoch": 0.28622170112247186,
      "grad_norm": 0.35938429832458496,
      "learning_rate": 1.8855182164594723e-05,
      "loss": 0.5145,
      "step": 16600
    },
    {
      "epoch": 0.28794592823766746,
      "grad_norm": 0.07259891927242279,
      "learning_rate": 1.884828525613394e-05,
      "loss": 0.42,
      "step": 16700
    },
    {
      "epoch": 0.28967015535286306,
      "grad_norm": 22.092668533325195,
      "learning_rate": 1.8841388347673156e-05,
      "loss": 0.5918,
      "step": 16800
    },
    {
      "epoch": 0.2913943824680587,
      "grad_norm": 0.3091842830181122,
      "learning_rate": 1.8834491439212373e-05,
      "loss": 0.3806,
      "step": 16900
    },
    {
      "epoch": 0.2931186095832543,
      "grad_norm": 0.10031657665967941,
      "learning_rate": 1.8827594530751593e-05,
      "loss": 0.5504,
      "step": 17000
    },
    {
      "epoch": 0.2948428366984499,
      "grad_norm": 0.06452590972185135,
      "learning_rate": 1.882069762229081e-05,
      "loss": 0.4977,
      "step": 17100
    },
    {
      "epoch": 0.29656706381364556,
      "grad_norm": 0.04729921370744705,
      "learning_rate": 1.881380071383003e-05,
      "loss": 0.3664,
      "step": 17200
    },
    {
      "epoch": 0.29829129092884116,
      "grad_norm": 0.07215982675552368,
      "learning_rate": 1.8806903805369246e-05,
      "loss": 0.6178,
      "step": 17300
    },
    {
      "epoch": 0.30001551804403676,
      "grad_norm": 0.1079682782292366,
      "learning_rate": 1.8800006896908462e-05,
      "loss": 0.4562,
      "step": 17400
    },
    {
      "epoch": 0.30173974515923235,
      "grad_norm": 0.16027779877185822,
      "learning_rate": 1.879310998844768e-05,
      "loss": 0.3923,
      "step": 17500
    },
    {
      "epoch": 0.303463972274428,
      "grad_norm": 0.40435436367988586,
      "learning_rate": 1.8786213079986895e-05,
      "loss": 0.5447,
      "step": 17600
    },
    {
      "epoch": 0.3051881993896236,
      "grad_norm": 0.0983957052230835,
      "learning_rate": 1.8779316171526115e-05,
      "loss": 0.5389,
      "step": 17700
    },
    {
      "epoch": 0.3069124265048192,
      "grad_norm": 0.07941855490207672,
      "learning_rate": 1.8772419263065332e-05,
      "loss": 0.5637,
      "step": 17800
    },
    {
      "epoch": 0.30863665362001486,
      "grad_norm": 28.320341110229492,
      "learning_rate": 1.8765522354604552e-05,
      "loss": 0.5317,
      "step": 17900
    },
    {
      "epoch": 0.31036088073521045,
      "grad_norm": 0.2238464504480362,
      "learning_rate": 1.8758625446143768e-05,
      "loss": 0.4718,
      "step": 18000
    },
    {
      "epoch": 0.31208510785040605,
      "grad_norm": 0.7638015151023865,
      "learning_rate": 1.8751728537682985e-05,
      "loss": 0.5434,
      "step": 18100
    },
    {
      "epoch": 0.31380933496560165,
      "grad_norm": 0.6420515179634094,
      "learning_rate": 1.87448316292222e-05,
      "loss": 0.4939,
      "step": 18200
    },
    {
      "epoch": 0.3155335620807973,
      "grad_norm": 0.04610702395439148,
      "learning_rate": 1.8737934720761418e-05,
      "loss": 0.4968,
      "step": 18300
    },
    {
      "epoch": 0.3172577891959929,
      "grad_norm": 0.3993151783943176,
      "learning_rate": 1.8731037812300638e-05,
      "loss": 0.6435,
      "step": 18400
    },
    {
      "epoch": 0.3189820163111885,
      "grad_norm": 0.18522359430789948,
      "learning_rate": 1.8724140903839854e-05,
      "loss": 0.305,
      "step": 18500
    },
    {
      "epoch": 0.3207062434263841,
      "grad_norm": 0.21013151109218597,
      "learning_rate": 1.8717243995379074e-05,
      "loss": 0.4631,
      "step": 18600
    },
    {
      "epoch": 0.32243047054157975,
      "grad_norm": 0.16338099539279938,
      "learning_rate": 1.871034708691829e-05,
      "loss": 0.6249,
      "step": 18700
    },
    {
      "epoch": 0.32415469765677535,
      "grad_norm": 0.5304088592529297,
      "learning_rate": 1.8703450178457507e-05,
      "loss": 0.4627,
      "step": 18800
    },
    {
      "epoch": 0.32587892477197095,
      "grad_norm": 0.3569641709327698,
      "learning_rate": 1.8696553269996724e-05,
      "loss": 0.6384,
      "step": 18900
    },
    {
      "epoch": 0.3276031518871666,
      "grad_norm": 0.2728142738342285,
      "learning_rate": 1.8689656361535944e-05,
      "loss": 0.3915,
      "step": 19000
    },
    {
      "epoch": 0.3293273790023622,
      "grad_norm": 0.24355080723762512,
      "learning_rate": 1.868275945307516e-05,
      "loss": 0.467,
      "step": 19100
    },
    {
      "epoch": 0.3310516061175578,
      "grad_norm": 0.21026752889156342,
      "learning_rate": 1.8675862544614377e-05,
      "loss": 0.4478,
      "step": 19200
    },
    {
      "epoch": 0.3327758332327534,
      "grad_norm": 0.058284636586904526,
      "learning_rate": 1.8668965636153596e-05,
      "loss": 0.5829,
      "step": 19300
    },
    {
      "epoch": 0.33450006034794905,
      "grad_norm": 0.0369703434407711,
      "learning_rate": 1.8662068727692813e-05,
      "loss": 0.5834,
      "step": 19400
    },
    {
      "epoch": 0.33622428746314464,
      "grad_norm": 0.08446189761161804,
      "learning_rate": 1.865517181923203e-05,
      "loss": 0.4777,
      "step": 19500
    },
    {
      "epoch": 0.33794851457834024,
      "grad_norm": 0.31736263632774353,
      "learning_rate": 1.864827491077125e-05,
      "loss": 0.5748,
      "step": 19600
    },
    {
      "epoch": 0.3396727416935359,
      "grad_norm": 0.059636037796735764,
      "learning_rate": 1.8641378002310466e-05,
      "loss": 0.4043,
      "step": 19700
    },
    {
      "epoch": 0.3413969688087315,
      "grad_norm": 0.12738139927387238,
      "learning_rate": 1.8634481093849682e-05,
      "loss": 0.4507,
      "step": 19800
    },
    {
      "epoch": 0.3431211959239271,
      "grad_norm": 0.13845610618591309,
      "learning_rate": 1.86275841853889e-05,
      "loss": 0.5955,
      "step": 19900
    },
    {
      "epoch": 0.3448454230391227,
      "grad_norm": 0.22993749380111694,
      "learning_rate": 1.862068727692812e-05,
      "loss": 0.3171,
      "step": 20000
    },
    {
      "epoch": 0.34656965015431834,
      "grad_norm": 0.24265486001968384,
      "learning_rate": 1.861379036846734e-05,
      "loss": 0.7397,
      "step": 20100
    },
    {
      "epoch": 0.34829387726951394,
      "grad_norm": 0.071280337870121,
      "learning_rate": 1.8606893460006555e-05,
      "loss": 0.5787,
      "step": 20200
    },
    {
      "epoch": 0.35001810438470954,
      "grad_norm": 0.1349005252122879,
      "learning_rate": 1.8599996551545772e-05,
      "loss": 0.5058,
      "step": 20300
    },
    {
      "epoch": 0.3517423314999052,
      "grad_norm": 0.08503428101539612,
      "learning_rate": 1.859309964308499e-05,
      "loss": 0.3125,
      "step": 20400
    },
    {
      "epoch": 0.3534665586151008,
      "grad_norm": 0.2509493827819824,
      "learning_rate": 1.8586202734624205e-05,
      "loss": 0.6232,
      "step": 20500
    },
    {
      "epoch": 0.3551907857302964,
      "grad_norm": 0.14937733113765717,
      "learning_rate": 1.857930582616342e-05,
      "loss": 0.451,
      "step": 20600
    },
    {
      "epoch": 0.356915012845492,
      "grad_norm": 0.09230785071849823,
      "learning_rate": 1.857240891770264e-05,
      "loss": 0.6259,
      "step": 20700
    },
    {
      "epoch": 0.35863923996068764,
      "grad_norm": 0.2021866887807846,
      "learning_rate": 1.856551200924186e-05,
      "loss": 0.4404,
      "step": 20800
    },
    {
      "epoch": 0.36036346707588324,
      "grad_norm": 0.07096455246210098,
      "learning_rate": 1.8558615100781078e-05,
      "loss": 0.4074,
      "step": 20900
    },
    {
      "epoch": 0.36208769419107883,
      "grad_norm": 0.203050896525383,
      "learning_rate": 1.8551718192320294e-05,
      "loss": 0.34,
      "step": 21000
    },
    {
      "epoch": 0.3638119213062745,
      "grad_norm": 0.11079321801662445,
      "learning_rate": 1.854482128385951e-05,
      "loss": 0.5889,
      "step": 21100
    },
    {
      "epoch": 0.3655361484214701,
      "grad_norm": 0.19009846448898315,
      "learning_rate": 1.8537924375398727e-05,
      "loss": 0.5723,
      "step": 21200
    },
    {
      "epoch": 0.3672603755366657,
      "grad_norm": 0.0429583378136158,
      "learning_rate": 1.8531027466937947e-05,
      "loss": 0.5208,
      "step": 21300
    },
    {
      "epoch": 0.3689846026518613,
      "grad_norm": 26.335506439208984,
      "learning_rate": 1.8524130558477164e-05,
      "loss": 0.5042,
      "step": 21400
    },
    {
      "epoch": 0.37070882976705694,
      "grad_norm": 3.8251973819569685e-06,
      "learning_rate": 1.851723365001638e-05,
      "loss": 0.4082,
      "step": 21500
    },
    {
      "epoch": 0.37243305688225253,
      "grad_norm": 0.12101248651742935,
      "learning_rate": 1.85103367415556e-05,
      "loss": 0.4991,
      "step": 21600
    },
    {
      "epoch": 0.37415728399744813,
      "grad_norm": 0.07658636569976807,
      "learning_rate": 1.8503439833094817e-05,
      "loss": 0.6348,
      "step": 21700
    },
    {
      "epoch": 0.3758815111126438,
      "grad_norm": 0.12741002440452576,
      "learning_rate": 1.8496542924634033e-05,
      "loss": 0.4725,
      "step": 21800
    },
    {
      "epoch": 0.3776057382278394,
      "grad_norm": 0.1335509568452835,
      "learning_rate": 1.8489646016173253e-05,
      "loss": 0.6797,
      "step": 21900
    },
    {
      "epoch": 0.379329965343035,
      "grad_norm": 0.31515079736709595,
      "learning_rate": 1.848274910771247e-05,
      "loss": 0.7093,
      "step": 22000
    },
    {
      "epoch": 0.3810541924582306,
      "grad_norm": 0.29279014468193054,
      "learning_rate": 1.8475852199251686e-05,
      "loss": 0.3835,
      "step": 22100
    },
    {
      "epoch": 0.38277841957342623,
      "grad_norm": 0.03733493387699127,
      "learning_rate": 1.8468955290790903e-05,
      "loss": 0.4552,
      "step": 22200
    },
    {
      "epoch": 0.38450264668862183,
      "grad_norm": 27.56786346435547,
      "learning_rate": 1.8462058382330122e-05,
      "loss": 0.5073,
      "step": 22300
    },
    {
      "epoch": 0.3862268738038174,
      "grad_norm": 1.2419404811225832e-06,
      "learning_rate": 1.845516147386934e-05,
      "loss": 0.6088,
      "step": 22400
    },
    {
      "epoch": 0.387951100919013,
      "grad_norm": 0.4377785921096802,
      "learning_rate": 1.844826456540856e-05,
      "loss": 0.7455,
      "step": 22500
    },
    {
      "epoch": 0.3896753280342087,
      "grad_norm": 21.477022171020508,
      "learning_rate": 1.8441367656947775e-05,
      "loss": 0.6261,
      "step": 22600
    },
    {
      "epoch": 0.3913995551494043,
      "grad_norm": 0.12689320743083954,
      "learning_rate": 1.8434470748486992e-05,
      "loss": 0.4067,
      "step": 22700
    },
    {
      "epoch": 0.3931237822645999,
      "grad_norm": 0.07285209000110626,
      "learning_rate": 1.842757384002621e-05,
      "loss": 0.4827,
      "step": 22800
    },
    {
      "epoch": 0.39484800937979553,
      "grad_norm": 0.30848929286003113,
      "learning_rate": 1.8420676931565425e-05,
      "loss": 0.5446,
      "step": 22900
    },
    {
      "epoch": 0.3965722364949911,
      "grad_norm": 0.1976097822189331,
      "learning_rate": 1.8413780023104645e-05,
      "loss": 0.476,
      "step": 23000
    },
    {
      "epoch": 0.3982964636101867,
      "grad_norm": 0.361127644777298,
      "learning_rate": 1.8406883114643865e-05,
      "loss": 0.5615,
      "step": 23100
    },
    {
      "epoch": 0.4000206907253823,
      "grad_norm": 0.515874445438385,
      "learning_rate": 1.839998620618308e-05,
      "loss": 0.5314,
      "step": 23200
    },
    {
      "epoch": 0.401744917840578,
      "grad_norm": 0.12633955478668213,
      "learning_rate": 1.8393089297722298e-05,
      "loss": 0.5139,
      "step": 23300
    },
    {
      "epoch": 0.4034691449557736,
      "grad_norm": 0.09518903493881226,
      "learning_rate": 1.8386192389261514e-05,
      "loss": 0.4711,
      "step": 23400
    },
    {
      "epoch": 0.40519337207096917,
      "grad_norm": 0.15136194229125977,
      "learning_rate": 1.837929548080073e-05,
      "loss": 0.43,
      "step": 23500
    },
    {
      "epoch": 0.4069175991861648,
      "grad_norm": 0.21513526141643524,
      "learning_rate": 1.837239857233995e-05,
      "loss": 0.6914,
      "step": 23600
    },
    {
      "epoch": 0.4086418263013604,
      "grad_norm": 0.0655745416879654,
      "learning_rate": 1.8365501663879167e-05,
      "loss": 0.652,
      "step": 23700
    },
    {
      "epoch": 0.410366053416556,
      "grad_norm": 0.020776906982064247,
      "learning_rate": 1.8358604755418387e-05,
      "loss": 0.47,
      "step": 23800
    },
    {
      "epoch": 0.4120902805317516,
      "grad_norm": 0.0833551213145256,
      "learning_rate": 1.8351707846957604e-05,
      "loss": 0.5052,
      "step": 23900
    },
    {
      "epoch": 0.41381450764694727,
      "grad_norm": 0.07431630045175552,
      "learning_rate": 1.834481093849682e-05,
      "loss": 0.4565,
      "step": 24000
    },
    {
      "epoch": 0.41553873476214287,
      "grad_norm": 0.11421216279268265,
      "learning_rate": 1.8337914030036037e-05,
      "loss": 0.6728,
      "step": 24100
    },
    {
      "epoch": 0.41726296187733847,
      "grad_norm": 0.04779395833611488,
      "learning_rate": 1.8331017121575257e-05,
      "loss": 0.4745,
      "step": 24200
    },
    {
      "epoch": 0.4189871889925341,
      "grad_norm": 25.30015754699707,
      "learning_rate": 1.8324120213114473e-05,
      "loss": 0.5856,
      "step": 24300
    },
    {
      "epoch": 0.4207114161077297,
      "grad_norm": 0.17485924065113068,
      "learning_rate": 1.831722330465369e-05,
      "loss": 0.4277,
      "step": 24400
    },
    {
      "epoch": 0.4224356432229253,
      "grad_norm": 0.08493798226118088,
      "learning_rate": 1.831032639619291e-05,
      "loss": 0.538,
      "step": 24500
    },
    {
      "epoch": 0.4241598703381209,
      "grad_norm": 0.37051287293434143,
      "learning_rate": 1.8303429487732126e-05,
      "loss": 0.5316,
      "step": 24600
    },
    {
      "epoch": 0.42588409745331657,
      "grad_norm": 22.207069396972656,
      "learning_rate": 1.8296532579271343e-05,
      "loss": 0.4451,
      "step": 24700
    },
    {
      "epoch": 0.42760832456851217,
      "grad_norm": 0.1594422310590744,
      "learning_rate": 1.8289635670810562e-05,
      "loss": 0.5669,
      "step": 24800
    },
    {
      "epoch": 0.42933255168370776,
      "grad_norm": 0.09894683212041855,
      "learning_rate": 1.828273876234978e-05,
      "loss": 0.5841,
      "step": 24900
    },
    {
      "epoch": 0.4310567787989034,
      "grad_norm": 22.764265060424805,
      "learning_rate": 1.8275841853888995e-05,
      "loss": 0.5965,
      "step": 25000
    },
    {
      "epoch": 0.432781005914099,
      "grad_norm": 0.08616625517606735,
      "learning_rate": 1.8268944945428212e-05,
      "loss": 0.3376,
      "step": 25100
    },
    {
      "epoch": 0.4345052330292946,
      "grad_norm": 24.35100555419922,
      "learning_rate": 1.8262048036967432e-05,
      "loss": 0.4792,
      "step": 25200
    },
    {
      "epoch": 0.4362294601444902,
      "grad_norm": 0.01874660886824131,
      "learning_rate": 1.825515112850665e-05,
      "loss": 0.4298,
      "step": 25300
    },
    {
      "epoch": 0.43795368725968586,
      "grad_norm": 0.1477079689502716,
      "learning_rate": 1.824825422004587e-05,
      "loss": 0.5128,
      "step": 25400
    },
    {
      "epoch": 0.43967791437488146,
      "grad_norm": 0.21222886443138123,
      "learning_rate": 1.8241357311585085e-05,
      "loss": 0.4362,
      "step": 25500
    },
    {
      "epoch": 0.44140214149007706,
      "grad_norm": 0.3989523649215698,
      "learning_rate": 1.82344604031243e-05,
      "loss": 0.5069,
      "step": 25600
    },
    {
      "epoch": 0.4431263686052727,
      "grad_norm": 0.2646142542362213,
      "learning_rate": 1.8227563494663518e-05,
      "loss": 0.384,
      "step": 25700
    },
    {
      "epoch": 0.4448505957204683,
      "grad_norm": 0.07148793339729309,
      "learning_rate": 1.8220666586202734e-05,
      "loss": 0.3542,
      "step": 25800
    },
    {
      "epoch": 0.4465748228356639,
      "grad_norm": 0.32279491424560547,
      "learning_rate": 1.821376967774195e-05,
      "loss": 0.432,
      "step": 25900
    },
    {
      "epoch": 0.4482990499508595,
      "grad_norm": 0.06379710137844086,
      "learning_rate": 1.820687276928117e-05,
      "loss": 0.4082,
      "step": 26000
    },
    {
      "epoch": 0.45002327706605516,
      "grad_norm": 0.030838122591376305,
      "learning_rate": 1.819997586082039e-05,
      "loss": 0.5545,
      "step": 26100
    },
    {
      "epoch": 0.45174750418125076,
      "grad_norm": 0.06692543625831604,
      "learning_rate": 1.8193078952359607e-05,
      "loss": 0.4812,
      "step": 26200
    },
    {
      "epoch": 0.45347173129644636,
      "grad_norm": 0.15195971727371216,
      "learning_rate": 1.8186182043898824e-05,
      "loss": 0.659,
      "step": 26300
    },
    {
      "epoch": 0.45519595841164195,
      "grad_norm": 0.20317649841308594,
      "learning_rate": 1.817928513543804e-05,
      "loss": 0.3775,
      "step": 26400
    },
    {
      "epoch": 0.4569201855268376,
      "grad_norm": 0.06820143759250641,
      "learning_rate": 1.817238822697726e-05,
      "loss": 0.5626,
      "step": 26500
    },
    {
      "epoch": 0.4586444126420332,
      "grad_norm": 0.14751334488391876,
      "learning_rate": 1.8165491318516477e-05,
      "loss": 0.5155,
      "step": 26600
    },
    {
      "epoch": 0.4603686397572288,
      "grad_norm": 0.0661831945180893,
      "learning_rate": 1.8158594410055693e-05,
      "loss": 0.6344,
      "step": 26700
    },
    {
      "epoch": 0.46209286687242446,
      "grad_norm": 19.880390167236328,
      "learning_rate": 1.8151697501594913e-05,
      "loss": 0.6257,
      "step": 26800
    },
    {
      "epoch": 0.46381709398762005,
      "grad_norm": 23.40189552307129,
      "learning_rate": 1.814480059313413e-05,
      "loss": 0.7599,
      "step": 26900
    },
    {
      "epoch": 0.46554132110281565,
      "grad_norm": 25.303730010986328,
      "learning_rate": 1.8137903684673346e-05,
      "loss": 0.4195,
      "step": 27000
    },
    {
      "epoch": 0.46726554821801125,
      "grad_norm": 0.25110435485839844,
      "learning_rate": 1.8131006776212566e-05,
      "loss": 0.4672,
      "step": 27100
    },
    {
      "epoch": 0.4689897753332069,
      "grad_norm": 22.5779972076416,
      "learning_rate": 1.8124109867751783e-05,
      "loss": 0.5247,
      "step": 27200
    },
    {
      "epoch": 0.4707140024484025,
      "grad_norm": 0.05213237926363945,
      "learning_rate": 1.8117212959291e-05,
      "loss": 0.4288,
      "step": 27300
    },
    {
      "epoch": 0.4724382295635981,
      "grad_norm": 0.24267224967479706,
      "learning_rate": 1.8110316050830216e-05,
      "loss": 0.3471,
      "step": 27400
    },
    {
      "epoch": 0.47416245667879375,
      "grad_norm": 0.12371981143951416,
      "learning_rate": 1.8103419142369435e-05,
      "loss": 0.3973,
      "step": 27500
    },
    {
      "epoch": 0.47588668379398935,
      "grad_norm": 0.2702658772468567,
      "learning_rate": 1.8096522233908652e-05,
      "loss": 0.4797,
      "step": 27600
    },
    {
      "epoch": 0.47761091090918495,
      "grad_norm": 0.5218454599380493,
      "learning_rate": 1.8089625325447872e-05,
      "loss": 0.5382,
      "step": 27700
    },
    {
      "epoch": 0.47933513802438055,
      "grad_norm": 24.09190559387207,
      "learning_rate": 1.808272841698709e-05,
      "loss": 0.5824,
      "step": 27800
    },
    {
      "epoch": 0.4810593651395762,
      "grad_norm": 0.49273964762687683,
      "learning_rate": 1.8075831508526305e-05,
      "loss": 1.2696,
      "step": 27900
    },
    {
      "epoch": 0.4827835922547718,
      "grad_norm": 0.33869579434394836,
      "learning_rate": 1.806893460006552e-05,
      "loss": 0.4327,
      "step": 28000
    },
    {
      "epoch": 0.4845078193699674,
      "grad_norm": 22.082658767700195,
      "learning_rate": 1.8062037691604738e-05,
      "loss": 0.7244,
      "step": 28100
    },
    {
      "epoch": 0.48623204648516305,
      "grad_norm": 0.425142765045166,
      "learning_rate": 1.8055140783143958e-05,
      "loss": 0.6268,
      "step": 28200
    },
    {
      "epoch": 0.48795627360035865,
      "grad_norm": 18.746458053588867,
      "learning_rate": 1.8048243874683174e-05,
      "loss": 0.4333,
      "step": 28300
    },
    {
      "epoch": 0.48968050071555425,
      "grad_norm": 22.47191619873047,
      "learning_rate": 1.8041346966222394e-05,
      "loss": 0.7806,
      "step": 28400
    },
    {
      "epoch": 0.49140472783074984,
      "grad_norm": 0.16500571370124817,
      "learning_rate": 1.803445005776161e-05,
      "loss": 0.3787,
      "step": 28500
    },
    {
      "epoch": 0.4931289549459455,
      "grad_norm": 0.08355593681335449,
      "learning_rate": 1.8027553149300827e-05,
      "loss": 0.5326,
      "step": 28600
    },
    {
      "epoch": 0.4948531820611411,
      "grad_norm": 0.11658023297786713,
      "learning_rate": 1.8020656240840044e-05,
      "loss": 0.5823,
      "step": 28700
    },
    {
      "epoch": 0.4965774091763367,
      "grad_norm": 0.1762458086013794,
      "learning_rate": 1.801375933237926e-05,
      "loss": 0.5129,
      "step": 28800
    },
    {
      "epoch": 0.49830163629153235,
      "grad_norm": 0.11020947247743607,
      "learning_rate": 1.800686242391848e-05,
      "loss": 0.6305,
      "step": 28900
    },
    {
      "epoch": 0.5000258634067279,
      "grad_norm": 0.05582483112812042,
      "learning_rate": 1.7999965515457697e-05,
      "loss": 0.4441,
      "step": 29000
    },
    {
      "epoch": 0.5017500905219235,
      "grad_norm": 0.39946863055229187,
      "learning_rate": 1.7993068606996917e-05,
      "loss": 0.5188,
      "step": 29100
    },
    {
      "epoch": 0.5034743176371191,
      "grad_norm": 0.09437830746173859,
      "learning_rate": 1.7986171698536133e-05,
      "loss": 0.3813,
      "step": 29200
    },
    {
      "epoch": 0.5051985447523147,
      "grad_norm": 0.09326976537704468,
      "learning_rate": 1.797927479007535e-05,
      "loss": 0.3764,
      "step": 29300
    },
    {
      "epoch": 0.5069227718675103,
      "grad_norm": 0.01812480017542839,
      "learning_rate": 1.7972377881614566e-05,
      "loss": 0.3779,
      "step": 29400
    },
    {
      "epoch": 0.508646998982706,
      "grad_norm": 0.23909729719161987,
      "learning_rate": 1.7965480973153786e-05,
      "loss": 0.5334,
      "step": 29500
    },
    {
      "epoch": 0.5103712260979016,
      "grad_norm": 0.15587085485458374,
      "learning_rate": 1.7958584064693003e-05,
      "loss": 0.4452,
      "step": 29600
    },
    {
      "epoch": 0.5120954532130972,
      "grad_norm": 0.1974819004535675,
      "learning_rate": 1.795168715623222e-05,
      "loss": 0.4112,
      "step": 29700
    },
    {
      "epoch": 0.5138196803282928,
      "grad_norm": 0.09271617233753204,
      "learning_rate": 1.794479024777144e-05,
      "loss": 0.317,
      "step": 29800
    },
    {
      "epoch": 0.5155439074434884,
      "grad_norm": 0.12480194121599197,
      "learning_rate": 1.7937893339310656e-05,
      "loss": 0.5625,
      "step": 29900
    },
    {
      "epoch": 0.517268134558684,
      "grad_norm": 0.1445295512676239,
      "learning_rate": 1.7930996430849875e-05,
      "loss": 0.5644,
      "step": 30000
    },
    {
      "epoch": 0.5189923616738796,
      "grad_norm": 0.11141693592071533,
      "learning_rate": 1.7924099522389092e-05,
      "loss": 0.5701,
      "step": 30100
    },
    {
      "epoch": 0.5207165887890753,
      "grad_norm": 0.5084587931632996,
      "learning_rate": 1.791720261392831e-05,
      "loss": 0.6868,
      "step": 30200
    },
    {
      "epoch": 0.5224408159042709,
      "grad_norm": 0.06146326661109924,
      "learning_rate": 1.7910305705467525e-05,
      "loss": 0.3422,
      "step": 30300
    },
    {
      "epoch": 0.5241650430194665,
      "grad_norm": 0.12873360514640808,
      "learning_rate": 1.790340879700674e-05,
      "loss": 0.488,
      "step": 30400
    },
    {
      "epoch": 0.5258892701346621,
      "grad_norm": 0.030528143048286438,
      "learning_rate": 1.789651188854596e-05,
      "loss": 0.2952,
      "step": 30500
    },
    {
      "epoch": 0.5276134972498577,
      "grad_norm": 0.08936796337366104,
      "learning_rate": 1.7889614980085178e-05,
      "loss": 0.4333,
      "step": 30600
    },
    {
      "epoch": 0.5293377243650533,
      "grad_norm": 0.022053321823477745,
      "learning_rate": 1.7882718071624398e-05,
      "loss": 0.5315,
      "step": 30700
    },
    {
      "epoch": 0.5310619514802489,
      "grad_norm": 0.10051209479570389,
      "learning_rate": 1.7875821163163614e-05,
      "loss": 0.6331,
      "step": 30800
    },
    {
      "epoch": 0.5327861785954446,
      "grad_norm": 17.50245475769043,
      "learning_rate": 1.786892425470283e-05,
      "loss": 0.4781,
      "step": 30900
    },
    {
      "epoch": 0.5345104057106402,
      "grad_norm": 0.0644419714808464,
      "learning_rate": 1.7862027346242047e-05,
      "loss": 0.4739,
      "step": 31000
    },
    {
      "epoch": 0.5362346328258358,
      "grad_norm": 0.2069058120250702,
      "learning_rate": 1.7855130437781264e-05,
      "loss": 0.4277,
      "step": 31100
    },
    {
      "epoch": 0.5379588599410314,
      "grad_norm": 0.05845390260219574,
      "learning_rate": 1.7848233529320484e-05,
      "loss": 0.5689,
      "step": 31200
    },
    {
      "epoch": 0.539683087056227,
      "grad_norm": 0.058193087577819824,
      "learning_rate": 1.78413366208597e-05,
      "loss": 0.7278,
      "step": 31300
    },
    {
      "epoch": 0.5414073141714226,
      "grad_norm": 0.15114225447177887,
      "learning_rate": 1.783443971239892e-05,
      "loss": 0.5694,
      "step": 31400
    },
    {
      "epoch": 0.5431315412866182,
      "grad_norm": 28.132503509521484,
      "learning_rate": 1.7827542803938137e-05,
      "loss": 0.5361,
      "step": 31500
    },
    {
      "epoch": 0.5448557684018139,
      "grad_norm": 0.3674440383911133,
      "learning_rate": 1.7820645895477353e-05,
      "loss": 0.364,
      "step": 31600
    },
    {
      "epoch": 0.5465799955170095,
      "grad_norm": 0.07903497666120529,
      "learning_rate": 1.781374898701657e-05,
      "loss": 0.5981,
      "step": 31700
    },
    {
      "epoch": 0.5483042226322051,
      "grad_norm": 24.08686637878418,
      "learning_rate": 1.780685207855579e-05,
      "loss": 0.5209,
      "step": 31800
    },
    {
      "epoch": 0.5500284497474007,
      "grad_norm": 0.05121077969670296,
      "learning_rate": 1.7799955170095006e-05,
      "loss": 0.4896,
      "step": 31900
    },
    {
      "epoch": 0.5517526768625963,
      "grad_norm": 0.1595834344625473,
      "learning_rate": 1.7793058261634223e-05,
      "loss": 0.507,
      "step": 32000
    },
    {
      "epoch": 0.5534769039777919,
      "grad_norm": 0.17320974171161652,
      "learning_rate": 1.7786161353173443e-05,
      "loss": 0.4561,
      "step": 32100
    },
    {
      "epoch": 0.5552011310929875,
      "grad_norm": 0.04621662199497223,
      "learning_rate": 1.777926444471266e-05,
      "loss": 0.4749,
      "step": 32200
    },
    {
      "epoch": 0.5569253582081832,
      "grad_norm": 0.03460100293159485,
      "learning_rate": 1.7772367536251876e-05,
      "loss": 0.4,
      "step": 32300
    },
    {
      "epoch": 0.5586495853233788,
      "grad_norm": 0.08335381001234055,
      "learning_rate": 1.7765470627791096e-05,
      "loss": 0.5528,
      "step": 32400
    },
    {
      "epoch": 0.5603738124385744,
      "grad_norm": 0.14213328063488007,
      "learning_rate": 1.7758573719330312e-05,
      "loss": 0.6689,
      "step": 32500
    },
    {
      "epoch": 0.56209803955377,
      "grad_norm": 0.22054199874401093,
      "learning_rate": 1.775167681086953e-05,
      "loss": 0.6875,
      "step": 32600
    },
    {
      "epoch": 0.5638222666689656,
      "grad_norm": 0.41624191403388977,
      "learning_rate": 1.7744779902408745e-05,
      "loss": 0.4694,
      "step": 32700
    },
    {
      "epoch": 0.5655464937841612,
      "grad_norm": 0.13909998536109924,
      "learning_rate": 1.7737882993947965e-05,
      "loss": 0.7496,
      "step": 32800
    },
    {
      "epoch": 0.5672707208993568,
      "grad_norm": 0.2613110840320587,
      "learning_rate": 1.773098608548718e-05,
      "loss": 0.5559,
      "step": 32900
    },
    {
      "epoch": 0.5689949480145525,
      "grad_norm": 0.4154682159423828,
      "learning_rate": 1.77240891770264e-05,
      "loss": 0.5723,
      "step": 33000
    },
    {
      "epoch": 0.5707191751297481,
      "grad_norm": 0.18807139992713928,
      "learning_rate": 1.7717192268565618e-05,
      "loss": 0.6145,
      "step": 33100
    },
    {
      "epoch": 0.5724434022449437,
      "grad_norm": 0.1523434817790985,
      "learning_rate": 1.7710295360104834e-05,
      "loss": 0.3589,
      "step": 33200
    },
    {
      "epoch": 0.5741676293601393,
      "grad_norm": 0.4747079312801361,
      "learning_rate": 1.770339845164405e-05,
      "loss": 0.6086,
      "step": 33300
    },
    {
      "epoch": 0.5758918564753349,
      "grad_norm": 0.2302018254995346,
      "learning_rate": 1.7696501543183268e-05,
      "loss": 0.5129,
      "step": 33400
    },
    {
      "epoch": 0.5776160835905305,
      "grad_norm": 0.3484281599521637,
      "learning_rate": 1.7689604634722487e-05,
      "loss": 0.3605,
      "step": 33500
    },
    {
      "epoch": 0.5793403107057261,
      "grad_norm": 0.06892083585262299,
      "learning_rate": 1.7682707726261704e-05,
      "loss": 0.3783,
      "step": 33600
    },
    {
      "epoch": 0.5810645378209218,
      "grad_norm": 0.07842915505170822,
      "learning_rate": 1.7675810817800924e-05,
      "loss": 0.2815,
      "step": 33700
    },
    {
      "epoch": 0.5827887649361174,
      "grad_norm": 0.13255064189434052,
      "learning_rate": 1.766891390934014e-05,
      "loss": 0.6133,
      "step": 33800
    },
    {
      "epoch": 0.584512992051313,
      "grad_norm": 0.21469037234783173,
      "learning_rate": 1.7662017000879357e-05,
      "loss": 0.5543,
      "step": 33900
    },
    {
      "epoch": 0.5862372191665086,
      "grad_norm": 0.3935788869857788,
      "learning_rate": 1.7655120092418573e-05,
      "loss": 0.563,
      "step": 34000
    },
    {
      "epoch": 0.5879614462817042,
      "grad_norm": 0.4500136077404022,
      "learning_rate": 1.7648223183957793e-05,
      "loss": 0.6357,
      "step": 34100
    },
    {
      "epoch": 0.5896856733968998,
      "grad_norm": 0.2968722879886627,
      "learning_rate": 1.764132627549701e-05,
      "loss": 0.4775,
      "step": 34200
    },
    {
      "epoch": 0.5914099005120954,
      "grad_norm": 21.010705947875977,
      "learning_rate": 1.7634429367036226e-05,
      "loss": 0.5412,
      "step": 34300
    },
    {
      "epoch": 0.5931341276272911,
      "grad_norm": 0.2943432927131653,
      "learning_rate": 1.7627532458575446e-05,
      "loss": 0.4129,
      "step": 34400
    },
    {
      "epoch": 0.5948583547424867,
      "grad_norm": 0.18651866912841797,
      "learning_rate": 1.7620635550114663e-05,
      "loss": 0.5214,
      "step": 34500
    },
    {
      "epoch": 0.5965825818576823,
      "grad_norm": 20.224822998046875,
      "learning_rate": 1.761373864165388e-05,
      "loss": 0.6087,
      "step": 34600
    },
    {
      "epoch": 0.5983068089728779,
      "grad_norm": 0.2638995051383972,
      "learning_rate": 1.76068417331931e-05,
      "loss": 0.4986,
      "step": 34700
    },
    {
      "epoch": 0.6000310360880735,
      "grad_norm": 0.04352157562971115,
      "learning_rate": 1.7599944824732316e-05,
      "loss": 0.6064,
      "step": 34800
    },
    {
      "epoch": 0.6017552632032691,
      "grad_norm": 0.1600959300994873,
      "learning_rate": 1.7593047916271532e-05,
      "loss": 0.5389,
      "step": 34900
    },
    {
      "epoch": 0.6034794903184647,
      "grad_norm": 2.7562002742342884e-06,
      "learning_rate": 1.758615100781075e-05,
      "loss": 0.5923,
      "step": 35000
    },
    {
      "epoch": 0.6052037174336604,
      "grad_norm": 0.26151901483535767,
      "learning_rate": 1.757925409934997e-05,
      "loss": 0.6105,
      "step": 35100
    },
    {
      "epoch": 0.606927944548856,
      "grad_norm": 0.2983468472957611,
      "learning_rate": 1.7572357190889185e-05,
      "loss": 0.549,
      "step": 35200
    },
    {
      "epoch": 0.6086521716640516,
      "grad_norm": 0.179684117436409,
      "learning_rate": 1.7565460282428405e-05,
      "loss": 0.5307,
      "step": 35300
    },
    {
      "epoch": 0.6103763987792472,
      "grad_norm": 0.12416720390319824,
      "learning_rate": 1.755856337396762e-05,
      "loss": 0.4116,
      "step": 35400
    },
    {
      "epoch": 0.6121006258944428,
      "grad_norm": 23.70057487487793,
      "learning_rate": 1.7551666465506838e-05,
      "loss": 0.3211,
      "step": 35500
    },
    {
      "epoch": 0.6138248530096384,
      "grad_norm": 0.07559359818696976,
      "learning_rate": 1.7544769557046055e-05,
      "loss": 0.4697,
      "step": 35600
    },
    {
      "epoch": 0.615549080124834,
      "grad_norm": 23.391864776611328,
      "learning_rate": 1.753787264858527e-05,
      "loss": 0.6614,
      "step": 35700
    },
    {
      "epoch": 0.6172733072400297,
      "grad_norm": 0.16336730122566223,
      "learning_rate": 1.753097574012449e-05,
      "loss": 0.3736,
      "step": 35800
    },
    {
      "epoch": 0.6189975343552253,
      "grad_norm": 0.2173546999692917,
      "learning_rate": 1.7524078831663708e-05,
      "loss": 0.4378,
      "step": 35900
    },
    {
      "epoch": 0.6207217614704209,
      "grad_norm": 0.04368114471435547,
      "learning_rate": 1.7517181923202927e-05,
      "loss": 0.566,
      "step": 36000
    },
    {
      "epoch": 0.6224459885856165,
      "grad_norm": 0.08931086212396622,
      "learning_rate": 1.7510285014742144e-05,
      "loss": 0.6393,
      "step": 36100
    },
    {
      "epoch": 0.6241702157008121,
      "grad_norm": 0.38507509231567383,
      "learning_rate": 1.750338810628136e-05,
      "loss": 0.5832,
      "step": 36200
    },
    {
      "epoch": 0.6258944428160077,
      "grad_norm": 0.2680633068084717,
      "learning_rate": 1.7496491197820577e-05,
      "loss": 0.542,
      "step": 36300
    },
    {
      "epoch": 0.6276186699312033,
      "grad_norm": 32.81857681274414,
      "learning_rate": 1.7489594289359797e-05,
      "loss": 0.4257,
      "step": 36400
    },
    {
      "epoch": 0.6293428970463989,
      "grad_norm": 0.06721113622188568,
      "learning_rate": 1.7482697380899013e-05,
      "loss": 0.4466,
      "step": 36500
    },
    {
      "epoch": 0.6310671241615946,
      "grad_norm": 27.993547439575195,
      "learning_rate": 1.747580047243823e-05,
      "loss": 0.5896,
      "step": 36600
    },
    {
      "epoch": 0.6327913512767902,
      "grad_norm": 25.98080825805664,
      "learning_rate": 1.746890356397745e-05,
      "loss": 0.5021,
      "step": 36700
    },
    {
      "epoch": 0.6345155783919858,
      "grad_norm": 0.03960617631673813,
      "learning_rate": 1.7462006655516666e-05,
      "loss": 0.2939,
      "step": 36800
    },
    {
      "epoch": 0.6362398055071814,
      "grad_norm": 0.049085237085819244,
      "learning_rate": 1.7455109747055883e-05,
      "loss": 0.5493,
      "step": 36900
    },
    {
      "epoch": 0.637964032622377,
      "grad_norm": 0.22246839106082916,
      "learning_rate": 1.7448212838595103e-05,
      "loss": 0.3407,
      "step": 37000
    },
    {
      "epoch": 0.6396882597375726,
      "grad_norm": 0.12263381481170654,
      "learning_rate": 1.744131593013432e-05,
      "loss": 0.515,
      "step": 37100
    },
    {
      "epoch": 0.6414124868527682,
      "grad_norm": 0.10271012037992477,
      "learning_rate": 1.7434419021673536e-05,
      "loss": 0.4405,
      "step": 37200
    },
    {
      "epoch": 0.6431367139679639,
      "grad_norm": 20.060806274414062,
      "learning_rate": 1.7427522113212752e-05,
      "loss": 0.6055,
      "step": 37300
    },
    {
      "epoch": 0.6448609410831595,
      "grad_norm": 0.08985268324613571,
      "learning_rate": 1.7420625204751972e-05,
      "loss": 0.3471,
      "step": 37400
    },
    {
      "epoch": 0.6465851681983551,
      "grad_norm": 3.879486939695198e-06,
      "learning_rate": 1.741372829629119e-05,
      "loss": 0.3927,
      "step": 37500
    },
    {
      "epoch": 0.6483093953135507,
      "grad_norm": 0.2993517816066742,
      "learning_rate": 1.740683138783041e-05,
      "loss": 0.5049,
      "step": 37600
    },
    {
      "epoch": 0.6500336224287463,
      "grad_norm": 0.06862501800060272,
      "learning_rate": 1.7399934479369625e-05,
      "loss": 0.4458,
      "step": 37700
    },
    {
      "epoch": 0.6517578495439419,
      "grad_norm": 0.028160318732261658,
      "learning_rate": 1.739303757090884e-05,
      "loss": 0.4342,
      "step": 37800
    },
    {
      "epoch": 0.6534820766591375,
      "grad_norm": 33.17593002319336,
      "learning_rate": 1.7386140662448058e-05,
      "loss": 0.5572,
      "step": 37900
    },
    {
      "epoch": 0.6552063037743332,
      "grad_norm": 0.2852696478366852,
      "learning_rate": 1.7379243753987275e-05,
      "loss": 0.5489,
      "step": 38000
    },
    {
      "epoch": 0.6569305308895288,
      "grad_norm": 0.0499105304479599,
      "learning_rate": 1.7372346845526495e-05,
      "loss": 0.5267,
      "step": 38100
    },
    {
      "epoch": 0.6586547580047244,
      "grad_norm": 0.07842959463596344,
      "learning_rate": 1.736544993706571e-05,
      "loss": 0.6497,
      "step": 38200
    },
    {
      "epoch": 0.66037898511992,
      "grad_norm": 0.14999330043792725,
      "learning_rate": 1.735855302860493e-05,
      "loss": 0.5588,
      "step": 38300
    },
    {
      "epoch": 0.6621032122351156,
      "grad_norm": 0.13267293572425842,
      "learning_rate": 1.7351656120144148e-05,
      "loss": 0.4725,
      "step": 38400
    },
    {
      "epoch": 0.6638274393503112,
      "grad_norm": 30.06708526611328,
      "learning_rate": 1.7344759211683364e-05,
      "loss": 0.3258,
      "step": 38500
    },
    {
      "epoch": 0.6655516664655068,
      "grad_norm": 0.06636353582143784,
      "learning_rate": 1.733786230322258e-05,
      "loss": 0.4795,
      "step": 38600
    },
    {
      "epoch": 0.6672758935807025,
      "grad_norm": 0.08903754502534866,
      "learning_rate": 1.7330965394761797e-05,
      "loss": 0.4965,
      "step": 38700
    },
    {
      "epoch": 0.6690001206958981,
      "grad_norm": 20.79509925842285,
      "learning_rate": 1.7324068486301017e-05,
      "loss": 0.5401,
      "step": 38800
    },
    {
      "epoch": 0.6707243478110937,
      "grad_norm": 7.123867362679448e-07,
      "learning_rate": 1.7317171577840233e-05,
      "loss": 0.3312,
      "step": 38900
    },
    {
      "epoch": 0.6724485749262893,
      "grad_norm": 0.10268612951040268,
      "learning_rate": 1.7310274669379453e-05,
      "loss": 0.6234,
      "step": 39000
    },
    {
      "epoch": 0.6741728020414849,
      "grad_norm": 0.11532030999660492,
      "learning_rate": 1.730337776091867e-05,
      "loss": 0.4,
      "step": 39100
    },
    {
      "epoch": 0.6758970291566805,
      "grad_norm": 0.02244672179222107,
      "learning_rate": 1.7296480852457886e-05,
      "loss": 0.3854,
      "step": 39200
    },
    {
      "epoch": 0.6776212562718761,
      "grad_norm": 0.3480129837989807,
      "learning_rate": 1.7289583943997103e-05,
      "loss": 0.6477,
      "step": 39300
    },
    {
      "epoch": 0.6793454833870718,
      "grad_norm": 0.186749666929245,
      "learning_rate": 1.7282687035536323e-05,
      "loss": 0.4608,
      "step": 39400
    },
    {
      "epoch": 0.6810697105022674,
      "grad_norm": 18.013519287109375,
      "learning_rate": 1.727579012707554e-05,
      "loss": 0.5064,
      "step": 39500
    },
    {
      "epoch": 0.682793937617463,
      "grad_norm": 0.7668301463127136,
      "learning_rate": 1.7268893218614756e-05,
      "loss": 0.7338,
      "step": 39600
    },
    {
      "epoch": 0.6845181647326586,
      "grad_norm": 0.0767727643251419,
      "learning_rate": 1.7261996310153976e-05,
      "loss": 0.4818,
      "step": 39700
    },
    {
      "epoch": 0.6862423918478542,
      "grad_norm": 0.10387322306632996,
      "learning_rate": 1.7255099401693192e-05,
      "loss": 0.5974,
      "step": 39800
    },
    {
      "epoch": 0.6879666189630498,
      "grad_norm": 0.1534455269575119,
      "learning_rate": 1.7248202493232412e-05,
      "loss": 0.588,
      "step": 39900
    },
    {
      "epoch": 0.6896908460782454,
      "grad_norm": 0.08985763788223267,
      "learning_rate": 1.724130558477163e-05,
      "loss": 0.3696,
      "step": 40000
    },
    {
      "epoch": 0.6914150731934411,
      "grad_norm": 0.30089071393013,
      "learning_rate": 1.7234408676310845e-05,
      "loss": 0.6364,
      "step": 40100
    },
    {
      "epoch": 0.6931393003086367,
      "grad_norm": 26.21833610534668,
      "learning_rate": 1.7227511767850062e-05,
      "loss": 0.6575,
      "step": 40200
    },
    {
      "epoch": 0.6948635274238323,
      "grad_norm": 0.13634754717350006,
      "learning_rate": 1.7220614859389278e-05,
      "loss": 0.7204,
      "step": 40300
    },
    {
      "epoch": 0.6965877545390279,
      "grad_norm": 0.3465701639652252,
      "learning_rate": 1.7213717950928498e-05,
      "loss": 0.3626,
      "step": 40400
    },
    {
      "epoch": 0.6983119816542235,
      "grad_norm": 0.2143261879682541,
      "learning_rate": 1.7206821042467715e-05,
      "loss": 0.7009,
      "step": 40500
    },
    {
      "epoch": 0.7000362087694191,
      "grad_norm": 0.0707826167345047,
      "learning_rate": 1.7199924134006935e-05,
      "loss": 0.4365,
      "step": 40600
    },
    {
      "epoch": 0.7017604358846147,
      "grad_norm": 0.08053423464298248,
      "learning_rate": 1.719302722554615e-05,
      "loss": 0.5489,
      "step": 40700
    },
    {
      "epoch": 0.7034846629998104,
      "grad_norm": 0.07061059772968292,
      "learning_rate": 1.7186130317085368e-05,
      "loss": 0.419,
      "step": 40800
    },
    {
      "epoch": 0.705208890115006,
      "grad_norm": 29.82541275024414,
      "learning_rate": 1.7179233408624584e-05,
      "loss": 0.4626,
      "step": 40900
    },
    {
      "epoch": 0.7069331172302016,
      "grad_norm": 0.034081362187862396,
      "learning_rate": 1.71723365001638e-05,
      "loss": 0.2992,
      "step": 41000
    },
    {
      "epoch": 0.7086573443453972,
      "grad_norm": 0.07142618298530579,
      "learning_rate": 1.716543959170302e-05,
      "loss": 0.6096,
      "step": 41100
    },
    {
      "epoch": 0.7103815714605928,
      "grad_norm": 0.14795705676078796,
      "learning_rate": 1.7158542683242237e-05,
      "loss": 0.5455,
      "step": 41200
    },
    {
      "epoch": 0.7121057985757884,
      "grad_norm": 0.12343460321426392,
      "learning_rate": 1.7151645774781457e-05,
      "loss": 0.5835,
      "step": 41300
    },
    {
      "epoch": 0.713830025690984,
      "grad_norm": 0.09681017696857452,
      "learning_rate": 1.7144748866320674e-05,
      "loss": 0.6561,
      "step": 41400
    },
    {
      "epoch": 0.7155542528061797,
      "grad_norm": 0.2253541797399521,
      "learning_rate": 1.713785195785989e-05,
      "loss": 0.4747,
      "step": 41500
    },
    {
      "epoch": 0.7172784799213753,
      "grad_norm": 0.14815114438533783,
      "learning_rate": 1.7130955049399107e-05,
      "loss": 0.4319,
      "step": 41600
    },
    {
      "epoch": 0.7190027070365709,
      "grad_norm": 0.18204167485237122,
      "learning_rate": 1.7124058140938326e-05,
      "loss": 0.4325,
      "step": 41700
    },
    {
      "epoch": 0.7207269341517665,
      "grad_norm": 0.24914394319057465,
      "learning_rate": 1.7117161232477543e-05,
      "loss": 0.4641,
      "step": 41800
    },
    {
      "epoch": 0.7224511612669621,
      "grad_norm": 0.1307772397994995,
      "learning_rate": 1.711026432401676e-05,
      "loss": 0.5258,
      "step": 41900
    },
    {
      "epoch": 0.7241753883821577,
      "grad_norm": 0.18512091040611267,
      "learning_rate": 1.710336741555598e-05,
      "loss": 0.6283,
      "step": 42000
    },
    {
      "epoch": 0.7258996154973533,
      "grad_norm": 0.1568540334701538,
      "learning_rate": 1.7096470507095196e-05,
      "loss": 0.4817,
      "step": 42100
    },
    {
      "epoch": 0.727623842612549,
      "grad_norm": 0.31244176626205444,
      "learning_rate": 1.7089573598634412e-05,
      "loss": 0.4383,
      "step": 42200
    },
    {
      "epoch": 0.7293480697277446,
      "grad_norm": 27.507783889770508,
      "learning_rate": 1.7082676690173632e-05,
      "loss": 0.4133,
      "step": 42300
    },
    {
      "epoch": 0.7310722968429402,
      "grad_norm": 0.042490534484386444,
      "learning_rate": 1.707577978171285e-05,
      "loss": 0.5598,
      "step": 42400
    },
    {
      "epoch": 0.7327965239581358,
      "grad_norm": 23.898740768432617,
      "learning_rate": 1.7068882873252065e-05,
      "loss": 0.5462,
      "step": 42500
    },
    {
      "epoch": 0.7345207510733314,
      "grad_norm": 0.12669087946414948,
      "learning_rate": 1.7061985964791282e-05,
      "loss": 0.4585,
      "step": 42600
    },
    {
      "epoch": 0.736244978188527,
      "grad_norm": 0.3518564999103546,
      "learning_rate": 1.7055089056330502e-05,
      "loss": 0.4151,
      "step": 42700
    },
    {
      "epoch": 0.7379692053037226,
      "grad_norm": 0.12086038291454315,
      "learning_rate": 1.7048192147869718e-05,
      "loss": 0.6802,
      "step": 42800
    },
    {
      "epoch": 0.7396934324189183,
      "grad_norm": 0.5407415628433228,
      "learning_rate": 1.7041295239408938e-05,
      "loss": 0.5937,
      "step": 42900
    },
    {
      "epoch": 0.7414176595341139,
      "grad_norm": 0.0653611272573471,
      "learning_rate": 1.7034398330948155e-05,
      "loss": 0.4756,
      "step": 43000
    },
    {
      "epoch": 0.7431418866493095,
      "grad_norm": 0.14675915241241455,
      "learning_rate": 1.702750142248737e-05,
      "loss": 0.6217,
      "step": 43100
    },
    {
      "epoch": 0.7448661137645051,
      "grad_norm": 0.1339825987815857,
      "learning_rate": 1.7020604514026588e-05,
      "loss": 0.2459,
      "step": 43200
    },
    {
      "epoch": 0.7465903408797007,
      "grad_norm": 0.15624022483825684,
      "learning_rate": 1.7013707605565804e-05,
      "loss": 0.6831,
      "step": 43300
    },
    {
      "epoch": 0.7483145679948963,
      "grad_norm": 0.2050274759531021,
      "learning_rate": 1.7006810697105024e-05,
      "loss": 0.3713,
      "step": 43400
    },
    {
      "epoch": 0.7500387951100919,
      "grad_norm": 25.458343505859375,
      "learning_rate": 1.699991378864424e-05,
      "loss": 0.588,
      "step": 43500
    },
    {
      "epoch": 0.7517630222252876,
      "grad_norm": 0.45062509179115295,
      "learning_rate": 1.699301688018346e-05,
      "loss": 0.5937,
      "step": 43600
    },
    {
      "epoch": 0.7534872493404832,
      "grad_norm": 0.2114187777042389,
      "learning_rate": 1.6986119971722677e-05,
      "loss": 0.6287,
      "step": 43700
    },
    {
      "epoch": 0.7552114764556788,
      "grad_norm": 0.23925496637821198,
      "learning_rate": 1.6979223063261894e-05,
      "loss": 0.237,
      "step": 43800
    },
    {
      "epoch": 0.7569357035708744,
      "grad_norm": 0.14161041378974915,
      "learning_rate": 1.697232615480111e-05,
      "loss": 0.7198,
      "step": 43900
    },
    {
      "epoch": 0.75865993068607,
      "grad_norm": 0.16269129514694214,
      "learning_rate": 1.696542924634033e-05,
      "loss": 0.5891,
      "step": 44000
    },
    {
      "epoch": 0.7603841578012656,
      "grad_norm": 30.31646728515625,
      "learning_rate": 1.6958532337879547e-05,
      "loss": 0.4266,
      "step": 44100
    },
    {
      "epoch": 0.7621083849164612,
      "grad_norm": 25.847043991088867,
      "learning_rate": 1.6951635429418763e-05,
      "loss": 0.7559,
      "step": 44200
    },
    {
      "epoch": 0.7638326120316568,
      "grad_norm": 0.13221308588981628,
      "learning_rate": 1.6944738520957983e-05,
      "loss": 0.33,
      "step": 44300
    },
    {
      "epoch": 0.7655568391468525,
      "grad_norm": 0.2597128450870514,
      "learning_rate": 1.69378416124972e-05,
      "loss": 0.3885,
      "step": 44400
    },
    {
      "epoch": 0.7672810662620481,
      "grad_norm": 28.783205032348633,
      "learning_rate": 1.6930944704036416e-05,
      "loss": 0.5576,
      "step": 44500
    },
    {
      "epoch": 0.7690052933772437,
      "grad_norm": 0.10862461477518082,
      "learning_rate": 1.6924047795575636e-05,
      "loss": 0.4542,
      "step": 44600
    },
    {
      "epoch": 0.7707295204924393,
      "grad_norm": 0.3409329056739807,
      "learning_rate": 1.6917150887114852e-05,
      "loss": 0.354,
      "step": 44700
    },
    {
      "epoch": 0.7724537476076349,
      "grad_norm": 0.06319492310285568,
      "learning_rate": 1.691025397865407e-05,
      "loss": 0.421,
      "step": 44800
    },
    {
      "epoch": 0.7741779747228305,
      "grad_norm": 0.09569231420755386,
      "learning_rate": 1.6903357070193285e-05,
      "loss": 0.5846,
      "step": 44900
    },
    {
      "epoch": 0.775902201838026,
      "grad_norm": 0.09373044222593307,
      "learning_rate": 1.6896460161732505e-05,
      "loss": 0.5471,
      "step": 45000
    },
    {
      "epoch": 0.7776264289532218,
      "grad_norm": 0.25782662630081177,
      "learning_rate": 1.6889563253271722e-05,
      "loss": 0.5554,
      "step": 45100
    },
    {
      "epoch": 0.7793506560684174,
      "grad_norm": 27.247478485107422,
      "learning_rate": 1.6882666344810942e-05,
      "loss": 0.3513,
      "step": 45200
    },
    {
      "epoch": 0.781074883183613,
      "grad_norm": 0.08146336674690247,
      "learning_rate": 1.6875769436350158e-05,
      "loss": 0.6554,
      "step": 45300
    },
    {
      "epoch": 0.7827991102988086,
      "grad_norm": 0.04601597413420677,
      "learning_rate": 1.6868872527889375e-05,
      "loss": 0.3972,
      "step": 45400
    },
    {
      "epoch": 0.7845233374140042,
      "grad_norm": 0.10835941880941391,
      "learning_rate": 1.686197561942859e-05,
      "loss": 0.4195,
      "step": 45500
    },
    {
      "epoch": 0.7862475645291997,
      "grad_norm": 0.07601601630449295,
      "learning_rate": 1.6855078710967808e-05,
      "loss": 0.5694,
      "step": 45600
    },
    {
      "epoch": 0.7879717916443953,
      "grad_norm": 0.06516581028699875,
      "learning_rate": 1.6848181802507028e-05,
      "loss": 0.4607,
      "step": 45700
    },
    {
      "epoch": 0.7896960187595911,
      "grad_norm": 0.1295807808637619,
      "learning_rate": 1.6841284894046244e-05,
      "loss": 0.6022,
      "step": 45800
    },
    {
      "epoch": 0.7914202458747867,
      "grad_norm": 0.07963003218173981,
      "learning_rate": 1.6834387985585464e-05,
      "loss": 0.4534,
      "step": 45900
    },
    {
      "epoch": 0.7931444729899823,
      "grad_norm": 23.20067024230957,
      "learning_rate": 1.682749107712468e-05,
      "loss": 0.3061,
      "step": 46000
    },
    {
      "epoch": 0.7948687001051779,
      "grad_norm": 0.03897693008184433,
      "learning_rate": 1.6820594168663897e-05,
      "loss": 0.341,
      "step": 46100
    },
    {
      "epoch": 0.7965929272203734,
      "grad_norm": 0.1228659600019455,
      "learning_rate": 1.6813697260203114e-05,
      "loss": 0.6761,
      "step": 46200
    },
    {
      "epoch": 0.798317154335569,
      "grad_norm": 0.01803015172481537,
      "learning_rate": 1.6806800351742334e-05,
      "loss": 0.5698,
      "step": 46300
    },
    {
      "epoch": 0.8000413814507646,
      "grad_norm": 0.08721530437469482,
      "learning_rate": 1.679990344328155e-05,
      "loss": 0.2983,
      "step": 46400
    },
    {
      "epoch": 0.8017656085659604,
      "grad_norm": 8.684884846843488e-07,
      "learning_rate": 1.6793006534820767e-05,
      "loss": 0.5493,
      "step": 46500
    },
    {
      "epoch": 0.803489835681156,
      "grad_norm": 0.3076629936695099,
      "learning_rate": 1.6786109626359987e-05,
      "loss": 0.6947,
      "step": 46600
    },
    {
      "epoch": 0.8052140627963515,
      "grad_norm": 28.1126651763916,
      "learning_rate": 1.6779212717899203e-05,
      "loss": 0.455,
      "step": 46700
    },
    {
      "epoch": 0.8069382899115471,
      "grad_norm": 29.02652359008789,
      "learning_rate": 1.677231580943842e-05,
      "loss": 0.7334,
      "step": 46800
    },
    {
      "epoch": 0.8086625170267427,
      "grad_norm": 22.182008743286133,
      "learning_rate": 1.676541890097764e-05,
      "loss": 0.4915,
      "step": 46900
    },
    {
      "epoch": 0.8103867441419383,
      "grad_norm": 0.4536113739013672,
      "learning_rate": 1.6758521992516856e-05,
      "loss": 0.5388,
      "step": 47000
    },
    {
      "epoch": 0.8121109712571339,
      "grad_norm": 0.1286800652742386,
      "learning_rate": 1.6751625084056073e-05,
      "loss": 0.4046,
      "step": 47100
    },
    {
      "epoch": 0.8138351983723296,
      "grad_norm": 0.2111656814813614,
      "learning_rate": 1.674472817559529e-05,
      "loss": 0.4728,
      "step": 47200
    },
    {
      "epoch": 0.8155594254875252,
      "grad_norm": 0.3588111102581024,
      "learning_rate": 1.673783126713451e-05,
      "loss": 0.3855,
      "step": 47300
    },
    {
      "epoch": 0.8172836526027208,
      "grad_norm": 0.16768741607666016,
      "learning_rate": 1.6730934358673725e-05,
      "loss": 0.5509,
      "step": 47400
    },
    {
      "epoch": 0.8190078797179164,
      "grad_norm": 0.1988537460565567,
      "learning_rate": 1.6724037450212945e-05,
      "loss": 0.3319,
      "step": 47500
    },
    {
      "epoch": 0.820732106833112,
      "grad_norm": 0.07603830099105835,
      "learning_rate": 1.6717140541752162e-05,
      "loss": 0.4277,
      "step": 47600
    },
    {
      "epoch": 0.8224563339483076,
      "grad_norm": 0.07465258985757828,
      "learning_rate": 1.671024363329138e-05,
      "loss": 0.5263,
      "step": 47700
    },
    {
      "epoch": 0.8241805610635032,
      "grad_norm": 0.11527217924594879,
      "learning_rate": 1.6703346724830595e-05,
      "loss": 0.5724,
      "step": 47800
    },
    {
      "epoch": 0.825904788178699,
      "grad_norm": 27.92902374267578,
      "learning_rate": 1.669644981636981e-05,
      "loss": 0.5343,
      "step": 47900
    },
    {
      "epoch": 0.8276290152938945,
      "grad_norm": 31.024404525756836,
      "learning_rate": 1.668955290790903e-05,
      "loss": 0.6081,
      "step": 48000
    },
    {
      "epoch": 0.8293532424090901,
      "grad_norm": 0.06732353568077087,
      "learning_rate": 1.668265599944825e-05,
      "loss": 0.3243,
      "step": 48100
    },
    {
      "epoch": 0.8310774695242857,
      "grad_norm": 28.31081771850586,
      "learning_rate": 1.6675759090987468e-05,
      "loss": 0.6722,
      "step": 48200
    },
    {
      "epoch": 0.8328016966394813,
      "grad_norm": 0.12438949942588806,
      "learning_rate": 1.6668862182526684e-05,
      "loss": 0.4944,
      "step": 48300
    },
    {
      "epoch": 0.8345259237546769,
      "grad_norm": 0.06058720126748085,
      "learning_rate": 1.66619652740659e-05,
      "loss": 0.6867,
      "step": 48400
    },
    {
      "epoch": 0.8362501508698725,
      "grad_norm": 0.28436875343322754,
      "learning_rate": 1.6655068365605117e-05,
      "loss": 0.2714,
      "step": 48500
    },
    {
      "epoch": 0.8379743779850682,
      "grad_norm": 0.07870497554540634,
      "learning_rate": 1.6648171457144334e-05,
      "loss": 0.2426,
      "step": 48600
    },
    {
      "epoch": 0.8396986051002638,
      "grad_norm": 27.861061096191406,
      "learning_rate": 1.6641274548683554e-05,
      "loss": 0.5535,
      "step": 48700
    },
    {
      "epoch": 0.8414228322154594,
      "grad_norm": 24.925989151000977,
      "learning_rate": 1.663437764022277e-05,
      "loss": 0.5202,
      "step": 48800
    },
    {
      "epoch": 0.843147059330655,
      "grad_norm": 0.17984916269779205,
      "learning_rate": 1.662748073176199e-05,
      "loss": 0.5238,
      "step": 48900
    },
    {
      "epoch": 0.8448712864458506,
      "grad_norm": 0.038962554186582565,
      "learning_rate": 1.6620583823301207e-05,
      "loss": 0.4643,
      "step": 49000
    },
    {
      "epoch": 0.8465955135610462,
      "grad_norm": 0.10539886355400085,
      "learning_rate": 1.6613686914840423e-05,
      "loss": 0.4195,
      "step": 49100
    },
    {
      "epoch": 0.8483197406762418,
      "grad_norm": 27.726337432861328,
      "learning_rate": 1.6606790006379643e-05,
      "loss": 0.7828,
      "step": 49200
    },
    {
      "epoch": 0.8500439677914375,
      "grad_norm": 0.07843204587697983,
      "learning_rate": 1.659989309791886e-05,
      "loss": 0.4874,
      "step": 49300
    },
    {
      "epoch": 0.8517681949066331,
      "grad_norm": 0.16566887497901917,
      "learning_rate": 1.6592996189458076e-05,
      "loss": 0.4526,
      "step": 49400
    },
    {
      "epoch": 0.8534924220218287,
      "grad_norm": 28.78953742980957,
      "learning_rate": 1.6586099280997293e-05,
      "loss": 0.7439,
      "step": 49500
    },
    {
      "epoch": 0.8552166491370243,
      "grad_norm": 28.118000030517578,
      "learning_rate": 1.6579202372536513e-05,
      "loss": 0.7684,
      "step": 49600
    },
    {
      "epoch": 0.8569408762522199,
      "grad_norm": 0.13446475565433502,
      "learning_rate": 1.657230546407573e-05,
      "loss": 0.379,
      "step": 49700
    },
    {
      "epoch": 0.8586651033674155,
      "grad_norm": 0.05649685859680176,
      "learning_rate": 1.656540855561495e-05,
      "loss": 0.4434,
      "step": 49800
    },
    {
      "epoch": 0.8603893304826111,
      "grad_norm": 0.05699421837925911,
      "learning_rate": 1.6558511647154165e-05,
      "loss": 0.4622,
      "step": 49900
    },
    {
      "epoch": 0.8621135575978068,
      "grad_norm": 0.10195595026016235,
      "learning_rate": 1.6551614738693382e-05,
      "loss": 0.2759,
      "step": 50000
    },
    {
      "epoch": 0.8638377847130024,
      "grad_norm": 0.13888965547084808,
      "learning_rate": 1.65447178302326e-05,
      "loss": 0.4578,
      "step": 50100
    },
    {
      "epoch": 0.865562011828198,
      "grad_norm": 0.13402274250984192,
      "learning_rate": 1.6537820921771815e-05,
      "loss": 0.4775,
      "step": 50200
    },
    {
      "epoch": 0.8672862389433936,
      "grad_norm": 0.1817633956670761,
      "learning_rate": 1.6530924013311035e-05,
      "loss": 0.4994,
      "step": 50300
    },
    {
      "epoch": 0.8690104660585892,
      "grad_norm": 0.2770613729953766,
      "learning_rate": 1.6524027104850255e-05,
      "loss": 0.4807,
      "step": 50400
    },
    {
      "epoch": 0.8707346931737848,
      "grad_norm": 27.60018539428711,
      "learning_rate": 1.651713019638947e-05,
      "loss": 0.4964,
      "step": 50500
    },
    {
      "epoch": 0.8724589202889804,
      "grad_norm": 0.29869017004966736,
      "learning_rate": 1.6510233287928688e-05,
      "loss": 0.4241,
      "step": 50600
    },
    {
      "epoch": 0.8741831474041761,
      "grad_norm": 0.049998972564935684,
      "learning_rate": 1.6503336379467904e-05,
      "loss": 0.4942,
      "step": 50700
    },
    {
      "epoch": 0.8759073745193717,
      "grad_norm": 0.4798005223274231,
      "learning_rate": 1.649643947100712e-05,
      "loss": 0.6478,
      "step": 50800
    },
    {
      "epoch": 0.8776316016345673,
      "grad_norm": 20.164655685424805,
      "learning_rate": 1.6489542562546337e-05,
      "loss": 0.5462,
      "step": 50900
    },
    {
      "epoch": 0.8793558287497629,
      "grad_norm": 0.22563962638378143,
      "learning_rate": 1.6482645654085557e-05,
      "loss": 0.6453,
      "step": 51000
    },
    {
      "epoch": 0.8810800558649585,
      "grad_norm": 0.05893518030643463,
      "learning_rate": 1.6475748745624777e-05,
      "loss": 0.4501,
      "step": 51100
    },
    {
      "epoch": 0.8828042829801541,
      "grad_norm": 0.1822548508644104,
      "learning_rate": 1.6468851837163994e-05,
      "loss": 0.3857,
      "step": 51200
    },
    {
      "epoch": 0.8845285100953497,
      "grad_norm": 28.00020408630371,
      "learning_rate": 1.646195492870321e-05,
      "loss": 0.6371,
      "step": 51300
    },
    {
      "epoch": 0.8862527372105454,
      "grad_norm": 27.077123641967773,
      "learning_rate": 1.6455058020242427e-05,
      "loss": 0.6792,
      "step": 51400
    },
    {
      "epoch": 0.887976964325741,
      "grad_norm": 0.09579111635684967,
      "learning_rate": 1.6448161111781643e-05,
      "loss": 0.6236,
      "step": 51500
    },
    {
      "epoch": 0.8897011914409366,
      "grad_norm": 0.11803150177001953,
      "learning_rate": 1.6441264203320863e-05,
      "loss": 0.5756,
      "step": 51600
    },
    {
      "epoch": 0.8914254185561322,
      "grad_norm": 0.16279076039791107,
      "learning_rate": 1.643436729486008e-05,
      "loss": 0.4366,
      "step": 51700
    },
    {
      "epoch": 0.8931496456713278,
      "grad_norm": 0.17437143623828888,
      "learning_rate": 1.64274703863993e-05,
      "loss": 0.4019,
      "step": 51800
    },
    {
      "epoch": 0.8948738727865234,
      "grad_norm": 0.18236757814884186,
      "learning_rate": 1.6420573477938516e-05,
      "loss": 0.5421,
      "step": 51900
    },
    {
      "epoch": 0.896598099901719,
      "grad_norm": 0.12238051742315292,
      "learning_rate": 1.6413676569477733e-05,
      "loss": 0.341,
      "step": 52000
    },
    {
      "epoch": 0.8983223270169147,
      "grad_norm": 0.21101416647434235,
      "learning_rate": 1.640677966101695e-05,
      "loss": 0.601,
      "step": 52100
    },
    {
      "epoch": 0.9000465541321103,
      "grad_norm": 0.1902400553226471,
      "learning_rate": 1.639988275255617e-05,
      "loss": 0.4413,
      "step": 52200
    },
    {
      "epoch": 0.9017707812473059,
      "grad_norm": 28.411664962768555,
      "learning_rate": 1.6392985844095386e-05,
      "loss": 0.4789,
      "step": 52300
    },
    {
      "epoch": 0.9034950083625015,
      "grad_norm": 0.17802217602729797,
      "learning_rate": 1.6386088935634602e-05,
      "loss": 0.6334,
      "step": 52400
    },
    {
      "epoch": 0.9052192354776971,
      "grad_norm": 0.13149495422840118,
      "learning_rate": 1.6379192027173822e-05,
      "loss": 0.3449,
      "step": 52500
    },
    {
      "epoch": 0.9069434625928927,
      "grad_norm": 0.33516427874565125,
      "learning_rate": 1.637229511871304e-05,
      "loss": 0.614,
      "step": 52600
    },
    {
      "epoch": 0.9086676897080883,
      "grad_norm": 0.15401096642017365,
      "learning_rate": 1.636539821025226e-05,
      "loss": 0.3596,
      "step": 52700
    },
    {
      "epoch": 0.9103919168232839,
      "grad_norm": 0.3040560483932495,
      "learning_rate": 1.6358501301791475e-05,
      "loss": 0.5337,
      "step": 52800
    },
    {
      "epoch": 0.9121161439384796,
      "grad_norm": 0.0496898889541626,
      "learning_rate": 1.635160439333069e-05,
      "loss": 0.4358,
      "step": 52900
    },
    {
      "epoch": 0.9138403710536752,
      "grad_norm": 0.09824712574481964,
      "learning_rate": 1.6344707484869908e-05,
      "loss": 0.3765,
      "step": 53000
    },
    {
      "epoch": 0.9155645981688708,
      "grad_norm": 0.2949424982070923,
      "learning_rate": 1.6337810576409124e-05,
      "loss": 0.7586,
      "step": 53100
    },
    {
      "epoch": 0.9172888252840664,
      "grad_norm": 0.07771145552396774,
      "learning_rate": 1.633091366794834e-05,
      "loss": 0.323,
      "step": 53200
    },
    {
      "epoch": 0.919013052399262,
      "grad_norm": 0.15537135303020477,
      "learning_rate": 1.632401675948756e-05,
      "loss": 0.6417,
      "step": 53300
    },
    {
      "epoch": 0.9207372795144576,
      "grad_norm": 0.061778802424669266,
      "learning_rate": 1.631711985102678e-05,
      "loss": 0.4469,
      "step": 53400
    },
    {
      "epoch": 0.9224615066296532,
      "grad_norm": 30.93906021118164,
      "learning_rate": 1.6310222942565997e-05,
      "loss": 0.398,
      "step": 53500
    },
    {
      "epoch": 0.9241857337448489,
      "grad_norm": 23.304000854492188,
      "learning_rate": 1.6303326034105214e-05,
      "loss": 0.5818,
      "step": 53600
    },
    {
      "epoch": 0.9259099608600445,
      "grad_norm": 0.14156849682331085,
      "learning_rate": 1.629642912564443e-05,
      "loss": 0.54,
      "step": 53700
    },
    {
      "epoch": 0.9276341879752401,
      "grad_norm": 0.09419809281826019,
      "learning_rate": 1.6289532217183647e-05,
      "loss": 0.3249,
      "step": 53800
    },
    {
      "epoch": 0.9293584150904357,
      "grad_norm": 0.13865818083286285,
      "learning_rate": 1.6282635308722867e-05,
      "loss": 0.4117,
      "step": 53900
    },
    {
      "epoch": 0.9310826422056313,
      "grad_norm": 0.13546478748321533,
      "learning_rate": 1.6275738400262083e-05,
      "loss": 0.6477,
      "step": 54000
    },
    {
      "epoch": 0.9328068693208269,
      "grad_norm": 26.719839096069336,
      "learning_rate": 1.6268841491801303e-05,
      "loss": 0.5233,
      "step": 54100
    },
    {
      "epoch": 0.9345310964360225,
      "grad_norm": 0.16840435564517975,
      "learning_rate": 1.626194458334052e-05,
      "loss": 0.5533,
      "step": 54200
    },
    {
      "epoch": 0.9362553235512182,
      "grad_norm": 0.04266444221138954,
      "learning_rate": 1.6255047674879736e-05,
      "loss": 0.404,
      "step": 54300
    },
    {
      "epoch": 0.9379795506664138,
      "grad_norm": 0.1875784546136856,
      "learning_rate": 1.6248150766418953e-05,
      "loss": 0.4187,
      "step": 54400
    },
    {
      "epoch": 0.9397037777816094,
      "grad_norm": 0.12576322257518768,
      "learning_rate": 1.6241253857958173e-05,
      "loss": 0.4163,
      "step": 54500
    },
    {
      "epoch": 0.941428004896805,
      "grad_norm": 0.049560051411390305,
      "learning_rate": 1.623435694949739e-05,
      "loss": 0.3771,
      "step": 54600
    },
    {
      "epoch": 0.9431522320120006,
      "grad_norm": 0.06137806549668312,
      "learning_rate": 1.6227460041036606e-05,
      "loss": 0.4766,
      "step": 54700
    },
    {
      "epoch": 0.9448764591271962,
      "grad_norm": 0.05996232107281685,
      "learning_rate": 1.6220563132575826e-05,
      "loss": 0.612,
      "step": 54800
    },
    {
      "epoch": 0.9466006862423918,
      "grad_norm": 0.12308018654584885,
      "learning_rate": 1.6213666224115042e-05,
      "loss": 0.6925,
      "step": 54900
    },
    {
      "epoch": 0.9483249133575875,
      "grad_norm": 0.3208850920200348,
      "learning_rate": 1.620676931565426e-05,
      "loss": 0.6264,
      "step": 55000
    },
    {
      "epoch": 0.9500491404727831,
      "grad_norm": 0.5214998126029968,
      "learning_rate": 1.619987240719348e-05,
      "loss": 0.5638,
      "step": 55100
    },
    {
      "epoch": 0.9517733675879787,
      "grad_norm": 0.4099200665950775,
      "learning_rate": 1.6192975498732695e-05,
      "loss": 0.509,
      "step": 55200
    },
    {
      "epoch": 0.9534975947031743,
      "grad_norm": 0.04194909706711769,
      "learning_rate": 1.618607859027191e-05,
      "loss": 0.5713,
      "step": 55300
    },
    {
      "epoch": 0.9552218218183699,
      "grad_norm": 0.0778241902589798,
      "learning_rate": 1.6179181681811128e-05,
      "loss": 0.6988,
      "step": 55400
    },
    {
      "epoch": 0.9569460489335655,
      "grad_norm": 0.08208257704973221,
      "learning_rate": 1.6172284773350348e-05,
      "loss": 0.5551,
      "step": 55500
    },
    {
      "epoch": 0.9586702760487611,
      "grad_norm": 0.1041017547249794,
      "learning_rate": 1.6165387864889564e-05,
      "loss": 0.429,
      "step": 55600
    },
    {
      "epoch": 0.9603945031639568,
      "grad_norm": 1.7336918745058938e-07,
      "learning_rate": 1.6158490956428784e-05,
      "loss": 0.4286,
      "step": 55700
    },
    {
      "epoch": 0.9621187302791524,
      "grad_norm": 0.02675407938659191,
      "learning_rate": 1.6151594047968e-05,
      "loss": 0.2749,
      "step": 55800
    },
    {
      "epoch": 0.963842957394348,
      "grad_norm": 4.464711764740059e-07,
      "learning_rate": 1.6144697139507217e-05,
      "loss": 0.5946,
      "step": 55900
    },
    {
      "epoch": 0.9655671845095436,
      "grad_norm": 0.1788875162601471,
      "learning_rate": 1.6137800231046434e-05,
      "loss": 0.4479,
      "step": 56000
    },
    {
      "epoch": 0.9672914116247392,
      "grad_norm": 1.3757393446667265e-07,
      "learning_rate": 1.613090332258565e-05,
      "loss": 0.5342,
      "step": 56100
    },
    {
      "epoch": 0.9690156387399348,
      "grad_norm": 0.1634410172700882,
      "learning_rate": 1.612400641412487e-05,
      "loss": 0.5204,
      "step": 56200
    },
    {
      "epoch": 0.9707398658551304,
      "grad_norm": 0.028024284169077873,
      "learning_rate": 1.6117109505664087e-05,
      "loss": 0.4688,
      "step": 56300
    },
    {
      "epoch": 0.9724640929703261,
      "grad_norm": 0.02610204555094242,
      "learning_rate": 1.6110212597203307e-05,
      "loss": 0.4902,
      "step": 56400
    },
    {
      "epoch": 0.9741883200855217,
      "grad_norm": 0.3195503354072571,
      "learning_rate": 1.6103315688742523e-05,
      "loss": 0.2797,
      "step": 56500
    },
    {
      "epoch": 0.9759125472007173,
      "grad_norm": 0.3237062990665436,
      "learning_rate": 1.609641878028174e-05,
      "loss": 0.6036,
      "step": 56600
    },
    {
      "epoch": 0.9776367743159129,
      "grad_norm": 25.442134857177734,
      "learning_rate": 1.6089521871820956e-05,
      "loss": 0.4135,
      "step": 56700
    },
    {
      "epoch": 0.9793610014311085,
      "grad_norm": 0.16152891516685486,
      "learning_rate": 1.6082624963360176e-05,
      "loss": 0.4823,
      "step": 56800
    },
    {
      "epoch": 0.9810852285463041,
      "grad_norm": 0.07039114832878113,
      "learning_rate": 1.6075728054899393e-05,
      "loss": 0.4008,
      "step": 56900
    },
    {
      "epoch": 0.9828094556614997,
      "grad_norm": 0.027623197063803673,
      "learning_rate": 1.606883114643861e-05,
      "loss": 0.4493,
      "step": 57000
    },
    {
      "epoch": 0.9845336827766954,
      "grad_norm": 28.304088592529297,
      "learning_rate": 1.606193423797783e-05,
      "loss": 0.4705,
      "step": 57100
    },
    {
      "epoch": 0.986257909891891,
      "grad_norm": 0.038647543638944626,
      "learning_rate": 1.6055037329517046e-05,
      "loss": 0.6327,
      "step": 57200
    },
    {
      "epoch": 0.9879821370070866,
      "grad_norm": 0.02837909385561943,
      "learning_rate": 1.6048140421056262e-05,
      "loss": 0.535,
      "step": 57300
    },
    {
      "epoch": 0.9897063641222822,
      "grad_norm": 0.08319424092769623,
      "learning_rate": 1.6041243512595482e-05,
      "loss": 0.4595,
      "step": 57400
    },
    {
      "epoch": 0.9914305912374778,
      "grad_norm": 22.303321838378906,
      "learning_rate": 1.60343466041347e-05,
      "loss": 0.5062,
      "step": 57500
    },
    {
      "epoch": 0.9931548183526734,
      "grad_norm": 0.05578587204217911,
      "learning_rate": 1.6027449695673915e-05,
      "loss": 0.4433,
      "step": 57600
    },
    {
      "epoch": 0.994879045467869,
      "grad_norm": 0.027611136436462402,
      "learning_rate": 1.602055278721313e-05,
      "loss": 0.1629,
      "step": 57700
    },
    {
      "epoch": 0.9966032725830647,
      "grad_norm": 1.0648070514207575e-07,
      "learning_rate": 1.601365587875235e-05,
      "loss": 0.4011,
      "step": 57800
    },
    {
      "epoch": 0.9983274996982603,
      "grad_norm": 2.717962104270555e-07,
      "learning_rate": 1.6006758970291568e-05,
      "loss": 0.5727,
      "step": 57900
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9948703699745656,
      "eval_f1_macro": 0.7106471454561206,
      "eval_loss": 0.5246626734733582,
      "eval_runtime": 182.7989,
      "eval_samples_per_second": 1791.635,
      "eval_steps_per_second": 55.99,
      "step": 57997
    },
    {
      "epoch": 1.0000517268134559,
      "grad_norm": 0.223309725522995,
      "learning_rate": 1.5999862061830788e-05,
      "loss": 0.4484,
      "step": 58000
    },
    {
      "epoch": 1.0017759539286515,
      "grad_norm": 25.5106258392334,
      "learning_rate": 1.5992965153370004e-05,
      "loss": 0.6589,
      "step": 58100
    },
    {
      "epoch": 1.003500181043847,
      "grad_norm": 0.31701576709747314,
      "learning_rate": 1.598606824490922e-05,
      "loss": 0.3865,
      "step": 58200
    },
    {
      "epoch": 1.0052244081590427,
      "grad_norm": 26.192646026611328,
      "learning_rate": 1.5979171336448437e-05,
      "loss": 0.4747,
      "step": 58300
    },
    {
      "epoch": 1.0069486352742383,
      "grad_norm": 0.4137796461582184,
      "learning_rate": 1.5972274427987654e-05,
      "loss": 0.6705,
      "step": 58400
    },
    {
      "epoch": 1.0086728623894339,
      "grad_norm": 0.3014972507953644,
      "learning_rate": 1.5965377519526874e-05,
      "loss": 0.6115,
      "step": 58500
    },
    {
      "epoch": 1.0103970895046295,
      "grad_norm": 0.07459808140993118,
      "learning_rate": 1.595848061106609e-05,
      "loss": 0.4172,
      "step": 58600
    },
    {
      "epoch": 1.012121316619825,
      "grad_norm": 0.14791443943977356,
      "learning_rate": 1.595158370260531e-05,
      "loss": 0.5074,
      "step": 58700
    },
    {
      "epoch": 1.0138455437350207,
      "grad_norm": 0.0436321422457695,
      "learning_rate": 1.5944686794144527e-05,
      "loss": 0.4625,
      "step": 58800
    },
    {
      "epoch": 1.0155697708502165,
      "grad_norm": 0.15617574751377106,
      "learning_rate": 1.5937789885683743e-05,
      "loss": 0.4501,
      "step": 58900
    },
    {
      "epoch": 1.017293997965412,
      "grad_norm": 27.22781753540039,
      "learning_rate": 1.593089297722296e-05,
      "loss": 0.3961,
      "step": 59000
    },
    {
      "epoch": 1.0190182250806077,
      "grad_norm": 0.3609050512313843,
      "learning_rate": 1.592399606876218e-05,
      "loss": 0.5516,
      "step": 59100
    },
    {
      "epoch": 1.0207424521958033,
      "grad_norm": 20.4749813079834,
      "learning_rate": 1.5917099160301396e-05,
      "loss": 0.6089,
      "step": 59200
    },
    {
      "epoch": 1.0224666793109989,
      "grad_norm": 0.25792625546455383,
      "learning_rate": 1.5910202251840613e-05,
      "loss": 0.5701,
      "step": 59300
    },
    {
      "epoch": 1.0241909064261945,
      "grad_norm": 0.22311469912528992,
      "learning_rate": 1.5903305343379833e-05,
      "loss": 0.3665,
      "step": 59400
    },
    {
      "epoch": 1.02591513354139,
      "grad_norm": 0.17700240015983582,
      "learning_rate": 1.589640843491905e-05,
      "loss": 0.5848,
      "step": 59500
    },
    {
      "epoch": 1.0276393606565857,
      "grad_norm": 0.18136969208717346,
      "learning_rate": 1.5889511526458266e-05,
      "loss": 0.3638,
      "step": 59600
    },
    {
      "epoch": 1.0293635877717813,
      "grad_norm": 0.18653926253318787,
      "learning_rate": 1.5882614617997486e-05,
      "loss": 0.4592,
      "step": 59700
    },
    {
      "epoch": 1.0310878148869769,
      "grad_norm": 0.036781374365091324,
      "learning_rate": 1.5875717709536702e-05,
      "loss": 0.373,
      "step": 59800
    },
    {
      "epoch": 1.0328120420021725,
      "grad_norm": 23.30181312561035,
      "learning_rate": 1.586882080107592e-05,
      "loss": 0.5767,
      "step": 59900
    },
    {
      "epoch": 1.034536269117368,
      "grad_norm": 0.06637129187583923,
      "learning_rate": 1.5861923892615135e-05,
      "loss": 0.5421,
      "step": 60000
    },
    {
      "epoch": 1.0362604962325637,
      "grad_norm": 0.07444221526384354,
      "learning_rate": 1.5855026984154355e-05,
      "loss": 0.4501,
      "step": 60100
    },
    {
      "epoch": 1.0379847233477593,
      "grad_norm": 25.15996742248535,
      "learning_rate": 1.584813007569357e-05,
      "loss": 0.6589,
      "step": 60200
    },
    {
      "epoch": 1.039708950462955,
      "grad_norm": 0.12334585189819336,
      "learning_rate": 1.584123316723279e-05,
      "loss": 0.4728,
      "step": 60300
    },
    {
      "epoch": 1.0414331775781507,
      "grad_norm": 0.11696935445070267,
      "learning_rate": 1.5834336258772008e-05,
      "loss": 0.5781,
      "step": 60400
    },
    {
      "epoch": 1.0431574046933463,
      "grad_norm": 0.06762202084064484,
      "learning_rate": 1.5827439350311225e-05,
      "loss": 0.3377,
      "step": 60500
    },
    {
      "epoch": 1.0448816318085419,
      "grad_norm": 0.23896633088588715,
      "learning_rate": 1.582054244185044e-05,
      "loss": 0.4587,
      "step": 60600
    },
    {
      "epoch": 1.0466058589237375,
      "grad_norm": 0.1180613711476326,
      "learning_rate": 1.5813645533389658e-05,
      "loss": 0.4553,
      "step": 60700
    },
    {
      "epoch": 1.048330086038933,
      "grad_norm": 0.24718433618545532,
      "learning_rate": 1.5806748624928877e-05,
      "loss": 0.5269,
      "step": 60800
    },
    {
      "epoch": 1.0500543131541287,
      "grad_norm": 0.06366726011037827,
      "learning_rate": 1.5799851716468094e-05,
      "loss": 0.5859,
      "step": 60900
    },
    {
      "epoch": 1.0517785402693243,
      "grad_norm": 0.24019986391067505,
      "learning_rate": 1.5792954808007314e-05,
      "loss": 0.4229,
      "step": 61000
    },
    {
      "epoch": 1.0535027673845199,
      "grad_norm": 0.031158272176980972,
      "learning_rate": 1.578605789954653e-05,
      "loss": 0.7676,
      "step": 61100
    },
    {
      "epoch": 1.0552269944997155,
      "grad_norm": 0.1390271931886673,
      "learning_rate": 1.5779160991085747e-05,
      "loss": 0.5125,
      "step": 61200
    },
    {
      "epoch": 1.056951221614911,
      "grad_norm": 0.22644709050655365,
      "learning_rate": 1.5772264082624963e-05,
      "loss": 0.6368,
      "step": 61300
    },
    {
      "epoch": 1.0586754487301067,
      "grad_norm": 0.10673864185810089,
      "learning_rate": 1.576536717416418e-05,
      "loss": 0.3769,
      "step": 61400
    },
    {
      "epoch": 1.0603996758453023,
      "grad_norm": 0.17848707735538483,
      "learning_rate": 1.57584702657034e-05,
      "loss": 0.5358,
      "step": 61500
    },
    {
      "epoch": 1.0621239029604979,
      "grad_norm": 0.11180038005113602,
      "learning_rate": 1.5751573357242616e-05,
      "loss": 0.4156,
      "step": 61600
    },
    {
      "epoch": 1.0638481300756935,
      "grad_norm": 0.1785222291946411,
      "learning_rate": 1.5744676448781836e-05,
      "loss": 0.6128,
      "step": 61700
    },
    {
      "epoch": 1.0655723571908893,
      "grad_norm": 0.0927412286400795,
      "learning_rate": 1.5737779540321053e-05,
      "loss": 0.5291,
      "step": 61800
    },
    {
      "epoch": 1.0672965843060849,
      "grad_norm": 27.238643646240234,
      "learning_rate": 1.573088263186027e-05,
      "loss": 0.5522,
      "step": 61900
    },
    {
      "epoch": 1.0690208114212805,
      "grad_norm": 0.1548982858657837,
      "learning_rate": 1.5723985723399486e-05,
      "loss": 0.4346,
      "step": 62000
    },
    {
      "epoch": 1.070745038536476,
      "grad_norm": 0.26724204421043396,
      "learning_rate": 1.5717088814938706e-05,
      "loss": 0.6966,
      "step": 62100
    },
    {
      "epoch": 1.0724692656516717,
      "grad_norm": 0.13874316215515137,
      "learning_rate": 1.5710191906477922e-05,
      "loss": 0.5079,
      "step": 62200
    },
    {
      "epoch": 1.0741934927668673,
      "grad_norm": 0.3661942780017853,
      "learning_rate": 1.570329499801714e-05,
      "loss": 0.6636,
      "step": 62300
    },
    {
      "epoch": 1.0759177198820629,
      "grad_norm": 0.09736207872629166,
      "learning_rate": 1.569639808955636e-05,
      "loss": 0.5513,
      "step": 62400
    },
    {
      "epoch": 1.0776419469972585,
      "grad_norm": 0.013728411868214607,
      "learning_rate": 1.5689501181095575e-05,
      "loss": 0.4085,
      "step": 62500
    },
    {
      "epoch": 1.079366174112454,
      "grad_norm": 0.23496173322200775,
      "learning_rate": 1.5682604272634795e-05,
      "loss": 0.4608,
      "step": 62600
    },
    {
      "epoch": 1.0810904012276497,
      "grad_norm": 0.2747552692890167,
      "learning_rate": 1.567570736417401e-05,
      "loss": 0.4723,
      "step": 62700
    },
    {
      "epoch": 1.0828146283428453,
      "grad_norm": 18.62775421142578,
      "learning_rate": 1.5668810455713228e-05,
      "loss": 0.5878,
      "step": 62800
    },
    {
      "epoch": 1.0845388554580409,
      "grad_norm": 0.0368410088121891,
      "learning_rate": 1.5661913547252445e-05,
      "loss": 0.33,
      "step": 62900
    },
    {
      "epoch": 1.0862630825732364,
      "grad_norm": 0.06153634563088417,
      "learning_rate": 1.565501663879166e-05,
      "loss": 0.4573,
      "step": 63000
    },
    {
      "epoch": 1.0879873096884323,
      "grad_norm": 0.4268342852592468,
      "learning_rate": 1.564811973033088e-05,
      "loss": 0.7842,
      "step": 63100
    },
    {
      "epoch": 1.0897115368036279,
      "grad_norm": 0.15810079872608185,
      "learning_rate": 1.5641222821870098e-05,
      "loss": 0.3905,
      "step": 63200
    },
    {
      "epoch": 1.0914357639188235,
      "grad_norm": 7.407129487546626e-08,
      "learning_rate": 1.5634325913409317e-05,
      "loss": 0.3726,
      "step": 63300
    },
    {
      "epoch": 1.093159991034019,
      "grad_norm": 0.3080180585384369,
      "learning_rate": 1.5627429004948534e-05,
      "loss": 0.4691,
      "step": 63400
    },
    {
      "epoch": 1.0948842181492147,
      "grad_norm": 0.11110015958547592,
      "learning_rate": 1.562053209648775e-05,
      "loss": 0.5109,
      "step": 63500
    },
    {
      "epoch": 1.0966084452644103,
      "grad_norm": 0.10731075704097748,
      "learning_rate": 1.5613635188026967e-05,
      "loss": 0.519,
      "step": 63600
    },
    {
      "epoch": 1.0983326723796059,
      "grad_norm": 0.053199585527181625,
      "learning_rate": 1.5606738279566184e-05,
      "loss": 0.5738,
      "step": 63700
    },
    {
      "epoch": 1.1000568994948015,
      "grad_norm": 0.1165611669421196,
      "learning_rate": 1.5599841371105403e-05,
      "loss": 0.4684,
      "step": 63800
    },
    {
      "epoch": 1.101781126609997,
      "grad_norm": 0.14803923666477203,
      "learning_rate": 1.559294446264462e-05,
      "loss": 0.4556,
      "step": 63900
    },
    {
      "epoch": 1.1035053537251927,
      "grad_norm": 0.08619555830955505,
      "learning_rate": 1.558604755418384e-05,
      "loss": 0.6891,
      "step": 64000
    },
    {
      "epoch": 1.1052295808403882,
      "grad_norm": 0.23249180614948273,
      "learning_rate": 1.5579150645723056e-05,
      "loss": 0.5728,
      "step": 64100
    },
    {
      "epoch": 1.1069538079555838,
      "grad_norm": 0.12629586458206177,
      "learning_rate": 1.5572253737262273e-05,
      "loss": 0.4549,
      "step": 64200
    },
    {
      "epoch": 1.1086780350707794,
      "grad_norm": 0.10913322865962982,
      "learning_rate": 1.556535682880149e-05,
      "loss": 0.4435,
      "step": 64300
    },
    {
      "epoch": 1.110402262185975,
      "grad_norm": 0.33666330575942993,
      "learning_rate": 1.555845992034071e-05,
      "loss": 0.6296,
      "step": 64400
    },
    {
      "epoch": 1.1121264893011706,
      "grad_norm": 0.11579824239015579,
      "learning_rate": 1.5551563011879926e-05,
      "loss": 0.5136,
      "step": 64500
    },
    {
      "epoch": 1.1138507164163665,
      "grad_norm": 32.67267608642578,
      "learning_rate": 1.5544666103419142e-05,
      "loss": 0.3897,
      "step": 64600
    },
    {
      "epoch": 1.115574943531562,
      "grad_norm": 0.05393315851688385,
      "learning_rate": 1.5537769194958362e-05,
      "loss": 0.7048,
      "step": 64700
    },
    {
      "epoch": 1.1172991706467577,
      "grad_norm": 25.203615188598633,
      "learning_rate": 1.553087228649758e-05,
      "loss": 0.5378,
      "step": 64800
    },
    {
      "epoch": 1.1190233977619533,
      "grad_norm": 0.0669870376586914,
      "learning_rate": 1.5523975378036795e-05,
      "loss": 0.4941,
      "step": 64900
    },
    {
      "epoch": 1.1207476248771489,
      "grad_norm": 0.27788132429122925,
      "learning_rate": 1.5517078469576015e-05,
      "loss": 0.5374,
      "step": 65000
    },
    {
      "epoch": 1.1224718519923444,
      "grad_norm": 0.25040403008461,
      "learning_rate": 1.5510181561115232e-05,
      "loss": 0.3651,
      "step": 65100
    },
    {
      "epoch": 1.12419607910754,
      "grad_norm": 6.158973064884776e-08,
      "learning_rate": 1.5503284652654448e-05,
      "loss": 0.5977,
      "step": 65200
    },
    {
      "epoch": 1.1259203062227356,
      "grad_norm": 0.5126662254333496,
      "learning_rate": 1.5496387744193665e-05,
      "loss": 0.6477,
      "step": 65300
    },
    {
      "epoch": 1.1276445333379312,
      "grad_norm": 0.2170446664094925,
      "learning_rate": 1.5489490835732885e-05,
      "loss": 0.5343,
      "step": 65400
    },
    {
      "epoch": 1.1293687604531268,
      "grad_norm": 0.08066996186971664,
      "learning_rate": 1.54825939272721e-05,
      "loss": 0.5445,
      "step": 65500
    },
    {
      "epoch": 1.1310929875683224,
      "grad_norm": 0.03769377991557121,
      "learning_rate": 1.547569701881132e-05,
      "loss": 0.3195,
      "step": 65600
    },
    {
      "epoch": 1.132817214683518,
      "grad_norm": 0.10589599609375,
      "learning_rate": 1.5468800110350538e-05,
      "loss": 0.5676,
      "step": 65700
    },
    {
      "epoch": 1.1345414417987136,
      "grad_norm": 0.20410028100013733,
      "learning_rate": 1.5461903201889754e-05,
      "loss": 0.4484,
      "step": 65800
    },
    {
      "epoch": 1.1362656689139095,
      "grad_norm": 0.03484715148806572,
      "learning_rate": 1.545500629342897e-05,
      "loss": 0.5866,
      "step": 65900
    },
    {
      "epoch": 1.1379898960291048,
      "grad_norm": 0.06614567339420319,
      "learning_rate": 1.5448109384968187e-05,
      "loss": 0.541,
      "step": 66000
    },
    {
      "epoch": 1.1397141231443007,
      "grad_norm": 0.24819476902484894,
      "learning_rate": 1.5441212476507407e-05,
      "loss": 0.4087,
      "step": 66100
    },
    {
      "epoch": 1.1414383502594962,
      "grad_norm": 0.13881610333919525,
      "learning_rate": 1.5434315568046624e-05,
      "loss": 0.3739,
      "step": 66200
    },
    {
      "epoch": 1.1431625773746918,
      "grad_norm": 0.040333591401576996,
      "learning_rate": 1.5427418659585843e-05,
      "loss": 0.3624,
      "step": 66300
    },
    {
      "epoch": 1.1448868044898874,
      "grad_norm": 0.19422197341918945,
      "learning_rate": 1.542052175112506e-05,
      "loss": 0.4301,
      "step": 66400
    },
    {
      "epoch": 1.146611031605083,
      "grad_norm": 25.533105850219727,
      "learning_rate": 1.5413624842664277e-05,
      "loss": 0.7133,
      "step": 66500
    },
    {
      "epoch": 1.1483352587202786,
      "grad_norm": 0.13137762248516083,
      "learning_rate": 1.5406727934203493e-05,
      "loss": 0.554,
      "step": 66600
    },
    {
      "epoch": 1.1500594858354742,
      "grad_norm": 30.120668411254883,
      "learning_rate": 1.5399831025742713e-05,
      "loss": 0.5999,
      "step": 66700
    },
    {
      "epoch": 1.1517837129506698,
      "grad_norm": 22.100231170654297,
      "learning_rate": 1.539293411728193e-05,
      "loss": 0.456,
      "step": 66800
    },
    {
      "epoch": 1.1535079400658654,
      "grad_norm": 0.3078567683696747,
      "learning_rate": 1.5386037208821146e-05,
      "loss": 0.3724,
      "step": 66900
    },
    {
      "epoch": 1.155232167181061,
      "grad_norm": 0.14868026971817017,
      "learning_rate": 1.5379140300360366e-05,
      "loss": 0.6763,
      "step": 67000
    },
    {
      "epoch": 1.1569563942962566,
      "grad_norm": 0.009665192104876041,
      "learning_rate": 1.5372243391899582e-05,
      "loss": 0.2984,
      "step": 67100
    },
    {
      "epoch": 1.1586806214114522,
      "grad_norm": 0.020346002653241158,
      "learning_rate": 1.53653464834388e-05,
      "loss": 0.4617,
      "step": 67200
    },
    {
      "epoch": 1.1604048485266478,
      "grad_norm": 28.909793853759766,
      "learning_rate": 1.535844957497802e-05,
      "loss": 0.5908,
      "step": 67300
    },
    {
      "epoch": 1.1621290756418436,
      "grad_norm": 0.04342775046825409,
      "learning_rate": 1.5351552666517235e-05,
      "loss": 0.3729,
      "step": 67400
    },
    {
      "epoch": 1.1638533027570392,
      "grad_norm": 0.03180258721113205,
      "learning_rate": 1.5344655758056452e-05,
      "loss": 0.5485,
      "step": 67500
    },
    {
      "epoch": 1.1655775298722348,
      "grad_norm": 0.08181397616863251,
      "learning_rate": 1.533775884959567e-05,
      "loss": 0.255,
      "step": 67600
    },
    {
      "epoch": 1.1673017569874304,
      "grad_norm": 0.20115944743156433,
      "learning_rate": 1.5330861941134888e-05,
      "loss": 0.5406,
      "step": 67700
    },
    {
      "epoch": 1.169025984102626,
      "grad_norm": 0.033608775585889816,
      "learning_rate": 1.5323965032674105e-05,
      "loss": 0.4812,
      "step": 67800
    },
    {
      "epoch": 1.1707502112178216,
      "grad_norm": 0.146344855427742,
      "learning_rate": 1.5317068124213325e-05,
      "loss": 0.5379,
      "step": 67900
    },
    {
      "epoch": 1.1724744383330172,
      "grad_norm": 0.23315677046775818,
      "learning_rate": 1.531017121575254e-05,
      "loss": 0.4063,
      "step": 68000
    },
    {
      "epoch": 1.1741986654482128,
      "grad_norm": 0.1280200481414795,
      "learning_rate": 1.5303274307291758e-05,
      "loss": 0.3648,
      "step": 68100
    },
    {
      "epoch": 1.1759228925634084,
      "grad_norm": 0.1271541714668274,
      "learning_rate": 1.5296377398830974e-05,
      "loss": 0.7727,
      "step": 68200
    },
    {
      "epoch": 1.177647119678604,
      "grad_norm": 0.13232047855854034,
      "learning_rate": 1.528948049037019e-05,
      "loss": 0.4708,
      "step": 68300
    },
    {
      "epoch": 1.1793713467937996,
      "grad_norm": 0.1788843721151352,
      "learning_rate": 1.528258358190941e-05,
      "loss": 0.5608,
      "step": 68400
    },
    {
      "epoch": 1.1810955739089952,
      "grad_norm": 8.687334940304936e-08,
      "learning_rate": 1.5275686673448627e-05,
      "loss": 0.3109,
      "step": 68500
    },
    {
      "epoch": 1.1828198010241908,
      "grad_norm": 0.04785698652267456,
      "learning_rate": 1.5268789764987847e-05,
      "loss": 0.5074,
      "step": 68600
    },
    {
      "epoch": 1.1845440281393866,
      "grad_norm": 0.3178900182247162,
      "learning_rate": 1.5261892856527064e-05,
      "loss": 0.4979,
      "step": 68700
    },
    {
      "epoch": 1.186268255254582,
      "grad_norm": 0.2590862810611725,
      "learning_rate": 1.525499594806628e-05,
      "loss": 0.4642,
      "step": 68800
    },
    {
      "epoch": 1.1879924823697778,
      "grad_norm": 0.04865911230444908,
      "learning_rate": 1.5248099039605497e-05,
      "loss": 0.5004,
      "step": 68900
    },
    {
      "epoch": 1.1897167094849734,
      "grad_norm": 0.11235379427671432,
      "learning_rate": 1.5241202131144717e-05,
      "loss": 0.3627,
      "step": 69000
    },
    {
      "epoch": 1.191440936600169,
      "grad_norm": 0.059372883290052414,
      "learning_rate": 1.5234305222683933e-05,
      "loss": 0.3096,
      "step": 69100
    },
    {
      "epoch": 1.1931651637153646,
      "grad_norm": 17.314834594726562,
      "learning_rate": 1.5227408314223151e-05,
      "loss": 0.68,
      "step": 69200
    },
    {
      "epoch": 1.1948893908305602,
      "grad_norm": 0.13847918808460236,
      "learning_rate": 1.5220511405762368e-05,
      "loss": 0.5533,
      "step": 69300
    },
    {
      "epoch": 1.1966136179457558,
      "grad_norm": 0.21669039130210876,
      "learning_rate": 1.5213614497301586e-05,
      "loss": 0.4055,
      "step": 69400
    },
    {
      "epoch": 1.1983378450609514,
      "grad_norm": 0.2950912117958069,
      "learning_rate": 1.5206717588840802e-05,
      "loss": 0.452,
      "step": 69500
    },
    {
      "epoch": 1.200062072176147,
      "grad_norm": 0.026904430240392685,
      "learning_rate": 1.5199820680380022e-05,
      "loss": 0.5598,
      "step": 69600
    },
    {
      "epoch": 1.2017862992913426,
      "grad_norm": 0.11622873693704605,
      "learning_rate": 1.5192923771919239e-05,
      "loss": 0.4953,
      "step": 69700
    },
    {
      "epoch": 1.2035105264065382,
      "grad_norm": 0.09660248458385468,
      "learning_rate": 1.5186026863458455e-05,
      "loss": 0.3683,
      "step": 69800
    },
    {
      "epoch": 1.2052347535217338,
      "grad_norm": 0.08727653324604034,
      "learning_rate": 1.5179129954997674e-05,
      "loss": 0.4192,
      "step": 69900
    },
    {
      "epoch": 1.2069589806369294,
      "grad_norm": 0.04687720164656639,
      "learning_rate": 1.517223304653689e-05,
      "loss": 0.3429,
      "step": 70000
    },
    {
      "epoch": 1.208683207752125,
      "grad_norm": 25.837791442871094,
      "learning_rate": 1.5165336138076108e-05,
      "loss": 0.7329,
      "step": 70100
    },
    {
      "epoch": 1.2104074348673208,
      "grad_norm": 0.2288394570350647,
      "learning_rate": 1.5158439229615327e-05,
      "loss": 0.4841,
      "step": 70200
    },
    {
      "epoch": 1.2121316619825164,
      "grad_norm": 24.772262573242188,
      "learning_rate": 1.5151542321154545e-05,
      "loss": 0.5909,
      "step": 70300
    },
    {
      "epoch": 1.213855889097712,
      "grad_norm": 0.09572548419237137,
      "learning_rate": 1.5144645412693761e-05,
      "loss": 0.3146,
      "step": 70400
    },
    {
      "epoch": 1.2155801162129076,
      "grad_norm": 0.06259499490261078,
      "learning_rate": 1.5137748504232978e-05,
      "loss": 0.6316,
      "step": 70500
    },
    {
      "epoch": 1.2173043433281032,
      "grad_norm": 0.20596426725387573,
      "learning_rate": 1.5130851595772196e-05,
      "loss": 0.4951,
      "step": 70600
    },
    {
      "epoch": 1.2190285704432988,
      "grad_norm": 30.0933780670166,
      "learning_rate": 1.5123954687311413e-05,
      "loss": 0.5353,
      "step": 70700
    },
    {
      "epoch": 1.2207527975584944,
      "grad_norm": 23.03551483154297,
      "learning_rate": 1.5117057778850632e-05,
      "loss": 0.3805,
      "step": 70800
    },
    {
      "epoch": 1.22247702467369,
      "grad_norm": 0.26514801383018494,
      "learning_rate": 1.5110160870389849e-05,
      "loss": 0.4987,
      "step": 70900
    },
    {
      "epoch": 1.2242012517888856,
      "grad_norm": 0.051205046474933624,
      "learning_rate": 1.5103263961929067e-05,
      "loss": 0.5337,
      "step": 71000
    },
    {
      "epoch": 1.2259254789040812,
      "grad_norm": 0.03966240957379341,
      "learning_rate": 1.5096367053468284e-05,
      "loss": 0.5384,
      "step": 71100
    },
    {
      "epoch": 1.2276497060192768,
      "grad_norm": 0.01718171499669552,
      "learning_rate": 1.50894701450075e-05,
      "loss": 0.2332,
      "step": 71200
    },
    {
      "epoch": 1.2293739331344724,
      "grad_norm": 0.06899354606866837,
      "learning_rate": 1.5082573236546718e-05,
      "loss": 0.5438,
      "step": 71300
    },
    {
      "epoch": 1.231098160249668,
      "grad_norm": 0.31508398056030273,
      "learning_rate": 1.5075676328085938e-05,
      "loss": 0.4412,
      "step": 71400
    },
    {
      "epoch": 1.2328223873648636,
      "grad_norm": 0.03276277706027031,
      "learning_rate": 1.5068779419625155e-05,
      "loss": 0.6349,
      "step": 71500
    },
    {
      "epoch": 1.2345466144800592,
      "grad_norm": 0.05034896731376648,
      "learning_rate": 1.5061882511164371e-05,
      "loss": 0.4486,
      "step": 71600
    },
    {
      "epoch": 1.236270841595255,
      "grad_norm": 0.04545962065458298,
      "learning_rate": 1.505498560270359e-05,
      "loss": 0.3951,
      "step": 71700
    },
    {
      "epoch": 1.2379950687104506,
      "grad_norm": 0.06396273523569107,
      "learning_rate": 1.5048088694242806e-05,
      "loss": 0.5734,
      "step": 71800
    },
    {
      "epoch": 1.2397192958256462,
      "grad_norm": 0.0943133607506752,
      "learning_rate": 1.5041191785782026e-05,
      "loss": 0.4946,
      "step": 71900
    },
    {
      "epoch": 1.2414435229408418,
      "grad_norm": 0.07912779599428177,
      "learning_rate": 1.5034294877321242e-05,
      "loss": 0.5352,
      "step": 72000
    },
    {
      "epoch": 1.2431677500560374,
      "grad_norm": 0.09480464458465576,
      "learning_rate": 1.502739796886046e-05,
      "loss": 0.3622,
      "step": 72100
    },
    {
      "epoch": 1.244891977171233,
      "grad_norm": 0.2560539245605469,
      "learning_rate": 1.5020501060399677e-05,
      "loss": 0.2247,
      "step": 72200
    },
    {
      "epoch": 1.2466162042864286,
      "grad_norm": 28.556781768798828,
      "learning_rate": 1.5013604151938894e-05,
      "loss": 0.6077,
      "step": 72300
    },
    {
      "epoch": 1.2483404314016242,
      "grad_norm": 0.2023458182811737,
      "learning_rate": 1.5006707243478112e-05,
      "loss": 0.6349,
      "step": 72400
    },
    {
      "epoch": 1.2500646585168198,
      "grad_norm": 0.08437636494636536,
      "learning_rate": 1.499981033501733e-05,
      "loss": 0.3641,
      "step": 72500
    },
    {
      "epoch": 1.2517888856320154,
      "grad_norm": 0.18615072965621948,
      "learning_rate": 1.4992913426556548e-05,
      "loss": 0.5061,
      "step": 72600
    },
    {
      "epoch": 1.253513112747211,
      "grad_norm": 0.21402080357074738,
      "learning_rate": 1.4986016518095765e-05,
      "loss": 0.4442,
      "step": 72700
    },
    {
      "epoch": 1.2552373398624066,
      "grad_norm": 26.621932983398438,
      "learning_rate": 1.4979119609634983e-05,
      "loss": 0.3903,
      "step": 72800
    },
    {
      "epoch": 1.2569615669776022,
      "grad_norm": 0.26744499802589417,
      "learning_rate": 1.49722227011742e-05,
      "loss": 0.634,
      "step": 72900
    },
    {
      "epoch": 1.258685794092798,
      "grad_norm": 26.28660774230957,
      "learning_rate": 1.4965325792713416e-05,
      "loss": 0.5011,
      "step": 73000
    },
    {
      "epoch": 1.2604100212079934,
      "grad_norm": 0.1051374077796936,
      "learning_rate": 1.4958428884252636e-05,
      "loss": 0.4052,
      "step": 73100
    },
    {
      "epoch": 1.2621342483231892,
      "grad_norm": 0.11510175466537476,
      "learning_rate": 1.4951531975791853e-05,
      "loss": 0.4198,
      "step": 73200
    },
    {
      "epoch": 1.2638584754383848,
      "grad_norm": 0.04879620298743248,
      "learning_rate": 1.494463506733107e-05,
      "loss": 0.812,
      "step": 73300
    },
    {
      "epoch": 1.2655827025535804,
      "grad_norm": 22.997013092041016,
      "learning_rate": 1.4937738158870287e-05,
      "loss": 0.3939,
      "step": 73400
    },
    {
      "epoch": 1.267306929668776,
      "grad_norm": 0.25535130500793457,
      "learning_rate": 1.4930841250409504e-05,
      "loss": 0.4149,
      "step": 73500
    },
    {
      "epoch": 1.2690311567839716,
      "grad_norm": 0.1378522515296936,
      "learning_rate": 1.4923944341948722e-05,
      "loss": 0.5627,
      "step": 73600
    },
    {
      "epoch": 1.2707553838991672,
      "grad_norm": 0.07992900162935257,
      "learning_rate": 1.4917047433487942e-05,
      "loss": 0.4734,
      "step": 73700
    },
    {
      "epoch": 1.2724796110143628,
      "grad_norm": 0.09030677378177643,
      "learning_rate": 1.4910150525027158e-05,
      "loss": 0.3493,
      "step": 73800
    },
    {
      "epoch": 1.2742038381295584,
      "grad_norm": 0.09124220162630081,
      "learning_rate": 1.4903253616566375e-05,
      "loss": 0.8354,
      "step": 73900
    },
    {
      "epoch": 1.275928065244754,
      "grad_norm": 0.09417764842510223,
      "learning_rate": 1.4896356708105593e-05,
      "loss": 0.3577,
      "step": 74000
    },
    {
      "epoch": 1.2776522923599496,
      "grad_norm": 0.15713727474212646,
      "learning_rate": 1.488945979964481e-05,
      "loss": 0.4275,
      "step": 74100
    },
    {
      "epoch": 1.2793765194751452,
      "grad_norm": 0.0999341532588005,
      "learning_rate": 1.4882562891184026e-05,
      "loss": 0.3962,
      "step": 74200
    },
    {
      "epoch": 1.281100746590341,
      "grad_norm": 0.28185731172561646,
      "learning_rate": 1.4875665982723246e-05,
      "loss": 0.5373,
      "step": 74300
    },
    {
      "epoch": 1.2828249737055364,
      "grad_norm": 0.1274941861629486,
      "learning_rate": 1.4868769074262464e-05,
      "loss": 0.6828,
      "step": 74400
    },
    {
      "epoch": 1.2845492008207322,
      "grad_norm": 0.11172498017549515,
      "learning_rate": 1.486187216580168e-05,
      "loss": 0.6331,
      "step": 74500
    },
    {
      "epoch": 1.2862734279359276,
      "grad_norm": 0.15365099906921387,
      "learning_rate": 1.4854975257340897e-05,
      "loss": 0.628,
      "step": 74600
    },
    {
      "epoch": 1.2879976550511234,
      "grad_norm": 0.1344704031944275,
      "learning_rate": 1.4848078348880116e-05,
      "loss": 0.4141,
      "step": 74700
    },
    {
      "epoch": 1.289721882166319,
      "grad_norm": 0.3406682312488556,
      "learning_rate": 1.4841181440419332e-05,
      "loss": 0.4817,
      "step": 74800
    },
    {
      "epoch": 1.2914461092815146,
      "grad_norm": 0.21751055121421814,
      "learning_rate": 1.4834284531958552e-05,
      "loss": 0.6396,
      "step": 74900
    },
    {
      "epoch": 1.2931703363967102,
      "grad_norm": 0.19647881388664246,
      "learning_rate": 1.4827387623497768e-05,
      "loss": 0.6569,
      "step": 75000
    },
    {
      "epoch": 1.2948945635119058,
      "grad_norm": 0.05976101756095886,
      "learning_rate": 1.4820490715036987e-05,
      "loss": 0.5373,
      "step": 75100
    },
    {
      "epoch": 1.2966187906271014,
      "grad_norm": 0.1809675246477127,
      "learning_rate": 1.4813593806576203e-05,
      "loss": 0.5802,
      "step": 75200
    },
    {
      "epoch": 1.298343017742297,
      "grad_norm": 0.07367341220378876,
      "learning_rate": 1.480669689811542e-05,
      "loss": 0.3989,
      "step": 75300
    },
    {
      "epoch": 1.3000672448574926,
      "grad_norm": 0.06706814467906952,
      "learning_rate": 1.479979998965464e-05,
      "loss": 0.4771,
      "step": 75400
    },
    {
      "epoch": 1.3017914719726882,
      "grad_norm": 0.24168071150779724,
      "learning_rate": 1.4792903081193856e-05,
      "loss": 0.4584,
      "step": 75500
    },
    {
      "epoch": 1.3035156990878838,
      "grad_norm": 0.025718383491039276,
      "learning_rate": 1.4786006172733074e-05,
      "loss": 0.71,
      "step": 75600
    },
    {
      "epoch": 1.3052399262030794,
      "grad_norm": 2.5768256364244735e-06,
      "learning_rate": 1.477910926427229e-05,
      "loss": 0.4933,
      "step": 75700
    },
    {
      "epoch": 1.3069641533182752,
      "grad_norm": 28.746936798095703,
      "learning_rate": 1.4772212355811509e-05,
      "loss": 0.3895,
      "step": 75800
    },
    {
      "epoch": 1.3086883804334706,
      "grad_norm": 0.2239588052034378,
      "learning_rate": 1.4765315447350726e-05,
      "loss": 0.5056,
      "step": 75900
    },
    {
      "epoch": 1.3104126075486664,
      "grad_norm": 0.08478138595819473,
      "learning_rate": 1.4758418538889945e-05,
      "loss": 0.2801,
      "step": 76000
    },
    {
      "epoch": 1.312136834663862,
      "grad_norm": 0.1630931943655014,
      "learning_rate": 1.4751521630429162e-05,
      "loss": 0.5977,
      "step": 76100
    },
    {
      "epoch": 1.3138610617790576,
      "grad_norm": 0.049251314252614975,
      "learning_rate": 1.4744624721968378e-05,
      "loss": 0.3889,
      "step": 76200
    },
    {
      "epoch": 1.3155852888942532,
      "grad_norm": 0.17036648094654083,
      "learning_rate": 1.4737727813507597e-05,
      "loss": 0.2957,
      "step": 76300
    },
    {
      "epoch": 1.3173095160094488,
      "grad_norm": 25.961824417114258,
      "learning_rate": 1.4730830905046813e-05,
      "loss": 0.5196,
      "step": 76400
    },
    {
      "epoch": 1.3190337431246444,
      "grad_norm": 27.634180068969727,
      "learning_rate": 1.4723933996586031e-05,
      "loss": 0.8022,
      "step": 76500
    },
    {
      "epoch": 1.32075797023984,
      "grad_norm": 0.08384663611650467,
      "learning_rate": 1.471703708812525e-05,
      "loss": 0.4202,
      "step": 76600
    },
    {
      "epoch": 1.3224821973550356,
      "grad_norm": 0.028861558064818382,
      "learning_rate": 1.4710140179664468e-05,
      "loss": 0.373,
      "step": 76700
    },
    {
      "epoch": 1.3242064244702312,
      "grad_norm": 1.4703604165333672e-06,
      "learning_rate": 1.4703243271203684e-05,
      "loss": 0.404,
      "step": 76800
    },
    {
      "epoch": 1.3259306515854268,
      "grad_norm": 0.09263622015714645,
      "learning_rate": 1.4696346362742901e-05,
      "loss": 0.3656,
      "step": 76900
    },
    {
      "epoch": 1.3276548787006224,
      "grad_norm": 0.12610231339931488,
      "learning_rate": 1.4689449454282119e-05,
      "loss": 0.5629,
      "step": 77000
    },
    {
      "epoch": 1.3293791058158182,
      "grad_norm": 0.13498061895370483,
      "learning_rate": 1.4682552545821336e-05,
      "loss": 0.378,
      "step": 77100
    },
    {
      "epoch": 1.3311033329310136,
      "grad_norm": 1.201125655825308e-06,
      "learning_rate": 1.4675655637360556e-05,
      "loss": 0.6169,
      "step": 77200
    },
    {
      "epoch": 1.3328275600462094,
      "grad_norm": 0.24101021885871887,
      "learning_rate": 1.4668758728899772e-05,
      "loss": 0.4867,
      "step": 77300
    },
    {
      "epoch": 1.3345517871614048,
      "grad_norm": 0.06675505638122559,
      "learning_rate": 1.466186182043899e-05,
      "loss": 0.5747,
      "step": 77400
    },
    {
      "epoch": 1.3362760142766006,
      "grad_norm": 0.25659656524658203,
      "learning_rate": 1.4654964911978207e-05,
      "loss": 0.368,
      "step": 77500
    },
    {
      "epoch": 1.3380002413917962,
      "grad_norm": 0.3704625368118286,
      "learning_rate": 1.4648068003517423e-05,
      "loss": 0.4918,
      "step": 77600
    },
    {
      "epoch": 1.3397244685069918,
      "grad_norm": 0.049302875995635986,
      "learning_rate": 1.4641171095056641e-05,
      "loss": 0.5589,
      "step": 77700
    },
    {
      "epoch": 1.3414486956221874,
      "grad_norm": 0.06420651823282242,
      "learning_rate": 1.463427418659586e-05,
      "loss": 0.4266,
      "step": 77800
    },
    {
      "epoch": 1.343172922737383,
      "grad_norm": 0.05637213587760925,
      "learning_rate": 1.4627377278135078e-05,
      "loss": 0.6087,
      "step": 77900
    },
    {
      "epoch": 1.3448971498525786,
      "grad_norm": 0.06697751581668854,
      "learning_rate": 1.4620480369674294e-05,
      "loss": 0.5825,
      "step": 78000
    },
    {
      "epoch": 1.3466213769677742,
      "grad_norm": 0.19529223442077637,
      "learning_rate": 1.4613583461213513e-05,
      "loss": 0.4279,
      "step": 78100
    },
    {
      "epoch": 1.3483456040829698,
      "grad_norm": 0.07475732266902924,
      "learning_rate": 1.4606686552752729e-05,
      "loss": 0.4339,
      "step": 78200
    },
    {
      "epoch": 1.3500698311981654,
      "grad_norm": 0.04218994081020355,
      "learning_rate": 1.4599789644291946e-05,
      "loss": 0.3678,
      "step": 78300
    },
    {
      "epoch": 1.351794058313361,
      "grad_norm": 0.051434919238090515,
      "learning_rate": 1.4592892735831166e-05,
      "loss": 0.5014,
      "step": 78400
    },
    {
      "epoch": 1.3535182854285566,
      "grad_norm": 33.35541915893555,
      "learning_rate": 1.4585995827370382e-05,
      "loss": 0.65,
      "step": 78500
    },
    {
      "epoch": 1.3552425125437524,
      "grad_norm": 0.09031727910041809,
      "learning_rate": 1.45790989189096e-05,
      "loss": 0.6607,
      "step": 78600
    },
    {
      "epoch": 1.3569667396589478,
      "grad_norm": 0.16361813247203827,
      "learning_rate": 1.4572202010448817e-05,
      "loss": 0.4329,
      "step": 78700
    },
    {
      "epoch": 1.3586909667741436,
      "grad_norm": 0.3019612431526184,
      "learning_rate": 1.4565305101988035e-05,
      "loss": 0.4858,
      "step": 78800
    },
    {
      "epoch": 1.3604151938893392,
      "grad_norm": 0.20247595012187958,
      "learning_rate": 1.4558408193527253e-05,
      "loss": 0.5854,
      "step": 78900
    },
    {
      "epoch": 1.3621394210045348,
      "grad_norm": 20.491802215576172,
      "learning_rate": 1.4551511285066471e-05,
      "loss": 0.5079,
      "step": 79000
    },
    {
      "epoch": 1.3638636481197304,
      "grad_norm": 0.40474650263786316,
      "learning_rate": 1.4544614376605688e-05,
      "loss": 0.5902,
      "step": 79100
    },
    {
      "epoch": 1.365587875234926,
      "grad_norm": 0.4522038400173187,
      "learning_rate": 1.4537717468144904e-05,
      "loss": 0.5674,
      "step": 79200
    },
    {
      "epoch": 1.3673121023501216,
      "grad_norm": 0.10694797337055206,
      "learning_rate": 1.4530820559684123e-05,
      "loss": 0.4775,
      "step": 79300
    },
    {
      "epoch": 1.3690363294653172,
      "grad_norm": 0.036657873541116714,
      "learning_rate": 1.452392365122334e-05,
      "loss": 0.4551,
      "step": 79400
    },
    {
      "epoch": 1.3707605565805128,
      "grad_norm": 0.1360735446214676,
      "learning_rate": 1.4517026742762559e-05,
      "loss": 0.3263,
      "step": 79500
    },
    {
      "epoch": 1.3724847836957084,
      "grad_norm": 0.1554776132106781,
      "learning_rate": 1.4510129834301776e-05,
      "loss": 0.4683,
      "step": 79600
    },
    {
      "epoch": 1.374209010810904,
      "grad_norm": 18.200763702392578,
      "learning_rate": 1.4503232925840994e-05,
      "loss": 0.5503,
      "step": 79700
    },
    {
      "epoch": 1.3759332379260996,
      "grad_norm": 0.09613292664289474,
      "learning_rate": 1.449633601738021e-05,
      "loss": 0.6052,
      "step": 79800
    },
    {
      "epoch": 1.3776574650412952,
      "grad_norm": 0.23468884825706482,
      "learning_rate": 1.4489439108919427e-05,
      "loss": 0.5206,
      "step": 79900
    },
    {
      "epoch": 1.3793816921564908,
      "grad_norm": 0.22919650375843048,
      "learning_rate": 1.4482542200458645e-05,
      "loss": 0.4959,
      "step": 80000
    },
    {
      "epoch": 1.3811059192716866,
      "grad_norm": 24.986753463745117,
      "learning_rate": 1.4475645291997863e-05,
      "loss": 0.5512,
      "step": 80100
    },
    {
      "epoch": 1.382830146386882,
      "grad_norm": 0.11737734079360962,
      "learning_rate": 1.4468748383537081e-05,
      "loss": 0.6165,
      "step": 80200
    },
    {
      "epoch": 1.3845543735020778,
      "grad_norm": 0.06900482624769211,
      "learning_rate": 1.4461851475076298e-05,
      "loss": 0.4305,
      "step": 80300
    },
    {
      "epoch": 1.3862786006172734,
      "grad_norm": 0.2338416576385498,
      "learning_rate": 1.4454954566615516e-05,
      "loss": 0.5663,
      "step": 80400
    },
    {
      "epoch": 1.388002827732469,
      "grad_norm": 0.24095185101032257,
      "learning_rate": 1.4448057658154733e-05,
      "loss": 0.8008,
      "step": 80500
    },
    {
      "epoch": 1.3897270548476646,
      "grad_norm": 0.2292691171169281,
      "learning_rate": 1.444116074969395e-05,
      "loss": 0.4892,
      "step": 80600
    },
    {
      "epoch": 1.3914512819628602,
      "grad_norm": 0.17458434402942657,
      "learning_rate": 1.4434263841233169e-05,
      "loss": 0.4536,
      "step": 80700
    },
    {
      "epoch": 1.3931755090780558,
      "grad_norm": 0.0352608896791935,
      "learning_rate": 1.4427366932772386e-05,
      "loss": 0.442,
      "step": 80800
    },
    {
      "epoch": 1.3948997361932514,
      "grad_norm": 0.19324365258216858,
      "learning_rate": 1.4420470024311604e-05,
      "loss": 0.508,
      "step": 80900
    },
    {
      "epoch": 1.396623963308447,
      "grad_norm": 0.24868552386760712,
      "learning_rate": 1.441357311585082e-05,
      "loss": 0.5442,
      "step": 81000
    },
    {
      "epoch": 1.3983481904236426,
      "grad_norm": 0.11426592618227005,
      "learning_rate": 1.4406676207390039e-05,
      "loss": 0.5547,
      "step": 81100
    },
    {
      "epoch": 1.4000724175388382,
      "grad_norm": 17.072490692138672,
      "learning_rate": 1.4399779298929255e-05,
      "loss": 0.4648,
      "step": 81200
    },
    {
      "epoch": 1.4017966446540338,
      "grad_norm": 0.1760355830192566,
      "learning_rate": 1.4392882390468475e-05,
      "loss": 0.6159,
      "step": 81300
    },
    {
      "epoch": 1.4035208717692296,
      "grad_norm": 24.726728439331055,
      "learning_rate": 1.4385985482007692e-05,
      "loss": 0.4743,
      "step": 81400
    },
    {
      "epoch": 1.405245098884425,
      "grad_norm": 0.2234983891248703,
      "learning_rate": 1.4379088573546908e-05,
      "loss": 0.5461,
      "step": 81500
    },
    {
      "epoch": 1.4069693259996208,
      "grad_norm": 0.29367396235466003,
      "learning_rate": 1.4372191665086126e-05,
      "loss": 0.6112,
      "step": 81600
    },
    {
      "epoch": 1.4086935531148164,
      "grad_norm": 0.10857505351305008,
      "learning_rate": 1.4365294756625343e-05,
      "loss": 0.4966,
      "step": 81700
    },
    {
      "epoch": 1.410417780230012,
      "grad_norm": 0.2583138942718506,
      "learning_rate": 1.4358397848164563e-05,
      "loss": 0.6532,
      "step": 81800
    },
    {
      "epoch": 1.4121420073452076,
      "grad_norm": 2.4699428990970773e-07,
      "learning_rate": 1.435150093970378e-05,
      "loss": 0.6139,
      "step": 81900
    },
    {
      "epoch": 1.4138662344604032,
      "grad_norm": 0.16958832740783691,
      "learning_rate": 1.4344604031242997e-05,
      "loss": 0.3161,
      "step": 82000
    },
    {
      "epoch": 1.4155904615755988,
      "grad_norm": 2.7914697398045973e-07,
      "learning_rate": 1.4337707122782214e-05,
      "loss": 0.5219,
      "step": 82100
    },
    {
      "epoch": 1.4173146886907944,
      "grad_norm": 2.3111392977170908e-07,
      "learning_rate": 1.433081021432143e-05,
      "loss": 0.6253,
      "step": 82200
    },
    {
      "epoch": 1.41903891580599,
      "grad_norm": 0.11727258563041687,
      "learning_rate": 1.4323913305860649e-05,
      "loss": 0.6223,
      "step": 82300
    },
    {
      "epoch": 1.4207631429211856,
      "grad_norm": 23.86087417602539,
      "learning_rate": 1.4317016397399867e-05,
      "loss": 0.3698,
      "step": 82400
    },
    {
      "epoch": 1.4224873700363811,
      "grad_norm": 0.11017074435949326,
      "learning_rate": 1.4310119488939085e-05,
      "loss": 0.3876,
      "step": 82500
    },
    {
      "epoch": 1.4242115971515767,
      "grad_norm": 24.203662872314453,
      "learning_rate": 1.4303222580478302e-05,
      "loss": 0.6441,
      "step": 82600
    },
    {
      "epoch": 1.4259358242667723,
      "grad_norm": 0.09796784073114395,
      "learning_rate": 1.429632567201752e-05,
      "loss": 0.6936,
      "step": 82700
    },
    {
      "epoch": 1.427660051381968,
      "grad_norm": 0.11820955574512482,
      "learning_rate": 1.4289428763556736e-05,
      "loss": 0.4464,
      "step": 82800
    },
    {
      "epoch": 1.4293842784971638,
      "grad_norm": 0.061036158353090286,
      "learning_rate": 1.4282531855095953e-05,
      "loss": 0.2841,
      "step": 82900
    },
    {
      "epoch": 1.4311085056123591,
      "grad_norm": 0.21074587106704712,
      "learning_rate": 1.4275634946635173e-05,
      "loss": 0.47,
      "step": 83000
    },
    {
      "epoch": 1.432832732727555,
      "grad_norm": 0.07837934792041779,
      "learning_rate": 1.426873803817439e-05,
      "loss": 0.3148,
      "step": 83100
    },
    {
      "epoch": 1.4345569598427506,
      "grad_norm": 0.08547456562519073,
      "learning_rate": 1.4261841129713607e-05,
      "loss": 0.3204,
      "step": 83200
    },
    {
      "epoch": 1.4362811869579462,
      "grad_norm": 0.06482624262571335,
      "learning_rate": 1.4254944221252824e-05,
      "loss": 0.3906,
      "step": 83300
    },
    {
      "epoch": 1.4380054140731418,
      "grad_norm": 28.192710876464844,
      "learning_rate": 1.4248047312792042e-05,
      "loss": 0.5739,
      "step": 83400
    },
    {
      "epoch": 1.4397296411883374,
      "grad_norm": 0.2139606773853302,
      "learning_rate": 1.4241150404331259e-05,
      "loss": 0.7277,
      "step": 83500
    },
    {
      "epoch": 1.441453868303533,
      "grad_norm": 0.07381648570299149,
      "learning_rate": 1.4234253495870479e-05,
      "loss": 0.4011,
      "step": 83600
    },
    {
      "epoch": 1.4431780954187285,
      "grad_norm": 0.07731292396783829,
      "learning_rate": 1.4227356587409695e-05,
      "loss": 0.5862,
      "step": 83700
    },
    {
      "epoch": 1.4449023225339241,
      "grad_norm": 26.17597007751465,
      "learning_rate": 1.4220459678948912e-05,
      "loss": 0.6923,
      "step": 83800
    },
    {
      "epoch": 1.4466265496491197,
      "grad_norm": 0.16637542843818665,
      "learning_rate": 1.421356277048813e-05,
      "loss": 0.6581,
      "step": 83900
    },
    {
      "epoch": 1.4483507767643153,
      "grad_norm": 29.906457901000977,
      "learning_rate": 1.4206665862027346e-05,
      "loss": 0.5124,
      "step": 84000
    },
    {
      "epoch": 1.450075003879511,
      "grad_norm": 0.3950381278991699,
      "learning_rate": 1.4199768953566565e-05,
      "loss": 0.6864,
      "step": 84100
    },
    {
      "epoch": 1.4517992309947068,
      "grad_norm": 0.07512688636779785,
      "learning_rate": 1.4192872045105783e-05,
      "loss": 0.4459,
      "step": 84200
    },
    {
      "epoch": 1.4535234581099021,
      "grad_norm": 0.027731478214263916,
      "learning_rate": 1.4185975136645001e-05,
      "loss": 0.7145,
      "step": 84300
    },
    {
      "epoch": 1.455247685225098,
      "grad_norm": 0.571986198425293,
      "learning_rate": 1.4179078228184218e-05,
      "loss": 0.5761,
      "step": 84400
    },
    {
      "epoch": 1.4569719123402933,
      "grad_norm": 4.2603167571542144e-07,
      "learning_rate": 1.4172181319723434e-05,
      "loss": 0.5238,
      "step": 84500
    },
    {
      "epoch": 1.4586961394554891,
      "grad_norm": 19.220046997070312,
      "learning_rate": 1.4165284411262652e-05,
      "loss": 0.4389,
      "step": 84600
    },
    {
      "epoch": 1.4604203665706847,
      "grad_norm": 0.08367518335580826,
      "learning_rate": 1.4158387502801869e-05,
      "loss": 0.6582,
      "step": 84700
    },
    {
      "epoch": 1.4621445936858803,
      "grad_norm": 0.06939811259508133,
      "learning_rate": 1.4151490594341089e-05,
      "loss": 0.475,
      "step": 84800
    },
    {
      "epoch": 1.463868820801076,
      "grad_norm": 0.09686297923326492,
      "learning_rate": 1.4144593685880305e-05,
      "loss": 0.366,
      "step": 84900
    },
    {
      "epoch": 1.4655930479162715,
      "grad_norm": 0.1386788785457611,
      "learning_rate": 1.4137696777419523e-05,
      "loss": 0.5686,
      "step": 85000
    },
    {
      "epoch": 1.4673172750314671,
      "grad_norm": 0.04902133718132973,
      "learning_rate": 1.413079986895874e-05,
      "loss": 0.36,
      "step": 85100
    },
    {
      "epoch": 1.4690415021466627,
      "grad_norm": 33.62373352050781,
      "learning_rate": 1.4123902960497956e-05,
      "loss": 0.5431,
      "step": 85200
    },
    {
      "epoch": 1.4707657292618583,
      "grad_norm": 0.1431739181280136,
      "learning_rate": 1.4117006052037176e-05,
      "loss": 0.4724,
      "step": 85300
    },
    {
      "epoch": 1.472489956377054,
      "grad_norm": 0.10942725092172623,
      "learning_rate": 1.4110109143576395e-05,
      "loss": 0.3912,
      "step": 85400
    },
    {
      "epoch": 1.4742141834922495,
      "grad_norm": 0.1229877918958664,
      "learning_rate": 1.4103212235115611e-05,
      "loss": 0.4739,
      "step": 85500
    },
    {
      "epoch": 1.4759384106074451,
      "grad_norm": 0.03882962465286255,
      "learning_rate": 1.4096315326654828e-05,
      "loss": 0.6029,
      "step": 85600
    },
    {
      "epoch": 1.477662637722641,
      "grad_norm": 0.04172874242067337,
      "learning_rate": 1.4089418418194046e-05,
      "loss": 0.3509,
      "step": 85700
    },
    {
      "epoch": 1.4793868648378363,
      "grad_norm": 34.10840606689453,
      "learning_rate": 1.4082521509733262e-05,
      "loss": 0.5437,
      "step": 85800
    },
    {
      "epoch": 1.4811110919530321,
      "grad_norm": 0.07069160044193268,
      "learning_rate": 1.4075624601272482e-05,
      "loss": 0.5084,
      "step": 85900
    },
    {
      "epoch": 1.4828353190682277,
      "grad_norm": 0.08717790246009827,
      "learning_rate": 1.4068727692811699e-05,
      "loss": 0.4596,
      "step": 86000
    },
    {
      "epoch": 1.4845595461834233,
      "grad_norm": 0.17384809255599976,
      "learning_rate": 1.4061830784350917e-05,
      "loss": 0.5612,
      "step": 86100
    },
    {
      "epoch": 1.486283773298619,
      "grad_norm": 0.19864869117736816,
      "learning_rate": 1.4054933875890133e-05,
      "loss": 0.4493,
      "step": 86200
    },
    {
      "epoch": 1.4880080004138145,
      "grad_norm": 0.12336429953575134,
      "learning_rate": 1.404803696742935e-05,
      "loss": 0.5463,
      "step": 86300
    },
    {
      "epoch": 1.4897322275290101,
      "grad_norm": 0.09759163111448288,
      "learning_rate": 1.4041140058968568e-05,
      "loss": 0.3893,
      "step": 86400
    },
    {
      "epoch": 1.4914564546442057,
      "grad_norm": 0.1396716833114624,
      "learning_rate": 1.4034243150507786e-05,
      "loss": 0.6386,
      "step": 86500
    },
    {
      "epoch": 1.4931806817594013,
      "grad_norm": 0.08738630264997482,
      "learning_rate": 1.4027346242047005e-05,
      "loss": 0.3886,
      "step": 86600
    },
    {
      "epoch": 1.494904908874597,
      "grad_norm": 0.0614696629345417,
      "learning_rate": 1.4020449333586221e-05,
      "loss": 0.366,
      "step": 86700
    },
    {
      "epoch": 1.4966291359897925,
      "grad_norm": 29.07469940185547,
      "learning_rate": 1.4013552425125438e-05,
      "loss": 0.596,
      "step": 86800
    },
    {
      "epoch": 1.4983533631049881,
      "grad_norm": 0.2597777843475342,
      "learning_rate": 1.4006655516664656e-05,
      "loss": 0.7795,
      "step": 86900
    },
    {
      "epoch": 1.500077590220184,
      "grad_norm": 0.46476316452026367,
      "learning_rate": 1.3999758608203872e-05,
      "loss": 0.5766,
      "step": 87000
    },
    {
      "epoch": 1.5018018173353793,
      "grad_norm": 0.031346555799245834,
      "learning_rate": 1.3992861699743092e-05,
      "loss": 0.6271,
      "step": 87100
    },
    {
      "epoch": 1.5035260444505751,
      "grad_norm": 23.0332088470459,
      "learning_rate": 1.3985964791282309e-05,
      "loss": 0.6199,
      "step": 87200
    },
    {
      "epoch": 1.5052502715657705,
      "grad_norm": 0.08987607806921005,
      "learning_rate": 1.3979067882821527e-05,
      "loss": 0.5654,
      "step": 87300
    },
    {
      "epoch": 1.5069744986809663,
      "grad_norm": 0.019627876579761505,
      "learning_rate": 1.3972170974360743e-05,
      "loss": 0.2684,
      "step": 87400
    },
    {
      "epoch": 1.5086987257961617,
      "grad_norm": 28.43091583251953,
      "learning_rate": 1.396527406589996e-05,
      "loss": 0.5393,
      "step": 87500
    },
    {
      "epoch": 1.5104229529113575,
      "grad_norm": 0.22623740136623383,
      "learning_rate": 1.3958377157439178e-05,
      "loss": 0.7487,
      "step": 87600
    },
    {
      "epoch": 1.5121471800265531,
      "grad_norm": 0.17255525290966034,
      "learning_rate": 1.3951480248978398e-05,
      "loss": 0.5739,
      "step": 87700
    },
    {
      "epoch": 1.5138714071417487,
      "grad_norm": 0.0656760111451149,
      "learning_rate": 1.3944583340517615e-05,
      "loss": 0.3839,
      "step": 87800
    },
    {
      "epoch": 1.5155956342569443,
      "grad_norm": 0.15854862332344055,
      "learning_rate": 1.3937686432056831e-05,
      "loss": 0.6656,
      "step": 87900
    },
    {
      "epoch": 1.51731986137214,
      "grad_norm": 0.20713858306407928,
      "learning_rate": 1.393078952359605e-05,
      "loss": 0.4318,
      "step": 88000
    },
    {
      "epoch": 1.5190440884873355,
      "grad_norm": 0.08380379527807236,
      "learning_rate": 1.3923892615135266e-05,
      "loss": 0.4473,
      "step": 88100
    },
    {
      "epoch": 1.5207683156025311,
      "grad_norm": 26.16441535949707,
      "learning_rate": 1.3916995706674486e-05,
      "loss": 0.5044,
      "step": 88200
    },
    {
      "epoch": 1.522492542717727,
      "grad_norm": 3.1751318374517723e-07,
      "learning_rate": 1.3910098798213702e-05,
      "loss": 0.509,
      "step": 88300
    },
    {
      "epoch": 1.5242167698329223,
      "grad_norm": 0.041988663375377655,
      "learning_rate": 1.390320188975292e-05,
      "loss": 0.4994,
      "step": 88400
    },
    {
      "epoch": 1.5259409969481181,
      "grad_norm": 0.39075884222984314,
      "learning_rate": 1.3896304981292137e-05,
      "loss": 0.7633,
      "step": 88500
    },
    {
      "epoch": 1.5276652240633135,
      "grad_norm": 0.11350907385349274,
      "learning_rate": 1.3889408072831354e-05,
      "loss": 0.4117,
      "step": 88600
    },
    {
      "epoch": 1.5293894511785093,
      "grad_norm": 0.28652772307395935,
      "learning_rate": 1.3882511164370572e-05,
      "loss": 0.3718,
      "step": 88700
    },
    {
      "epoch": 1.5311136782937047,
      "grad_norm": 0.12530891597270966,
      "learning_rate": 1.387561425590979e-05,
      "loss": 0.5919,
      "step": 88800
    },
    {
      "epoch": 1.5328379054089005,
      "grad_norm": 0.17880411446094513,
      "learning_rate": 1.3868717347449008e-05,
      "loss": 0.5879,
      "step": 88900
    },
    {
      "epoch": 1.5345621325240961,
      "grad_norm": 0.09368356317281723,
      "learning_rate": 1.3861820438988225e-05,
      "loss": 0.6603,
      "step": 89000
    },
    {
      "epoch": 1.5362863596392917,
      "grad_norm": 0.07608253508806229,
      "learning_rate": 1.3854923530527443e-05,
      "loss": 0.3981,
      "step": 89100
    },
    {
      "epoch": 1.5380105867544873,
      "grad_norm": 0.2690261900424957,
      "learning_rate": 1.384802662206666e-05,
      "loss": 0.4786,
      "step": 89200
    },
    {
      "epoch": 1.539734813869683,
      "grad_norm": 0.07668712735176086,
      "learning_rate": 1.3841129713605876e-05,
      "loss": 0.5419,
      "step": 89300
    },
    {
      "epoch": 1.5414590409848785,
      "grad_norm": 0.1117805615067482,
      "learning_rate": 1.3834232805145096e-05,
      "loss": 0.3727,
      "step": 89400
    },
    {
      "epoch": 1.5431832681000741,
      "grad_norm": 0.17439909279346466,
      "learning_rate": 1.3827335896684312e-05,
      "loss": 0.3966,
      "step": 89500
    },
    {
      "epoch": 1.5449074952152697,
      "grad_norm": 0.09148380160331726,
      "learning_rate": 1.382043898822353e-05,
      "loss": 0.6831,
      "step": 89600
    },
    {
      "epoch": 1.5466317223304653,
      "grad_norm": 31.483043670654297,
      "learning_rate": 1.3813542079762747e-05,
      "loss": 0.7218,
      "step": 89700
    },
    {
      "epoch": 1.5483559494456611,
      "grad_norm": 0.0953240916132927,
      "learning_rate": 1.3806645171301965e-05,
      "loss": 0.5997,
      "step": 89800
    },
    {
      "epoch": 1.5500801765608565,
      "grad_norm": 19.615909576416016,
      "learning_rate": 1.3799748262841182e-05,
      "loss": 0.4014,
      "step": 89900
    },
    {
      "epoch": 1.5518044036760523,
      "grad_norm": 0.3479824364185333,
      "learning_rate": 1.3792851354380402e-05,
      "loss": 0.4501,
      "step": 90000
    },
    {
      "epoch": 1.5535286307912477,
      "grad_norm": 0.20114140212535858,
      "learning_rate": 1.3785954445919618e-05,
      "loss": 0.5449,
      "step": 90100
    },
    {
      "epoch": 1.5552528579064435,
      "grad_norm": 0.06955011934041977,
      "learning_rate": 1.3779057537458835e-05,
      "loss": 0.4035,
      "step": 90200
    },
    {
      "epoch": 1.556977085021639,
      "grad_norm": 0.22545340657234192,
      "learning_rate": 1.3772160628998053e-05,
      "loss": 0.5001,
      "step": 90300
    },
    {
      "epoch": 1.5587013121368347,
      "grad_norm": 0.12262845784425735,
      "learning_rate": 1.376526372053727e-05,
      "loss": 0.5342,
      "step": 90400
    },
    {
      "epoch": 1.5604255392520303,
      "grad_norm": 20.766685485839844,
      "learning_rate": 1.3758366812076488e-05,
      "loss": 0.6203,
      "step": 90500
    },
    {
      "epoch": 1.562149766367226,
      "grad_norm": 0.06244030222296715,
      "learning_rate": 1.3751469903615706e-05,
      "loss": 0.4332,
      "step": 90600
    },
    {
      "epoch": 1.5638739934824215,
      "grad_norm": 22.845367431640625,
      "learning_rate": 1.3744572995154924e-05,
      "loss": 0.4965,
      "step": 90700
    },
    {
      "epoch": 1.565598220597617,
      "grad_norm": 31.22080421447754,
      "learning_rate": 1.373767608669414e-05,
      "loss": 0.6167,
      "step": 90800
    },
    {
      "epoch": 1.5673224477128127,
      "grad_norm": 0.07472102344036102,
      "learning_rate": 1.3730779178233357e-05,
      "loss": 0.2807,
      "step": 90900
    },
    {
      "epoch": 1.5690466748280083,
      "grad_norm": 6.906800109618416e-08,
      "learning_rate": 1.3723882269772575e-05,
      "loss": 0.5914,
      "step": 91000
    },
    {
      "epoch": 1.5707709019432041,
      "grad_norm": 28.965293884277344,
      "learning_rate": 1.3716985361311792e-05,
      "loss": 0.4673,
      "step": 91100
    },
    {
      "epoch": 1.5724951290583995,
      "grad_norm": 0.23354001343250275,
      "learning_rate": 1.3710088452851012e-05,
      "loss": 0.5089,
      "step": 91200
    },
    {
      "epoch": 1.5742193561735953,
      "grad_norm": 31.726573944091797,
      "learning_rate": 1.3703191544390228e-05,
      "loss": 0.4305,
      "step": 91300
    },
    {
      "epoch": 1.5759435832887907,
      "grad_norm": 0.29639732837677,
      "learning_rate": 1.3696294635929446e-05,
      "loss": 0.6041,
      "step": 91400
    },
    {
      "epoch": 1.5776678104039865,
      "grad_norm": 0.07516583800315857,
      "learning_rate": 1.3689397727468663e-05,
      "loss": 0.6152,
      "step": 91500
    },
    {
      "epoch": 1.579392037519182,
      "grad_norm": 0.32503700256347656,
      "learning_rate": 1.368250081900788e-05,
      "loss": 0.6807,
      "step": 91600
    },
    {
      "epoch": 1.5811162646343777,
      "grad_norm": 0.22221235930919647,
      "learning_rate": 1.36756039105471e-05,
      "loss": 0.4613,
      "step": 91700
    },
    {
      "epoch": 1.5828404917495733,
      "grad_norm": 0.24603427946567535,
      "learning_rate": 1.3668707002086316e-05,
      "loss": 0.5119,
      "step": 91800
    },
    {
      "epoch": 1.584564718864769,
      "grad_norm": 0.0385122150182724,
      "learning_rate": 1.3661810093625534e-05,
      "loss": 0.5032,
      "step": 91900
    },
    {
      "epoch": 1.5862889459799645,
      "grad_norm": 0.25903403759002686,
      "learning_rate": 1.365491318516475e-05,
      "loss": 0.5905,
      "step": 92000
    },
    {
      "epoch": 1.58801317309516,
      "grad_norm": 0.07518042623996735,
      "learning_rate": 1.3648016276703969e-05,
      "loss": 0.433,
      "step": 92100
    },
    {
      "epoch": 1.5897374002103557,
      "grad_norm": 2.2487813566840487e-07,
      "learning_rate": 1.3641119368243185e-05,
      "loss": 0.5124,
      "step": 92200
    },
    {
      "epoch": 1.5914616273255513,
      "grad_norm": 0.16351668536663055,
      "learning_rate": 1.3634222459782405e-05,
      "loss": 0.4345,
      "step": 92300
    },
    {
      "epoch": 1.593185854440747,
      "grad_norm": 0.24654020369052887,
      "learning_rate": 1.3627325551321622e-05,
      "loss": 0.7709,
      "step": 92400
    },
    {
      "epoch": 1.5949100815559425,
      "grad_norm": 0.2923048138618469,
      "learning_rate": 1.3620428642860838e-05,
      "loss": 0.4623,
      "step": 92500
    },
    {
      "epoch": 1.5966343086711383,
      "grad_norm": 0.04381503909826279,
      "learning_rate": 1.3613531734400057e-05,
      "loss": 0.2348,
      "step": 92600
    },
    {
      "epoch": 1.5983585357863337,
      "grad_norm": 0.15785852074623108,
      "learning_rate": 1.3606634825939273e-05,
      "loss": 0.5364,
      "step": 92700
    },
    {
      "epoch": 1.6000827629015295,
      "grad_norm": 0.20068812370300293,
      "learning_rate": 1.3599737917478491e-05,
      "loss": 0.4559,
      "step": 92800
    },
    {
      "epoch": 1.6018069900167249,
      "grad_norm": 0.18617582321166992,
      "learning_rate": 1.359284100901771e-05,
      "loss": 0.5183,
      "step": 92900
    },
    {
      "epoch": 1.6035312171319207,
      "grad_norm": 4.7136495595623273e-08,
      "learning_rate": 1.3585944100556928e-05,
      "loss": 0.5099,
      "step": 93000
    },
    {
      "epoch": 1.605255444247116,
      "grad_norm": 30.759235382080078,
      "learning_rate": 1.3579047192096144e-05,
      "loss": 0.5828,
      "step": 93100
    },
    {
      "epoch": 1.606979671362312,
      "grad_norm": 24.34117317199707,
      "learning_rate": 1.357215028363536e-05,
      "loss": 0.4869,
      "step": 93200
    },
    {
      "epoch": 1.6087038984775075,
      "grad_norm": 0.1368582397699356,
      "learning_rate": 1.3565253375174579e-05,
      "loss": 0.4126,
      "step": 93300
    },
    {
      "epoch": 1.610428125592703,
      "grad_norm": 0.3080313801765442,
      "learning_rate": 1.3558356466713795e-05,
      "loss": 0.5582,
      "step": 93400
    },
    {
      "epoch": 1.6121523527078987,
      "grad_norm": 0.32184088230133057,
      "learning_rate": 1.3551459558253015e-05,
      "loss": 0.4537,
      "step": 93500
    },
    {
      "epoch": 1.6138765798230943,
      "grad_norm": 0.059137407690286636,
      "learning_rate": 1.3544562649792232e-05,
      "loss": 0.5424,
      "step": 93600
    },
    {
      "epoch": 1.61560080693829,
      "grad_norm": 0.0371350534260273,
      "learning_rate": 1.353766574133145e-05,
      "loss": 0.4602,
      "step": 93700
    },
    {
      "epoch": 1.6173250340534855,
      "grad_norm": 0.07116417586803436,
      "learning_rate": 1.3530768832870667e-05,
      "loss": 0.6412,
      "step": 93800
    },
    {
      "epoch": 1.619049261168681,
      "grad_norm": 0.06812910735607147,
      "learning_rate": 1.3523871924409883e-05,
      "loss": 0.4827,
      "step": 93900
    },
    {
      "epoch": 1.6207734882838767,
      "grad_norm": 0.0391024611890316,
      "learning_rate": 1.3516975015949101e-05,
      "loss": 0.5995,
      "step": 94000
    },
    {
      "epoch": 1.6224977153990725,
      "grad_norm": 20.8408260345459,
      "learning_rate": 1.351007810748832e-05,
      "loss": 0.3697,
      "step": 94100
    },
    {
      "epoch": 1.6242219425142679,
      "grad_norm": 6.271388031109382e-08,
      "learning_rate": 1.3503181199027538e-05,
      "loss": 0.5871,
      "step": 94200
    },
    {
      "epoch": 1.6259461696294637,
      "grad_norm": 21.702041625976562,
      "learning_rate": 1.3496284290566754e-05,
      "loss": 0.6071,
      "step": 94300
    },
    {
      "epoch": 1.627670396744659,
      "grad_norm": 0.11697763949632645,
      "learning_rate": 1.3489387382105972e-05,
      "loss": 0.4391,
      "step": 94400
    },
    {
      "epoch": 1.629394623859855,
      "grad_norm": 25.284704208374023,
      "learning_rate": 1.3482490473645189e-05,
      "loss": 0.5291,
      "step": 94500
    },
    {
      "epoch": 1.6311188509750503,
      "grad_norm": 0.03622635453939438,
      "learning_rate": 1.3475593565184405e-05,
      "loss": 0.6633,
      "step": 94600
    },
    {
      "epoch": 1.632843078090246,
      "grad_norm": 0.12209221720695496,
      "learning_rate": 1.3468696656723625e-05,
      "loss": 0.5495,
      "step": 94700
    },
    {
      "epoch": 1.6345673052054417,
      "grad_norm": 30.92694664001465,
      "learning_rate": 1.3461799748262842e-05,
      "loss": 0.6357,
      "step": 94800
    },
    {
      "epoch": 1.6362915323206373,
      "grad_norm": 0.21942488849163055,
      "learning_rate": 1.345490283980206e-05,
      "loss": 0.6436,
      "step": 94900
    },
    {
      "epoch": 1.6380157594358329,
      "grad_norm": 0.4777907729148865,
      "learning_rate": 1.3448005931341277e-05,
      "loss": 0.3004,
      "step": 95000
    },
    {
      "epoch": 1.6397399865510285,
      "grad_norm": 0.23441281914710999,
      "learning_rate": 1.3441109022880495e-05,
      "loss": 0.4167,
      "step": 95100
    },
    {
      "epoch": 1.641464213666224,
      "grad_norm": 0.21021026372909546,
      "learning_rate": 1.3434212114419713e-05,
      "loss": 0.801,
      "step": 95200
    },
    {
      "epoch": 1.6431884407814197,
      "grad_norm": 0.2312358319759369,
      "learning_rate": 1.3427315205958931e-05,
      "loss": 0.4572,
      "step": 95300
    },
    {
      "epoch": 1.6449126678966155,
      "grad_norm": 0.22620151937007904,
      "learning_rate": 1.3420418297498148e-05,
      "loss": 0.5089,
      "step": 95400
    },
    {
      "epoch": 1.6466368950118109,
      "grad_norm": 0.0220425333827734,
      "learning_rate": 1.3413521389037364e-05,
      "loss": 0.3873,
      "step": 95500
    },
    {
      "epoch": 1.6483611221270067,
      "grad_norm": 31.17864418029785,
      "learning_rate": 1.3406624480576582e-05,
      "loss": 0.4534,
      "step": 95600
    },
    {
      "epoch": 1.650085349242202,
      "grad_norm": 0.06333307921886444,
      "learning_rate": 1.3399727572115799e-05,
      "loss": 0.5502,
      "step": 95700
    },
    {
      "epoch": 1.651809576357398,
      "grad_norm": 0.4281170964241028,
      "learning_rate": 1.3392830663655019e-05,
      "loss": 0.4711,
      "step": 95800
    },
    {
      "epoch": 1.6535338034725933,
      "grad_norm": 0.11439735442399979,
      "learning_rate": 1.3385933755194235e-05,
      "loss": 0.464,
      "step": 95900
    },
    {
      "epoch": 1.655258030587789,
      "grad_norm": 31.209348678588867,
      "learning_rate": 1.3379036846733454e-05,
      "loss": 0.5132,
      "step": 96000
    },
    {
      "epoch": 1.6569822577029847,
      "grad_norm": 0.18537214398384094,
      "learning_rate": 1.337213993827267e-05,
      "loss": 0.3818,
      "step": 96100
    },
    {
      "epoch": 1.6587064848181803,
      "grad_norm": 0.09339480102062225,
      "learning_rate": 1.3365243029811887e-05,
      "loss": 0.4283,
      "step": 96200
    },
    {
      "epoch": 1.6604307119333759,
      "grad_norm": 0.1734444946050644,
      "learning_rate": 1.3358346121351105e-05,
      "loss": 0.5272,
      "step": 96300
    },
    {
      "epoch": 1.6621549390485715,
      "grad_norm": 0.11679874360561371,
      "learning_rate": 1.3351449212890323e-05,
      "loss": 0.3695,
      "step": 96400
    },
    {
      "epoch": 1.663879166163767,
      "grad_norm": 25.754032135009766,
      "learning_rate": 1.3344552304429541e-05,
      "loss": 0.5084,
      "step": 96500
    },
    {
      "epoch": 1.6656033932789627,
      "grad_norm": 0.22837510704994202,
      "learning_rate": 1.3337655395968758e-05,
      "loss": 0.6318,
      "step": 96600
    },
    {
      "epoch": 1.6673276203941583,
      "grad_norm": 0.15146850049495697,
      "learning_rate": 1.3330758487507976e-05,
      "loss": 0.6123,
      "step": 96700
    },
    {
      "epoch": 1.6690518475093539,
      "grad_norm": 0.11706845462322235,
      "learning_rate": 1.3323861579047193e-05,
      "loss": 0.49,
      "step": 96800
    },
    {
      "epoch": 1.6707760746245497,
      "grad_norm": 0.025565868243575096,
      "learning_rate": 1.3316964670586409e-05,
      "loss": 0.5955,
      "step": 96900
    },
    {
      "epoch": 1.672500301739745,
      "grad_norm": 0.06743480265140533,
      "learning_rate": 1.3310067762125629e-05,
      "loss": 0.4579,
      "step": 97000
    },
    {
      "epoch": 1.6742245288549409,
      "grad_norm": 0.09769120067358017,
      "learning_rate": 1.3303170853664845e-05,
      "loss": 0.3967,
      "step": 97100
    },
    {
      "epoch": 1.6759487559701363,
      "grad_norm": 0.021975014358758926,
      "learning_rate": 1.3296273945204064e-05,
      "loss": 0.3538,
      "step": 97200
    },
    {
      "epoch": 1.677672983085332,
      "grad_norm": 0.0761813074350357,
      "learning_rate": 1.328937703674328e-05,
      "loss": 0.6531,
      "step": 97300
    },
    {
      "epoch": 1.6793972102005275,
      "grad_norm": 0.13052335381507874,
      "learning_rate": 1.3282480128282498e-05,
      "loss": 0.5369,
      "step": 97400
    },
    {
      "epoch": 1.6811214373157233,
      "grad_norm": 0.046889930963516235,
      "learning_rate": 1.3275583219821715e-05,
      "loss": 0.5284,
      "step": 97500
    },
    {
      "epoch": 1.6828456644309189,
      "grad_norm": 21.166927337646484,
      "learning_rate": 1.3268686311360935e-05,
      "loss": 0.543,
      "step": 97600
    },
    {
      "epoch": 1.6845698915461145,
      "grad_norm": 0.296172171831131,
      "learning_rate": 1.3261789402900151e-05,
      "loss": 0.4284,
      "step": 97700
    },
    {
      "epoch": 1.68629411866131,
      "grad_norm": 0.21273848414421082,
      "learning_rate": 1.3254892494439368e-05,
      "loss": 0.5223,
      "step": 97800
    },
    {
      "epoch": 1.6880183457765057,
      "grad_norm": 0.04532140493392944,
      "learning_rate": 1.3247995585978586e-05,
      "loss": 0.4406,
      "step": 97900
    },
    {
      "epoch": 1.6897425728917013,
      "grad_norm": 23.258445739746094,
      "learning_rate": 1.3241098677517803e-05,
      "loss": 0.4761,
      "step": 98000
    },
    {
      "epoch": 1.6914668000068969,
      "grad_norm": 0.37891003489494324,
      "learning_rate": 1.3234201769057022e-05,
      "loss": 0.4784,
      "step": 98100
    },
    {
      "epoch": 1.6931910271220927,
      "grad_norm": 28.15189552307129,
      "learning_rate": 1.3227304860596239e-05,
      "loss": 0.5589,
      "step": 98200
    },
    {
      "epoch": 1.694915254237288,
      "grad_norm": 0.5043702721595764,
      "learning_rate": 1.3220407952135457e-05,
      "loss": 0.7375,
      "step": 98300
    },
    {
      "epoch": 1.6966394813524839,
      "grad_norm": 0.11340557783842087,
      "learning_rate": 1.3213511043674674e-05,
      "loss": 0.3536,
      "step": 98400
    },
    {
      "epoch": 1.6983637084676793,
      "grad_norm": 27.51388168334961,
      "learning_rate": 1.320661413521389e-05,
      "loss": 0.2939,
      "step": 98500
    },
    {
      "epoch": 1.700087935582875,
      "grad_norm": 0.10532428324222565,
      "learning_rate": 1.3199717226753108e-05,
      "loss": 0.4346,
      "step": 98600
    },
    {
      "epoch": 1.7018121626980705,
      "grad_norm": 22.57655906677246,
      "learning_rate": 1.3192820318292328e-05,
      "loss": 0.3723,
      "step": 98700
    },
    {
      "epoch": 1.7035363898132663,
      "grad_norm": 0.12358399480581284,
      "learning_rate": 1.3185923409831545e-05,
      "loss": 0.311,
      "step": 98800
    },
    {
      "epoch": 1.7052606169284619,
      "grad_norm": 0.06251529604196548,
      "learning_rate": 1.3179026501370761e-05,
      "loss": 0.5262,
      "step": 98900
    },
    {
      "epoch": 1.7069848440436575,
      "grad_norm": 0.09763596951961517,
      "learning_rate": 1.317212959290998e-05,
      "loss": 0.3087,
      "step": 99000
    },
    {
      "epoch": 1.708709071158853,
      "grad_norm": 0.06229093670845032,
      "learning_rate": 1.3165232684449196e-05,
      "loss": 0.5957,
      "step": 99100
    },
    {
      "epoch": 1.7104332982740487,
      "grad_norm": 0.243672713637352,
      "learning_rate": 1.3158335775988413e-05,
      "loss": 0.5701,
      "step": 99200
    },
    {
      "epoch": 1.7121575253892443,
      "grad_norm": 27.551889419555664,
      "learning_rate": 1.3151438867527633e-05,
      "loss": 0.4778,
      "step": 99300
    },
    {
      "epoch": 1.7138817525044399,
      "grad_norm": 0.04084799066185951,
      "learning_rate": 1.314454195906685e-05,
      "loss": 0.377,
      "step": 99400
    },
    {
      "epoch": 1.7156059796196355,
      "grad_norm": 0.028541773557662964,
      "learning_rate": 1.3137645050606067e-05,
      "loss": 0.5047,
      "step": 99500
    },
    {
      "epoch": 1.717330206734831,
      "grad_norm": 0.29186204075813293,
      "learning_rate": 1.3130748142145284e-05,
      "loss": 0.6667,
      "step": 99600
    },
    {
      "epoch": 1.7190544338500269,
      "grad_norm": 0.20842772722244263,
      "learning_rate": 1.3123851233684502e-05,
      "loss": 0.4851,
      "step": 99700
    },
    {
      "epoch": 1.7207786609652223,
      "grad_norm": 0.15979546308517456,
      "learning_rate": 1.3116954325223719e-05,
      "loss": 0.6507,
      "step": 99800
    },
    {
      "epoch": 1.722502888080418,
      "grad_norm": 0.2191811203956604,
      "learning_rate": 1.3110057416762938e-05,
      "loss": 0.351,
      "step": 99900
    },
    {
      "epoch": 1.7242271151956134,
      "grad_norm": 0.023658886551856995,
      "learning_rate": 1.3103160508302155e-05,
      "loss": 0.5996,
      "step": 100000
    },
    {
      "epoch": 1.7259513423108093,
      "grad_norm": 0.07129406183958054,
      "learning_rate": 1.3096263599841373e-05,
      "loss": 0.4547,
      "step": 100100
    },
    {
      "epoch": 1.7276755694260046,
      "grad_norm": 0.04919437691569328,
      "learning_rate": 1.308936669138059e-05,
      "loss": 0.3592,
      "step": 100200
    },
    {
      "epoch": 1.7293997965412005,
      "grad_norm": 0.23038913309574127,
      "learning_rate": 1.3082469782919806e-05,
      "loss": 0.722,
      "step": 100300
    },
    {
      "epoch": 1.731124023656396,
      "grad_norm": 28.842382431030273,
      "learning_rate": 1.3075572874459024e-05,
      "loss": 0.2303,
      "step": 100400
    },
    {
      "epoch": 1.7328482507715917,
      "grad_norm": 0.015617488883435726,
      "learning_rate": 1.3068675965998243e-05,
      "loss": 0.4708,
      "step": 100500
    },
    {
      "epoch": 1.7345724778867873,
      "grad_norm": 25.055850982666016,
      "learning_rate": 1.306177905753746e-05,
      "loss": 0.5835,
      "step": 100600
    },
    {
      "epoch": 1.7362967050019829,
      "grad_norm": 1.9561240094390087e-07,
      "learning_rate": 1.3054882149076677e-05,
      "loss": 0.4828,
      "step": 100700
    },
    {
      "epoch": 1.7380209321171785,
      "grad_norm": 0.1843988001346588,
      "learning_rate": 1.3047985240615894e-05,
      "loss": 0.4866,
      "step": 100800
    },
    {
      "epoch": 1.739745159232374,
      "grad_norm": 0.25501278042793274,
      "learning_rate": 1.3041088332155112e-05,
      "loss": 0.7001,
      "step": 100900
    },
    {
      "epoch": 1.7414693863475696,
      "grad_norm": 0.10968010872602463,
      "learning_rate": 1.3034191423694329e-05,
      "loss": 0.3969,
      "step": 101000
    },
    {
      "epoch": 1.7431936134627652,
      "grad_norm": 0.07610368728637695,
      "learning_rate": 1.3027294515233548e-05,
      "loss": 0.5827,
      "step": 101100
    },
    {
      "epoch": 1.744917840577961,
      "grad_norm": 29.965421676635742,
      "learning_rate": 1.3020397606772765e-05,
      "loss": 0.4488,
      "step": 101200
    },
    {
      "epoch": 1.7466420676931564,
      "grad_norm": 0.1225069910287857,
      "learning_rate": 1.3013500698311983e-05,
      "loss": 0.6117,
      "step": 101300
    },
    {
      "epoch": 1.7483662948083523,
      "grad_norm": 0.03108508698642254,
      "learning_rate": 1.30066037898512e-05,
      "loss": 0.3799,
      "step": 101400
    },
    {
      "epoch": 1.7500905219235476,
      "grad_norm": 0.08821766078472137,
      "learning_rate": 1.2999706881390416e-05,
      "loss": 0.6727,
      "step": 101500
    },
    {
      "epoch": 1.7518147490387435,
      "grad_norm": 0.17049701511859894,
      "learning_rate": 1.2992809972929636e-05,
      "loss": 0.4348,
      "step": 101600
    },
    {
      "epoch": 1.7535389761539388,
      "grad_norm": 0.17491625249385834,
      "learning_rate": 1.2985913064468854e-05,
      "loss": 0.4962,
      "step": 101700
    },
    {
      "epoch": 1.7552632032691347,
      "grad_norm": 0.1490325927734375,
      "learning_rate": 1.2979016156008071e-05,
      "loss": 0.3146,
      "step": 101800
    },
    {
      "epoch": 1.7569874303843303,
      "grad_norm": 33.85011291503906,
      "learning_rate": 1.2972119247547287e-05,
      "loss": 0.5974,
      "step": 101900
    },
    {
      "epoch": 1.7587116574995258,
      "grad_norm": 0.1411101073026657,
      "learning_rate": 1.2965222339086506e-05,
      "loss": 0.6398,
      "step": 102000
    },
    {
      "epoch": 1.7604358846147214,
      "grad_norm": 0.2162001132965088,
      "learning_rate": 1.2958325430625722e-05,
      "loss": 0.4646,
      "step": 102100
    },
    {
      "epoch": 1.762160111729917,
      "grad_norm": 0.2125290483236313,
      "learning_rate": 1.2951428522164942e-05,
      "loss": 0.4244,
      "step": 102200
    },
    {
      "epoch": 1.7638843388451126,
      "grad_norm": 0.024679381400346756,
      "learning_rate": 1.2944531613704159e-05,
      "loss": 0.5647,
      "step": 102300
    },
    {
      "epoch": 1.7656085659603082,
      "grad_norm": 0.044838715344667435,
      "learning_rate": 1.2937634705243377e-05,
      "loss": 0.4611,
      "step": 102400
    },
    {
      "epoch": 1.767332793075504,
      "grad_norm": 0.26819396018981934,
      "learning_rate": 1.2930737796782593e-05,
      "loss": 0.529,
      "step": 102500
    },
    {
      "epoch": 1.7690570201906994,
      "grad_norm": 0.09223757684230804,
      "learning_rate": 1.292384088832181e-05,
      "loss": 0.4144,
      "step": 102600
    },
    {
      "epoch": 1.7707812473058953,
      "grad_norm": 0.17964114248752594,
      "learning_rate": 1.2916943979861028e-05,
      "loss": 0.5157,
      "step": 102700
    },
    {
      "epoch": 1.7725054744210906,
      "grad_norm": 0.05584865063428879,
      "learning_rate": 1.2910047071400246e-05,
      "loss": 0.4305,
      "step": 102800
    },
    {
      "epoch": 1.7742297015362865,
      "grad_norm": 0.09253667294979095,
      "learning_rate": 1.2903150162939464e-05,
      "loss": 0.4781,
      "step": 102900
    },
    {
      "epoch": 1.7759539286514818,
      "grad_norm": 0.1511167734861374,
      "learning_rate": 1.2896253254478681e-05,
      "loss": 0.846,
      "step": 103000
    },
    {
      "epoch": 1.7776781557666776,
      "grad_norm": 0.05447153374552727,
      "learning_rate": 1.2889356346017899e-05,
      "loss": 0.3696,
      "step": 103100
    },
    {
      "epoch": 1.7794023828818732,
      "grad_norm": 26.28807830810547,
      "learning_rate": 1.2882459437557116e-05,
      "loss": 0.4391,
      "step": 103200
    },
    {
      "epoch": 1.7811266099970688,
      "grad_norm": 0.06867165863513947,
      "learning_rate": 1.2875562529096332e-05,
      "loss": 0.4094,
      "step": 103300
    },
    {
      "epoch": 1.7828508371122644,
      "grad_norm": 0.16547273099422455,
      "learning_rate": 1.2868665620635552e-05,
      "loss": 0.5226,
      "step": 103400
    },
    {
      "epoch": 1.78457506422746,
      "grad_norm": 0.2907522916793823,
      "learning_rate": 1.2861768712174769e-05,
      "loss": 0.6059,
      "step": 103500
    },
    {
      "epoch": 1.7862992913426556,
      "grad_norm": 26.08692169189453,
      "learning_rate": 1.2854871803713987e-05,
      "loss": 0.4814,
      "step": 103600
    },
    {
      "epoch": 1.7880235184578512,
      "grad_norm": 0.22912073135375977,
      "learning_rate": 1.2847974895253203e-05,
      "loss": 0.6689,
      "step": 103700
    },
    {
      "epoch": 1.7897477455730468,
      "grad_norm": 0.10018593072891235,
      "learning_rate": 1.2841077986792422e-05,
      "loss": 0.5019,
      "step": 103800
    },
    {
      "epoch": 1.7914719726882424,
      "grad_norm": 0.227556049823761,
      "learning_rate": 1.2834181078331638e-05,
      "loss": 0.4909,
      "step": 103900
    },
    {
      "epoch": 1.7931961998034383,
      "grad_norm": 0.03872299939393997,
      "learning_rate": 1.2827284169870858e-05,
      "loss": 0.5539,
      "step": 104000
    },
    {
      "epoch": 1.7949204269186336,
      "grad_norm": 0.04865559563040733,
      "learning_rate": 1.2820387261410074e-05,
      "loss": 0.6083,
      "step": 104100
    },
    {
      "epoch": 1.7966446540338294,
      "grad_norm": 0.03888735547661781,
      "learning_rate": 1.2813490352949291e-05,
      "loss": 0.5478,
      "step": 104200
    },
    {
      "epoch": 1.7983688811490248,
      "grad_norm": 0.027137039229273796,
      "learning_rate": 1.280659344448851e-05,
      "loss": 0.5526,
      "step": 104300
    },
    {
      "epoch": 1.8000931082642206,
      "grad_norm": 23.961759567260742,
      "learning_rate": 1.2799696536027726e-05,
      "loss": 0.6156,
      "step": 104400
    },
    {
      "epoch": 1.801817335379416,
      "grad_norm": 0.016828399151563644,
      "learning_rate": 1.2792799627566946e-05,
      "loss": 0.3318,
      "step": 104500
    },
    {
      "epoch": 1.8035415624946118,
      "grad_norm": 0.13510681688785553,
      "learning_rate": 1.2785902719106162e-05,
      "loss": 0.6507,
      "step": 104600
    },
    {
      "epoch": 1.8052657896098074,
      "grad_norm": 0.05136915296316147,
      "learning_rate": 1.277900581064538e-05,
      "loss": 0.3616,
      "step": 104700
    },
    {
      "epoch": 1.806990016725003,
      "grad_norm": 0.07309027761220932,
      "learning_rate": 1.2772108902184597e-05,
      "loss": 0.4434,
      "step": 104800
    },
    {
      "epoch": 1.8087142438401986,
      "grad_norm": 0.029254520311951637,
      "learning_rate": 1.2765211993723813e-05,
      "loss": 0.4011,
      "step": 104900
    },
    {
      "epoch": 1.8104384709553942,
      "grad_norm": 0.16767431795597076,
      "learning_rate": 1.2758315085263032e-05,
      "loss": 0.5977,
      "step": 105000
    },
    {
      "epoch": 1.8121626980705898,
      "grad_norm": 27.194406509399414,
      "learning_rate": 1.275141817680225e-05,
      "loss": 0.6086,
      "step": 105100
    },
    {
      "epoch": 1.8138869251857854,
      "grad_norm": 0.1425204873085022,
      "learning_rate": 1.2744521268341468e-05,
      "loss": 0.5879,
      "step": 105200
    },
    {
      "epoch": 1.8156111523009812,
      "grad_norm": 0.11573690176010132,
      "learning_rate": 1.2737624359880684e-05,
      "loss": 0.5331,
      "step": 105300
    },
    {
      "epoch": 1.8173353794161766,
      "grad_norm": 0.1217397078871727,
      "learning_rate": 1.2730727451419903e-05,
      "loss": 0.6124,
      "step": 105400
    },
    {
      "epoch": 1.8190596065313724,
      "grad_norm": 0.03769298270344734,
      "learning_rate": 1.272383054295912e-05,
      "loss": 0.5426,
      "step": 105500
    },
    {
      "epoch": 1.8207838336465678,
      "grad_norm": 0.36212146282196045,
      "learning_rate": 1.2716933634498336e-05,
      "loss": 0.8271,
      "step": 105600
    },
    {
      "epoch": 1.8225080607617636,
      "grad_norm": 0.1753900796175003,
      "learning_rate": 1.2710036726037556e-05,
      "loss": 0.3022,
      "step": 105700
    },
    {
      "epoch": 1.824232287876959,
      "grad_norm": 0.21099740266799927,
      "learning_rate": 1.2703139817576772e-05,
      "loss": 0.477,
      "step": 105800
    },
    {
      "epoch": 1.8259565149921548,
      "grad_norm": 0.01539039146155119,
      "learning_rate": 1.269624290911599e-05,
      "loss": 0.4381,
      "step": 105900
    },
    {
      "epoch": 1.8276807421073504,
      "grad_norm": 0.1121734008193016,
      "learning_rate": 1.2689346000655207e-05,
      "loss": 0.2806,
      "step": 106000
    },
    {
      "epoch": 1.829404969222546,
      "grad_norm": 0.287140429019928,
      "learning_rate": 1.2682449092194425e-05,
      "loss": 0.5526,
      "step": 106100
    },
    {
      "epoch": 1.8311291963377416,
      "grad_norm": 0.07718881964683533,
      "learning_rate": 1.2675552183733642e-05,
      "loss": 0.3795,
      "step": 106200
    },
    {
      "epoch": 1.8328534234529372,
      "grad_norm": 0.12052493542432785,
      "learning_rate": 1.2668655275272862e-05,
      "loss": 0.2923,
      "step": 106300
    },
    {
      "epoch": 1.8345776505681328,
      "grad_norm": 0.22102418541908264,
      "learning_rate": 1.2661758366812078e-05,
      "loss": 0.4314,
      "step": 106400
    },
    {
      "epoch": 1.8363018776833284,
      "grad_norm": 0.3458719551563263,
      "learning_rate": 1.2654861458351295e-05,
      "loss": 0.5183,
      "step": 106500
    },
    {
      "epoch": 1.838026104798524,
      "grad_norm": 7.91157077628668e-08,
      "learning_rate": 1.2647964549890513e-05,
      "loss": 0.5622,
      "step": 106600
    },
    {
      "epoch": 1.8397503319137196,
      "grad_norm": 0.3178360164165497,
      "learning_rate": 1.264106764142973e-05,
      "loss": 0.5623,
      "step": 106700
    },
    {
      "epoch": 1.8414745590289154,
      "grad_norm": 0.048942141234874725,
      "learning_rate": 1.2634170732968947e-05,
      "loss": 0.5309,
      "step": 106800
    },
    {
      "epoch": 1.8431987861441108,
      "grad_norm": 0.11682306230068207,
      "learning_rate": 1.2627273824508166e-05,
      "loss": 0.5112,
      "step": 106900
    },
    {
      "epoch": 1.8449230132593066,
      "grad_norm": 0.11160139739513397,
      "learning_rate": 1.2620376916047384e-05,
      "loss": 0.3194,
      "step": 107000
    },
    {
      "epoch": 1.846647240374502,
      "grad_norm": 25.991138458251953,
      "learning_rate": 1.26134800075866e-05,
      "loss": 0.6693,
      "step": 107100
    },
    {
      "epoch": 1.8483714674896978,
      "grad_norm": 0.23724286258220673,
      "learning_rate": 1.2606583099125817e-05,
      "loss": 0.3336,
      "step": 107200
    },
    {
      "epoch": 1.8500956946048932,
      "grad_norm": 0.08007306605577469,
      "learning_rate": 1.2599686190665035e-05,
      "loss": 0.3138,
      "step": 107300
    },
    {
      "epoch": 1.851819921720089,
      "grad_norm": 0.16032640635967255,
      "learning_rate": 1.2592789282204252e-05,
      "loss": 0.5011,
      "step": 107400
    },
    {
      "epoch": 1.8535441488352846,
      "grad_norm": 0.09735960513353348,
      "learning_rate": 1.2585892373743472e-05,
      "loss": 0.5201,
      "step": 107500
    },
    {
      "epoch": 1.8552683759504802,
      "grad_norm": 1.729736993638653e-07,
      "learning_rate": 1.2578995465282688e-05,
      "loss": 0.5052,
      "step": 107600
    },
    {
      "epoch": 1.8569926030656758,
      "grad_norm": 0.1129593700170517,
      "learning_rate": 1.2572098556821906e-05,
      "loss": 0.509,
      "step": 107700
    },
    {
      "epoch": 1.8587168301808714,
      "grad_norm": 0.10858963429927826,
      "learning_rate": 1.2565201648361123e-05,
      "loss": 0.3831,
      "step": 107800
    },
    {
      "epoch": 1.860441057296067,
      "grad_norm": 0.13017016649246216,
      "learning_rate": 1.255830473990034e-05,
      "loss": 0.5075,
      "step": 107900
    },
    {
      "epoch": 1.8621652844112626,
      "grad_norm": 0.11389117687940598,
      "learning_rate": 1.255140783143956e-05,
      "loss": 0.4011,
      "step": 108000
    },
    {
      "epoch": 1.8638895115264584,
      "grad_norm": 0.10627706348896027,
      "learning_rate": 1.2544510922978776e-05,
      "loss": 0.5216,
      "step": 108100
    },
    {
      "epoch": 1.8656137386416538,
      "grad_norm": 0.014494827948510647,
      "learning_rate": 1.2537614014517994e-05,
      "loss": 0.4869,
      "step": 108200
    },
    {
      "epoch": 1.8673379657568496,
      "grad_norm": 0.07203511148691177,
      "learning_rate": 1.253071710605721e-05,
      "loss": 0.5202,
      "step": 108300
    },
    {
      "epoch": 1.869062192872045,
      "grad_norm": 0.09657321870326996,
      "learning_rate": 1.2523820197596429e-05,
      "loss": 0.6077,
      "step": 108400
    },
    {
      "epoch": 1.8707864199872408,
      "grad_norm": 0.13128690421581268,
      "learning_rate": 1.2516923289135645e-05,
      "loss": 0.7269,
      "step": 108500
    },
    {
      "epoch": 1.8725106471024362,
      "grad_norm": 28.545940399169922,
      "learning_rate": 1.2510026380674865e-05,
      "loss": 0.5107,
      "step": 108600
    },
    {
      "epoch": 1.874234874217632,
      "grad_norm": 0.3157186508178711,
      "learning_rate": 1.2503129472214082e-05,
      "loss": 0.3431,
      "step": 108700
    },
    {
      "epoch": 1.8759591013328274,
      "grad_norm": 0.13981662690639496,
      "learning_rate": 1.2496232563753298e-05,
      "loss": 0.626,
      "step": 108800
    },
    {
      "epoch": 1.8776833284480232,
      "grad_norm": 0.03489243984222412,
      "learning_rate": 1.2489335655292516e-05,
      "loss": 0.7761,
      "step": 108900
    },
    {
      "epoch": 1.8794075555632188,
      "grad_norm": 0.199432834982872,
      "learning_rate": 1.2482438746831733e-05,
      "loss": 0.4715,
      "step": 109000
    },
    {
      "epoch": 1.8811317826784144,
      "grad_norm": 0.08197317272424698,
      "learning_rate": 1.2475541838370951e-05,
      "loss": 0.4682,
      "step": 109100
    },
    {
      "epoch": 1.88285600979361,
      "grad_norm": 0.02369292639195919,
      "learning_rate": 1.246864492991017e-05,
      "loss": 0.5557,
      "step": 109200
    },
    {
      "epoch": 1.8845802369088056,
      "grad_norm": 2.9086893960084126e-07,
      "learning_rate": 1.2461748021449387e-05,
      "loss": 0.4668,
      "step": 109300
    },
    {
      "epoch": 1.8863044640240012,
      "grad_norm": 0.24175596237182617,
      "learning_rate": 1.2454851112988604e-05,
      "loss": 0.3636,
      "step": 109400
    },
    {
      "epoch": 1.8880286911391968,
      "grad_norm": 0.22224652767181396,
      "learning_rate": 1.244795420452782e-05,
      "loss": 0.4726,
      "step": 109500
    },
    {
      "epoch": 1.8897529182543926,
      "grad_norm": 0.09553650766611099,
      "learning_rate": 1.2441057296067039e-05,
      "loss": 0.5303,
      "step": 109600
    },
    {
      "epoch": 1.891477145369588,
      "grad_norm": 0.03750234097242355,
      "learning_rate": 1.2434160387606255e-05,
      "loss": 0.5369,
      "step": 109700
    },
    {
      "epoch": 1.8932013724847838,
      "grad_norm": 29.47915267944336,
      "learning_rate": 1.2427263479145475e-05,
      "loss": 0.3176,
      "step": 109800
    },
    {
      "epoch": 1.8949255995999792,
      "grad_norm": 28.463193893432617,
      "learning_rate": 1.2420366570684692e-05,
      "loss": 0.7383,
      "step": 109900
    },
    {
      "epoch": 1.896649826715175,
      "grad_norm": 0.05380040034651756,
      "learning_rate": 1.241346966222391e-05,
      "loss": 0.446,
      "step": 110000
    },
    {
      "epoch": 1.8983740538303704,
      "grad_norm": 0.14801065623760223,
      "learning_rate": 1.2406572753763126e-05,
      "loss": 0.3756,
      "step": 110100
    },
    {
      "epoch": 1.9000982809455662,
      "grad_norm": 0.1876123547554016,
      "learning_rate": 1.2399675845302343e-05,
      "loss": 0.3949,
      "step": 110200
    },
    {
      "epoch": 1.9018225080607618,
      "grad_norm": 0.24426190555095673,
      "learning_rate": 1.2392778936841561e-05,
      "loss": 0.4927,
      "step": 110300
    },
    {
      "epoch": 1.9035467351759574,
      "grad_norm": 25.642255783081055,
      "learning_rate": 1.238588202838078e-05,
      "loss": 0.7466,
      "step": 110400
    },
    {
      "epoch": 1.905270962291153,
      "grad_norm": 0.09563245624303818,
      "learning_rate": 1.2378985119919998e-05,
      "loss": 0.3851,
      "step": 110500
    },
    {
      "epoch": 1.9069951894063486,
      "grad_norm": 0.2780781090259552,
      "learning_rate": 1.2372088211459214e-05,
      "loss": 0.5573,
      "step": 110600
    },
    {
      "epoch": 1.9087194165215442,
      "grad_norm": 0.34781476855278015,
      "learning_rate": 1.2365191302998432e-05,
      "loss": 0.7007,
      "step": 110700
    },
    {
      "epoch": 1.9104436436367398,
      "grad_norm": 0.19864699244499207,
      "learning_rate": 1.2358294394537649e-05,
      "loss": 0.3655,
      "step": 110800
    },
    {
      "epoch": 1.9121678707519354,
      "grad_norm": 28.15855598449707,
      "learning_rate": 1.2351397486076865e-05,
      "loss": 0.6245,
      "step": 110900
    },
    {
      "epoch": 1.913892097867131,
      "grad_norm": 0.044056423008441925,
      "learning_rate": 1.2344500577616085e-05,
      "loss": 0.4448,
      "step": 111000
    },
    {
      "epoch": 1.9156163249823268,
      "grad_norm": 0.1782429814338684,
      "learning_rate": 1.2337603669155302e-05,
      "loss": 0.4801,
      "step": 111100
    },
    {
      "epoch": 1.9173405520975222,
      "grad_norm": 0.09062333405017853,
      "learning_rate": 1.233070676069452e-05,
      "loss": 0.6035,
      "step": 111200
    },
    {
      "epoch": 1.919064779212718,
      "grad_norm": 0.10097748041152954,
      "learning_rate": 1.2323809852233736e-05,
      "loss": 0.4031,
      "step": 111300
    },
    {
      "epoch": 1.9207890063279134,
      "grad_norm": 0.19256603717803955,
      "learning_rate": 1.2316912943772955e-05,
      "loss": 0.6925,
      "step": 111400
    },
    {
      "epoch": 1.9225132334431092,
      "grad_norm": 28.0238037109375,
      "learning_rate": 1.2310016035312173e-05,
      "loss": 0.6133,
      "step": 111500
    },
    {
      "epoch": 1.9242374605583046,
      "grad_norm": 0.32270529866218567,
      "learning_rate": 1.2303119126851391e-05,
      "loss": 0.4601,
      "step": 111600
    },
    {
      "epoch": 1.9259616876735004,
      "grad_norm": 0.024718506261706352,
      "learning_rate": 1.2296222218390608e-05,
      "loss": 0.6132,
      "step": 111700
    },
    {
      "epoch": 1.927685914788696,
      "grad_norm": 0.13110505044460297,
      "learning_rate": 1.2289325309929824e-05,
      "loss": 0.5691,
      "step": 111800
    },
    {
      "epoch": 1.9294101419038916,
      "grad_norm": 0.06721585243940353,
      "learning_rate": 1.2282428401469042e-05,
      "loss": 0.3431,
      "step": 111900
    },
    {
      "epoch": 1.9311343690190872,
      "grad_norm": 0.043725963681936264,
      "learning_rate": 1.2275531493008259e-05,
      "loss": 0.4917,
      "step": 112000
    },
    {
      "epoch": 1.9328585961342828,
      "grad_norm": 0.08296254277229309,
      "learning_rate": 1.2268634584547479e-05,
      "loss": 0.4921,
      "step": 112100
    },
    {
      "epoch": 1.9345828232494784,
      "grad_norm": 0.09403447061777115,
      "learning_rate": 1.2261737676086695e-05,
      "loss": 0.4973,
      "step": 112200
    },
    {
      "epoch": 1.936307050364674,
      "grad_norm": 0.18579547107219696,
      "learning_rate": 1.2254840767625913e-05,
      "loss": 0.4998,
      "step": 112300
    },
    {
      "epoch": 1.9380312774798698,
      "grad_norm": 0.05687510967254639,
      "learning_rate": 1.224794385916513e-05,
      "loss": 0.6012,
      "step": 112400
    },
    {
      "epoch": 1.9397555045950652,
      "grad_norm": 0.1280757337808609,
      "learning_rate": 1.2241046950704346e-05,
      "loss": 0.7694,
      "step": 112500
    },
    {
      "epoch": 1.941479731710261,
      "grad_norm": 0.29015132784843445,
      "learning_rate": 1.2234150042243565e-05,
      "loss": 0.6511,
      "step": 112600
    },
    {
      "epoch": 1.9432039588254564,
      "grad_norm": 29.260101318359375,
      "learning_rate": 1.2227253133782785e-05,
      "loss": 0.6946,
      "step": 112700
    },
    {
      "epoch": 1.9449281859406522,
      "grad_norm": 0.06462270766496658,
      "learning_rate": 1.2220356225322001e-05,
      "loss": 0.5727,
      "step": 112800
    },
    {
      "epoch": 1.9466524130558476,
      "grad_norm": 30.093420028686523,
      "learning_rate": 1.2213459316861218e-05,
      "loss": 0.4275,
      "step": 112900
    },
    {
      "epoch": 1.9483766401710434,
      "grad_norm": 0.14930181205272675,
      "learning_rate": 1.2206562408400436e-05,
      "loss": 0.4601,
      "step": 113000
    },
    {
      "epoch": 1.950100867286239,
      "grad_norm": 0.22853665053844452,
      "learning_rate": 1.2199665499939652e-05,
      "loss": 0.4171,
      "step": 113100
    },
    {
      "epoch": 1.9518250944014346,
      "grad_norm": 0.07192952930927277,
      "learning_rate": 1.2192768591478869e-05,
      "loss": 0.3973,
      "step": 113200
    },
    {
      "epoch": 1.9535493215166302,
      "grad_norm": 0.06451082974672318,
      "learning_rate": 1.2185871683018089e-05,
      "loss": 0.394,
      "step": 113300
    },
    {
      "epoch": 1.9552735486318258,
      "grad_norm": 0.06047599017620087,
      "learning_rate": 1.2178974774557307e-05,
      "loss": 0.7378,
      "step": 113400
    },
    {
      "epoch": 1.9569977757470214,
      "grad_norm": 0.10815294086933136,
      "learning_rate": 1.2172077866096523e-05,
      "loss": 0.549,
      "step": 113500
    },
    {
      "epoch": 1.958722002862217,
      "grad_norm": 6.067759983352516e-08,
      "learning_rate": 1.216518095763574e-05,
      "loss": 0.5515,
      "step": 113600
    },
    {
      "epoch": 1.9604462299774126,
      "grad_norm": 0.3443639576435089,
      "learning_rate": 1.2158284049174958e-05,
      "loss": 0.457,
      "step": 113700
    },
    {
      "epoch": 1.9621704570926082,
      "grad_norm": 0.07848376780748367,
      "learning_rate": 1.2151387140714175e-05,
      "loss": 0.5703,
      "step": 113800
    },
    {
      "epoch": 1.963894684207804,
      "grad_norm": 0.2012873739004135,
      "learning_rate": 1.2144490232253395e-05,
      "loss": 0.587,
      "step": 113900
    },
    {
      "epoch": 1.9656189113229994,
      "grad_norm": 33.18488693237305,
      "learning_rate": 1.2137593323792611e-05,
      "loss": 0.5616,
      "step": 114000
    },
    {
      "epoch": 1.9673431384381952,
      "grad_norm": 0.1413072943687439,
      "learning_rate": 1.2130696415331828e-05,
      "loss": 0.3212,
      "step": 114100
    },
    {
      "epoch": 1.9690673655533906,
      "grad_norm": 28.856117248535156,
      "learning_rate": 1.2123799506871046e-05,
      "loss": 0.5466,
      "step": 114200
    },
    {
      "epoch": 1.9707915926685864,
      "grad_norm": 0.23510894179344177,
      "learning_rate": 1.2116902598410262e-05,
      "loss": 0.5188,
      "step": 114300
    },
    {
      "epoch": 1.9725158197837818,
      "grad_norm": 0.29494333267211914,
      "learning_rate": 1.2110005689949482e-05,
      "loss": 0.5065,
      "step": 114400
    },
    {
      "epoch": 1.9742400468989776,
      "grad_norm": 0.09618070721626282,
      "learning_rate": 1.2103108781488699e-05,
      "loss": 0.5539,
      "step": 114500
    },
    {
      "epoch": 1.9759642740141732,
      "grad_norm": 0.2746145725250244,
      "learning_rate": 1.2096211873027917e-05,
      "loss": 0.461,
      "step": 114600
    },
    {
      "epoch": 1.9776885011293688,
      "grad_norm": 0.12446040660142899,
      "learning_rate": 1.2089314964567134e-05,
      "loss": 0.3623,
      "step": 114700
    },
    {
      "epoch": 1.9794127282445644,
      "grad_norm": 0.14958156645298004,
      "learning_rate": 1.208241805610635e-05,
      "loss": 0.6099,
      "step": 114800
    },
    {
      "epoch": 1.98113695535976,
      "grad_norm": 25.44306182861328,
      "learning_rate": 1.2075521147645568e-05,
      "loss": 0.4063,
      "step": 114900
    },
    {
      "epoch": 1.9828611824749556,
      "grad_norm": 0.14672620594501495,
      "learning_rate": 1.2068624239184788e-05,
      "loss": 0.5352,
      "step": 115000
    },
    {
      "epoch": 1.9845854095901512,
      "grad_norm": 0.0475686714053154,
      "learning_rate": 1.2061727330724005e-05,
      "loss": 0.444,
      "step": 115100
    },
    {
      "epoch": 1.986309636705347,
      "grad_norm": 0.31420016288757324,
      "learning_rate": 1.2054830422263221e-05,
      "loss": 0.4251,
      "step": 115200
    },
    {
      "epoch": 1.9880338638205424,
      "grad_norm": 0.0695868581533432,
      "learning_rate": 1.204793351380244e-05,
      "loss": 0.631,
      "step": 115300
    },
    {
      "epoch": 1.9897580909357382,
      "grad_norm": 0.280611515045166,
      "learning_rate": 1.2041036605341656e-05,
      "loss": 0.3543,
      "step": 115400
    },
    {
      "epoch": 1.9914823180509336,
      "grad_norm": 0.11285989731550217,
      "learning_rate": 1.2034139696880872e-05,
      "loss": 0.5211,
      "step": 115500
    },
    {
      "epoch": 1.9932065451661294,
      "grad_norm": 0.07588913291692734,
      "learning_rate": 1.2027242788420092e-05,
      "loss": 0.6042,
      "step": 115600
    },
    {
      "epoch": 1.9949307722813248,
      "grad_norm": 0.030643245205283165,
      "learning_rate": 1.202034587995931e-05,
      "loss": 0.3221,
      "step": 115700
    },
    {
      "epoch": 1.9966549993965206,
      "grad_norm": 0.09257492423057556,
      "learning_rate": 1.2013448971498527e-05,
      "loss": 0.5073,
      "step": 115800
    },
    {
      "epoch": 1.9983792265117162,
      "grad_norm": 0.11882416903972626,
      "learning_rate": 1.2006552063037744e-05,
      "loss": 0.4768,
      "step": 115900
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9948703699745656,
      "eval_f1_macro": 0.7106471454561206,
      "eval_loss": 0.5013225674629211,
      "eval_runtime": 182.3087,
      "eval_samples_per_second": 1796.453,
      "eval_steps_per_second": 56.141,
      "step": 115994
    }
  ],
  "logging_steps": 100,
  "max_steps": 289985,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2293289106686874e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
